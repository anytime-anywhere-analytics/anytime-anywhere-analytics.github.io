<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003d73;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'→';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'↓'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}}main{margin:auto;padding:.25rem;padding-bottom:0;max-width:120em}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;↗</sup></a>
      </nav>
    </header>
    
        <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; padding-left: 20px; padding-right: 20px; padding-bottom: 13px; margin-top: 30px; margin-bottom: 0px">
        
            <h4 style="margin-bottom: 0px; padding-top 10px;">Publications coauthored by</h4>
            <h2 style="margin-bottom: 0px; margin-top: 10px;">Sriram Karthik Badam</h2>
        
        </div>
    
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
  <ul>
      
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #78&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2022" onClick="copy('Badam2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2022" style="display: none;">
                        @article{Badam2022,
title={Integrating Annotations into Multidimensional Visual Dashboards},
author={Sriram Karthik Badam and Senthil Chandrasegaran and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/facetnotes/facetnotes.pdf},
year={2022},
date={2022-05-10},
journal={Information Visualization},
volume={21},
number={3},
pages={270--284},
abstract={Multidimensional data is often visualized using coordinated multiple views in an interactive dashboard. However, unlike in infographics where text is often a central part of the presentation, there is currently little knowledge of how to best integrate text and annotations in a visualization dashboard. In this paper, we explore a technique called FacetNotes for presenting these textual annotations on top of any visualization within a dashboard irrespective of the scale of data shown or the design of visual representation itself. FacetNotes does so by grouping and ordering the textual annotations based on properties of (1) the individual data points associated with the annotations, and (2) the target visual representation on which they should be shown. We present this technique along with a set of user interface features and guidelines to apply it to visualization interfaces. We also demonstrate FacetNotes in a custom visual dashboard interface. Finally, results from a user study of FacetNotes show that the technique improves the scope and complexity of insights developed during visual exploration.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/facetnotes/facetnotes.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Integrating Annotations into Multidimensional Visual Dashboards<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Multidimensional data is often visualized using coordinated multiple views in an interactive dashboard. However, unlike in infographics where text is often a central part of the presentation, there is currently little knowledge of how to best integrate text and annotations in a visualization dashboard. In this paper, we explore a technique called FacetNotes for presenting these textual annotations on top of any visualization within a dashboard irrespective of the scale of data shown or the design of visual representation itself. FacetNotes does so by grouping and ordering the textual annotations based on properties of (1) the individual data points associated with the annotations, and (2) the target visual representation on which they should be shown. We present this technique along with a set of user interface features and guidelines to apply it to visualization interfaces. We also demonstrate FacetNotes in a custom visual dashboard interface. Finally, results from a user study of FacetNotes show that the technique improves the scope and complexity of insights developed during visual exploration.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #75&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Wang2021" onClick="copy('Wang2021')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Wang2021" style="display: none;">
                        @article{Wang2021,
title={Topology-Aware Space Distortion for Structured Visualization Spaces},
author={Weihang Wang and Sriram Karthik Badam and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/zoomhalo/zoomhalo.pdf},
year={2021},
date={2021-09-29},
journal={Information Visualization},
abstract={We propose topology-aware space distortion (TASD), a family of interactive layout techniques for non-linearly distorting geometric space based on user attention and on the structure of the visual representation. TASD seamlessly adapts the visual substrate of any visualization to give more screen real estate to important regions of the representation at the expense of less important regions. In this paper, we present a concrete TASD technique that we call ZoomHalo for interactively distorting a two-dimensional space based on a degree-of-interest (DOI) function defined for the space. Using this DOI function, ZoomHalo derives several areas of interest, computes the available space around each area in relation to other areas and the current viewport extents, and then dynamically expands (or shrinks) each area given user input. We use our prototype to evaluate the technique in two user studies, as well as showcase examples of TASD for node-link diagrams, word clouds, and geographical maps.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2021">2021</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/zoomhalo/zoomhalo.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Topology-Aware Space Distortion for Structured Visualization Spaces<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/weihang-wang/">Weihang Wang</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We propose topology-aware space distortion (TASD), a family of interactive layout techniques for non-linearly distorting geometric space based on user attention and on the structure of the visual representation. TASD seamlessly adapts the visual substrate of any visualization to give more screen real estate to important regions of the representation at the expense of less important regions. In this paper, we present a concrete TASD technique that we call ZoomHalo for interactively distorting a two-dimensional space based on a degree-of-interest (DOI) function defined for the space. Using this DOI function, ZoomHalo derives several areas of interest, computes the available space around each area in relation to other areas and the current viewport extents, and then dynamically expands (or shrinks) each area given user input. We use our prototype to evaluate the technique in two user studies, as well as showcase examples of TASD for node-link diagrams, word clouds, and geographical maps.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #74&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2021" onClick="copy('Badam2021')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2021" style="display: none;">
                        @article{Badam2021,
title={Effects of Screen-Responsive Visualization on Data Comprehension},
author={Sriram Karthik Badam and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/touchinsight/touchinsight.pdf},
year={2021},
date={2021-09-01},
journal={Information Visualization},
volume={20},
number={4},
pages={229--244},
abstract={Visualization interfaces designed for heterogeneous devices such as wall displays and mobile screens must be responsive to varying display dimensions, resolution, and interaction capabilities. In this paper, we report on two user studies of visual representations for large versus small displays. The goal of our experiments was to investigate differences between a large vertical display and a mobile hand-held display in terms of the data comprehension and the quality of resulting insights. To this end, we developed a visual interface with a coordinated multiple view layout for the large display and two alternative designs of the same interface---a space-saving boundary visualization layout and an overview layout---for the mobile condition. The first experiment was a controlled laboratory study designed to evaluate the effect of display size on the perception of changes in a visual representation, and yielded significant correctness differences even while completion time remained similar. The second evaluation was a qualitative study in a practical setting and showed that participants were able to easily associate and work with the responsive visualizations. Based on the results, we conclude the paper by providing new guidelines for screen-responsive visualization interfaces.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2021">2021</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/touchinsight/touchinsight.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Effects of Screen-Responsive Visualization on Data Comprehension<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Visualization interfaces designed for heterogeneous devices such as wall displays and mobile screens must be responsive to varying display dimensions, resolution, and interaction capabilities. In this paper, we report on two user studies of visual representations for large versus small displays. The goal of our experiments was to investigate differences between a large vertical display and a mobile hand-held display in terms of the data comprehension and the quality of resulting insights. To this end, we developed a visual interface with a coordinated multiple view layout for the large display and two alternative designs of the same interface---a space-saving boundary visualization layout and an overview layout---for the mobile condition. The first experiment was a controlled laboratory study designed to evaluate the effect of display size on the perception of changes in a visual representation, and yielded significant correctness differences even while completion time remained similar. The second evaluation was a qualitative study in a practical setting and showed that participants were able to easily associate and work with the responsive visualizations. Based on the results, we conclude the paper by providing new guidelines for screen-responsive visualization interfaces.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #60&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2019b" onClick="copy('Badam2019b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2019b" style="display: none;">
                        @article{Badam2019b,
title={Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading},
author={Sriram Karthik Badam and Zhicheng Liu and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/elastic-documents/elastic-documents.pdf},
year={2019},
date={2019-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Today&#39;s data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, gures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader&#39;s current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/elastic-documents/elastic-documents.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhicheng-liu/">Zhicheng Liu</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Today&#39;s data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, gures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader&#39;s current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #59&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2019a" onClick="copy('Badam2019a')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2019a" style="display: none;">
                        @article{Badam2019a,
title={Vistrates: A Component Model for Ubiquitous Analytics},
author={Sriram Karthik Badam and Andreas Mathisen and Roman Rädle and Clemens Nylandsted Klokmose and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/vistrates/vistrates.pdf},
year={2019},
date={2019-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Visualization tools are often specialized for specic tasks, which turns the user&#39;s analytical workow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components—the building blocks of this model—can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic &quot;anytime&quot; and &quot;anywhere&quot; motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/vistrates/vistrates.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Vistrates: A Component Model for Ubiquitous Analytics<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andreas-mathisen/">Andreas Mathisen</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/roman-raedle/">Roman Rädle</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/clemens-nylandsted-klokmose/">Clemens Nylandsted Klokmose</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Visualization tools are often specialized for specic tasks, which turns the user&#39;s analytical workow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components—the building blocks of this model—can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic &quot;anytime&quot; and &quot;anywhere&quot; motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #58&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-zcui2018" onClick="copy('zcui2018')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-zcui2018" style="display: none;">
                        @article{zcui2018,
title={DataSite: Proactive Visual Data Exploration with Computation of Insight-based Recommendations},
author={Zhe Cui and Sriram Karthik Badam and Mehmet Adil Yalcin and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/datasite/datasite.pdf},
year={2019},
date={2019-01-01},
journal={Information Visualization},
volume={18},
number={2},
pages={251--267},
abstract={Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/datasite/datasite.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> DataSite: Proactive Visual Data Exploration with Computation of Insight-based Recommendations<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhe-cui/">Zhe Cui</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mehmet-adil-yalcin/">Mehmet Adil Yalcin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #55&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Cui2018" onClick="copy('Cui2018')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Cui2018" style="display: none;">
                        @article{Cui2018,
title={VisHive: Supporting Web-based Visualization through Ad-hoc Computational Clusters of Mobile Devices},
author={Zhe Cui and Shivalik Sen and Sriram Karthik Badam and Niklas Elmqvist },
url={http://www.umiacs.umd.edu/~elm/projects/vishive/vishive.pdf},
year={2018},
date={2018-01-01},
journal={Information Visualization},
abstract={Current web-based visualizations are designed for single computers and cannot make use of additional devices on the client side, even if today’s users often have access to several, such as a tablet, a smartphone, and a smartwatch. We present a framework for ad-hoc computational clusters that leverage these local devices for visualization computations. Furthermore, we present an instantiating JavaScript toolkit called VisHive for constructing web-based visualization applications that can transparently connect multiple devices---called cells---into such ad-hoc clusters---called a hive---for local computation. Hives are formed either using a matchmaking service or through manual configuration. Cells are organized into a master-slave architecture, where the master provides the visual interface to the user and controls the slaves, and the slaves perform computation. VisHive is built entirely using current web technologies, runs in the native browser of each cell, and requires no specific software to be downloaded on the involved devices. We demonstrate VisHive using four distributed examples: a text analytics visualization, a database query for exploratory visualization, a},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/vishive/vishive.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> VisHive: Supporting Web-based Visualization through Ad-hoc Computational Clusters of Mobile Devices<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhe-cui/">Zhe Cui</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/shivalik-sen/">Shivalik Sen</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist </span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Current web-based visualizations are designed for single computers and cannot make use of additional devices on the client side, even if today’s users often have access to several, such as a tablet, a smartphone, and a smartwatch. We present a framework for ad-hoc computational clusters that leverage these local devices for visualization computations. Furthermore, we present an instantiating JavaScript toolkit called VisHive for constructing web-based visualization applications that can transparently connect multiple devices---called cells---into such ad-hoc clusters---called a hive---for local computation. Hives are formed either using a matchmaking service or through manual configuration. Cells are organized into a master-slave architecture, where the master provides the visual interface to the user and controls the slaves, and the slaves perform computation. VisHive is built entirely using current web technologies, runs in the native browser of each cell, and requires no specific software to be downloaded on the involved devices. We demonstrate VisHive using four distributed examples: a text analytics visualization, a database query for exploratory visualization, a</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #47&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Horak2018" onClick="copy('Horak2018')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Horak2018" style="display: none;">
                        @inproceedings{Horak2018,
title={When David Meets Goliath: Combining Smartwatches with a Large Vertical Display for Visual Data Exploration},
author={Tom Horak and Sriram Karthik Badam and Niklas Elmqvist and Raimund Dachselt},
url={http://www.umiacs.umd.edu/~elm/projects/david-goliath/david-goliath.pdf},
year={2018},
date={2018-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
abstract={We explore the combination of smartwatches and a large interactive display to support visual data analysis. These two extremes of interactive surfaces are increasingly popular, but feature different characteristics—display and input modalities, personal/public use, performance, and portability. In this paper, we first identify possible roles for both devices and the interplay between them through an example scenario. We then propose a conceptual framework to enable analysts to explore data items, track interaction histories, and alter visualization configurations through mechanisms using both devices in combination. We validate an implementation of our framework through a formative evaluation and a user study. The results show that this device combination, compared to just a large display, allows users to develop complex insights more fluidly by leveraging the roles of the two devices. Finally, we report on the interaction patterns and interplay between the devices for visual exploration as observed during our study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/david-goliath/david-goliath.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> When David Meets Goliath: Combining Smartwatches with a Large Vertical Display for Visual Data Exploration<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/tom-horak/">Tom Horak</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/raimund-dachselt/">Raimund Dachselt</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We explore the combination of smartwatches and a large interactive display to support visual data analysis. These two extremes of interactive surfaces are increasingly popular, but feature different characteristics—display and input modalities, personal/public use, performance, and portability. In this paper, we first identify possible roles for both devices and the interplay between them through an example scenario. We then propose a conceptual framework to enable analysts to explore data items, track interaction histories, and alter visualization configurations through mechanisms using both devices in combination. We validate an implementation of our framework through a formative evaluation and a user study. The results show that this device combination, compared to just a large display, allows users to develop complex insights more fluidly by leveraging the roles of the two devices. Finally, we report on the interaction patterns and interplay between the devices for visual exploration as observed during our study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #51&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2017bb" onClick="copy('Badam2017bb')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2017bb" style="display: none;">
                        @article{Badam2017bb,
title={Visfer: Camera-based Visual Data Transfer for Cross-Device Visualization},
author={Sriram Karthik Badam and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/qrvis/visfer.pdf},
year={2017},
date={2017-09-08},
journal={Information Visualization},
abstract={Going beyond the desktop to leverage novel devices—such as smartphones, tablets, or large displays—for visual sensemaking typically requires supporting extraneous operations for device discovery, interaction sharing, and view management. Such operations can be time-consuming and tedious, and distract the user from the actual analysis. Embodied interaction models in these multi-device environments can take advantage of the natural interaction and physicality afforded by multimodal devices and help effectively carry out these operations in visual sensemaking. In this paper, we present cross-device interaction models for visualization spaces, that are embodied in nature, by conducting a user study to elicit actions from participants that could trigger a portrayed effect of sharing visualizations (and therefore information) across devices. We then explore one common interaction style from this design elicitation called Visfer, a technique for effortlessly sharing visualizations across devices using the visual medium. More specifically, this technique involves taking pictures of visualizations, or rather the QR codes augmenting them, on a display using the built-in camera on a handheld device. Our contributions include a conceptual framework for cross-device interaction and the Visfer technique itself, as well as transformation guidelines to exploit the capabilities of each specific device and a web framework for encoding visualization components into animated QR codes, which capture multiple frames of QR codes to embed more information. Beyond this, we also present the results from a performance evaluation for the visual data transfer enabled by Visfer. We end the paper by presenting the application examples of our Visfer framework. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/qrvis/visfer.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Visfer: Camera-based Visual Data Transfer for Cross-Device Visualization<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Going beyond the desktop to leverage novel devices—such as smartphones, tablets, or large displays—for visual sensemaking typically requires supporting extraneous operations for device discovery, interaction sharing, and view management. Such operations can be time-consuming and tedious, and distract the user from the actual analysis. Embodied interaction models in these multi-device environments can take advantage of the natural interaction and physicality afforded by multimodal devices and help effectively carry out these operations in visual sensemaking. In this paper, we present cross-device interaction models for visualization spaces, that are embodied in nature, by conducting a user study to elicit actions from participants that could trigger a portrayed effect of sharing visualizations (and therefore information) across devices. We then explore one common interaction style from this design elicitation called Visfer, a technique for effortlessly sharing visualizations across devices using the visual medium. More specifically, this technique involves taking pictures of visualizations, or rather the QR codes augmenting them, on a display using the built-in camera on a handheld device. Our contributions include a conceptual framework for cross-device interaction and the Visfer technique itself, as well as transformation guidelines to exploit the capabilities of each specific device and a web framework for encoding visualization components into animated QR codes, which capture multiple frames of QR codes to embed more information. Beyond this, we also present the results from a performance evaluation for the visual data transfer enabled by Visfer. We end the paper by presenting the application examples of our Visfer framework. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #45&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chandrasegaran2017" onClick="copy('Chandrasegaran2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chandrasegaran2017" style="display: none;">
                        @inproceedings{Chandrasegaran2017,
title={Merging Sketches for Creative Design Exploration: An Evaluation of Physical and Cognitive Operations},
author={Senthil Chandrasegaran and Sriram Karthik Badam and Ninger Zhou and Zhenpeng Zhao and Lorraine Kisselburgh and Kylie Peppler and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/merge-study/merge-study.pdf},
year={2017},
date={2017-05-15},
journal={Proceedings of Graphics Interface},
booktitle={Proceedings of Graphics Interface},
abstract={Despite its grounding in creativity techniques, merging multiple source sketches to create new ideas has received scant attention in design literature. In this paper, we identify the physical operations that in merging sketch components. We also introduce cognitive operations of reuse, repurpose, refactor, and reinterpret, and explore their relevance to creative design. To examine the relationship of cognitive operations, physical techniques, and creative sketch outcomes, we conducted a qualitative user study where student designers merged existing sketches to generate either an alternative design, or an unrelated new design. We compared two digital selection techniques: freeform selection, and a stroke-cluster-based &quot;object select&quot; technique. The resulting merge sketches were subjected to crowdsourced evaluation of these sketches, and manual coding for the use of cognitive operations. Our findings establish a firm connection between the proposed cognitive operations and the context and outcome of creative tasks. Key findings indicate that reinterpret cognitive operations correlate strongly with creativity in merged sketches, while reuse operations correlate negatively with creativity. Furthermore, freeform selection techniques are preferred significantly by designers. We discuss the empirical contributions of understanding the use of cognitive operations during design exploration, and the practical implications for designing interfaces in digital tools that facilitate creativity in merging sketches. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-graphics-interface">Proceedings of Graphics Interface • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/merge-study/merge-study.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Merging Sketches for Creative Design Exploration: An Evaluation of Physical and Cognitive Operations<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ninger-zhou/">Ninger Zhou</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhenpeng-zhao/">Zhenpeng Zhao</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/kylie-peppler/">Kylie Peppler</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Despite its grounding in creativity techniques, merging multiple source sketches to create new ideas has received scant attention in design literature. In this paper, we identify the physical operations that in merging sketch components. We also introduce cognitive operations of reuse, repurpose, refactor, and reinterpret, and explore their relevance to creative design. To examine the relationship of cognitive operations, physical techniques, and creative sketch outcomes, we conducted a qualitative user study where student designers merged existing sketches to generate either an alternative design, or an unrelated new design. We compared two digital selection techniques: freeform selection, and a stroke-cluster-based &quot;object select&quot; technique. The resulting merge sketches were subjected to crowdsourced evaluation of these sketches, and manual coding for the use of cognitive operations. Our findings establish a firm connection between the proposed cognitive operations and the context and outcome of creative tasks. Key findings indicate that reinterpret cognitive operations correlate strongly with creativity in merged sketches, while reuse operations correlate negatively with creativity. Furthermore, freeform selection techniques are preferred significantly by designers. We discuss the empirical contributions of understanding the use of cognitive operations during design exploration, and the practical implications for designing interfaces in digital tools that facilitate creativity in merging sketches. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #44&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2017" onClick="copy('Badam2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2017" style="display: none;">
                        @inproceedings{Badam2017,
title={Supporting Team-First Visual Analytics through Group Activity Representations},
author={Sriram Karthik Badam and Zehua Zheng and Emily Wall and Alex Endert and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/group-awareness/group-awareness.pdf},
year={2017},
date={2017-05-15},
journal={Proceedings of Graphics Interface},
booktitle={Proceedings of Graphics Interface},
abstract={Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of &quot;team-first&quot; visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-graphics-interface">Proceedings of Graphics Interface • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/group-awareness/group-awareness.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Supporting Team-First Visual Analytics through Group Activity Representations<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zehua-zheng/">Zehua Zheng</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/emily-wall/">Emily Wall</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/alex-endert/">Alex Endert</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of &quot;team-first&quot; visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #49&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2017b" onClick="copy('Badam2017b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2017b" style="display: none;">
                        @article{Badam2017b,
title={Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics},
author={Sriram Karthik Badam and Niklas Elmqvist and Jean-Daniel Fekete},
url={http://www.umiacs.umd.edu/~elm/projects/insightsfeed/insightsfeed.pdf},
year={2017},
date={2017-05-15},
journal={Computer Graphics Forum},
volume={36},
abstract={Progressive visual analytics (PVA) has emerged in recent years to manage the latency of data analysis systems. When analysis is performed progressively, rough estimates of the results are generated quickly and are then improved over time. Analysts can therefore monitor the progression of the results, steer the analysis algorithms, and make early decisions if the estimates provide a convincing picture. In this article, we describe interface design guidelines for helping users understand progressively updating results and make early decisions based on progressive estimates. To illustrate our ideas, we present a prototype PVA tool called InsightsFeed for exploring Twitter data at scale. As validation, we investigate the tradeoffs of our tool when exploring a Twitter dataset in a user study. We report the usage patterns in making early decisions using the user interface, guiding computational methods, and exploring different subsets of the dataset, compared to sequential analysis without progression.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/computer-graphics-forum">Computer Graphics Forum • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/insightsfeed/insightsfeed.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jean-daniel-fekete/">Jean-Daniel Fekete</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Progressive visual analytics (PVA) has emerged in recent years to manage the latency of data analysis systems. When analysis is performed progressively, rough estimates of the results are generated quickly and are then improved over time. Analysts can therefore monitor the progression of the results, steer the analysis algorithms, and make early decisions if the estimates provide a convincing picture. In this article, we describe interface design guidelines for helping users understand progressively updating results and make early decisions based on progressive estimates. To illustrate our ideas, we present a prototype PVA tool called InsightsFeed for exploring Twitter data at scale. As validation, we investigate the tradeoffs of our tool when exploring a Twitter dataset in a user study. We report the usage patterns in making early decisions using the user interface, guiding computational methods, and exploring different subsets of the dataset, compared to sequential analysis without progression.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #48&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chandrasegaran2017c" onClick="copy('Chandrasegaran2017c')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chandrasegaran2017c" style="display: none;">
                        @article{Chandrasegaran2017c,
title={Integrating Visual Analytics Support for Grounded Theory Practice in Qualitative Text Analysis},
author={Senthil Chandrasegaran and Sriram Karthik Badam and Lorraine Kisselburgh and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/gthelper/gthelper.pdf},
year={2017},
date={2017-05-15},
journal={Computer Graphics Forum},
volume={36},
abstract={We present an argument for using visual analytics to aid Grounded Theory methodologies in qualitative data analysis. Grounded theory methods involve the inductive analysis of data to generate novel insights and theoretical constructs. Making sense of unstructured text data is uniquely suited for visual analytics. Using natural language processing techniques such as parts-of-speech tagging, retrieving information content, and topic modeling, different parts of the data can be structured and semantically associated, and interactively explored, thereby providing conceptual depth to the guided discovery process. We review grounded theory methods and identify processes that can be enhanced through visual analytic techniques. Next, we develop an interface for qualitative text analysis, and evaluate our design with qualitative research practitioners who analyze texts with and without visual analytics support. The results of our study suggest how visual analytics can be incorporated into qualitative data analysis tools, and the analytic and interpretive benefits that can result.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/computer-graphics-forum">Computer Graphics Forum • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/gthelper/gthelper.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Integrating Visual Analytics Support for Grounded Theory Practice in Qualitative Text Analysis<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present an argument for using visual analytics to aid Grounded Theory methodologies in qualitative data analysis. Grounded theory methods involve the inductive analysis of data to generate novel insights and theoretical constructs. Making sense of unstructured text data is uniquely suited for visual analytics. Using natural language processing techniques such as parts-of-speech tagging, retrieving information content, and topic modeling, different parts of the data can be structured and semantically associated, and interactively explored, thereby providing conceptual depth to the guided discovery process. We review grounded theory methods and identify processes that can be enhanced through visual analytic techniques. Next, we develop an interface for qualitative text analysis, and evaluate our design with qualitative research practitioners who analyze texts with and without visual analytics support. The results of our study suggest how visual analytics can be incorporated into qualitative data analysis tools, and the analytic and interpretive benefits that can result.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #47&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chandrasegaran2017b" onClick="copy('Chandrasegaran2017b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chandrasegaran2017b" style="display: none;">
                        @article{Chandrasegaran2017b,
title={VizScribe: A Visual Analytics Approach to Understand Designer Behavior},
author={Senthil Chandrasegaran and Sriram Karthik Badam and Lorraine Kisselburgh and Kylie Peppler and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/vizscribe/vizscribe.pdf},
year={2017},
date={2017-01-02},
journal={International Journal of Human-Computer Interaction},
volume={100},
pages={66--80},
abstract={Design protocol analysis is a technique to understand designers’ cognitive processes by analyzing sequences of observations on their behavior. These observations typically use audio, video, and transcript data in order to gain insights into the designer&#39;s behavior and the design process. The recent availability of sophisticated sensing technology has made such data highly multimodal, requiring more flexible protocol analysis tools. To address this need, we present VizScribe, a visual analytics framework that employs multiple coordinated multiple views that enable the viewing of such data from different perspectives. VizScribe allows designers to create, customize, and extend interactive visualizations for design protocol data such as video, transcripts, sketches, sensor data, and user logs. User studies where design researchers used VizScribe for protocol analysis indicated that the linked views and interactive navigation offered by VizScribe afforded the researchers multiple, useful ways to approach and interpret such multimodal data.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/international-journal-of-human-computer-interaction">International Journal of Human-Computer Interaction • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/vizscribe/vizscribe.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> VizScribe: A Visual Analytics Approach to Understand Designer Behavior<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/kylie-peppler/">Kylie Peppler</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Design protocol analysis is a technique to understand designers’ cognitive processes by analyzing sequences of observations on their behavior. These observations typically use audio, video, and transcript data in order to gain insights into the designer&#39;s behavior and the design process. The recent availability of sophisticated sensing technology has made such data highly multimodal, requiring more flexible protocol analysis tools. To address this need, we present VizScribe, a visual analytics framework that employs multiple coordinated multiple views that enable the viewing of such data from different perspectives. VizScribe allows designers to create, customize, and extend interactive visualizations for design protocol data such as video, transcripts, sketches, sensor data, and user logs. User studies where design researchers used VizScribe for protocol analysis indicated that the linked views and interactive navigation offered by VizScribe afforded the researchers multiple, useful ways to approach and interpret such multimodal data.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #40&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2016b" onClick="copy('Badam2016b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2016b" style="display: none;">
                        @inproceedings{Badam2016b,
title={Supporting Visual Exploration for Multiple Users in Large Display Environments},
author={Sriram Karthik Badam and Feresteh Amini and Niklas Elmqvist and Pourang Irani},
url={http://umiacs.umd.edu/~elm/projects/multiuser-vis/multiuser-vis.pdf},
video={https://www.youtube.com/watch?v=xd7G_q8nocc},
year={2016},
date={2016-10-21},
journal={Proceedings of the IEEE Conference on Visual Analytics Science &amp; Technology},
booktitle={Proceedings of the IEEE Conference on Visual Analytics Science &amp; Technology},
abstract={We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between the implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-ieee-conference-on-visual-analytics-science-and-technology">Proceedings of the IEEE Conference on Visual Analytics Science &amp; Technology • <a class="secondary" style="font-weight: 500" href="/publications/year/2016">2016</a></small>
                  </div>
                  <a href="http://umiacs.umd.edu/~elm/projects/multiuser-vis/multiuser-vis.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Supporting Visual Exploration for Multiple Users in Large Display Environments<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/feresteh-amini/">Feresteh Amini</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/pourang-irani/">Pourang Irani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between the implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles. </p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=xd7G_q8nocc" style="font-weight: 500"><small>📺 Video</small></a>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #38&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2016" onClick="copy('Badam2016')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2016" style="display: none;">
                        @inproceedings{Badam2016,
title={TimeFork: Interactive Prediction of Time Series},
author={Sriram Karthik Badam and Jieqiong Zhao and Shivalik Sen and Niklas Elmqvist and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/timefork/timefork.pdf},
year={2016},
date={2016-05-05},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={5409--5420},
abstract={We present TimeFork, an interactive prediction technique to support users predicting the future of time-series data, such as in financial, scientific, or medical domains. TimeFork combines visual representations of multiple time series with prediction information generated by computational models. Using this method, analysts engage in a back-and-forth dialogue with the computational model by alternating between manually predicting future changes through interaction and letting the model automatically determine the most likely outcomes, to eventually come to a common prediction using the model. This computer-supported prediction approach allows for harnessing the user’s knowledge of factors influencing future behavior, as well as sophisticated computational models drawing on past performance. To validate the TimeFork technique, we conducted a user study in a stock market prediction game. We present evidence of improved performance for participants using TimeFork compared to fully manual or fully automatic predictions, and characterize qualitative usage patterns observed during the user study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2016">2016</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/timefork/timefork.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> TimeFork: Interactive Prediction of Time Series<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jieqiong-zhao/">Jieqiong Zhao</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/shivalik-sen/">Shivalik Sen</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present TimeFork, an interactive prediction technique to support users predicting the future of time-series data, such as in financial, scientific, or medical domains. TimeFork combines visual representations of multiple time series with prediction information generated by computational models. Using this method, analysts engage in a back-and-forth dialogue with the computational model by alternating between manually predicting future changes through interaction and letting the model automatically determine the most likely outcomes, to eventually come to a common prediction using the model. This computer-supported prediction approach allows for harnessing the user’s knowledge of factors influencing future behavior, as well as sophisticated computational models drawing on past performance. To validate the TimeFork technique, we conducted a user study in a stock market prediction game. We present evidence of improved performance for participants using TimeFork compared to fully manual or fully automatic predictions, and characterize qualitative usage patterns observed during the user study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #36&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2015" onClick="copy('Badam2015')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2015" style="display: none;">
                        @article{Badam2015,
title={Munin: A Peer-to-Peer Middleware for Ubiquitous Analytics and Visualization Spaces},
author={Sriram Karthik Badam and Eli Raymond Fisher and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/munin/munin.pdf},
video={https://www.youtube.com/watch?v=ZKIXSdUm6-s},
year={2015},
date={2015-02-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
volume={21},
number={2},
pages={215--228},
abstract={We present Munin, a software framework for building ubiquitous analytics environments consisting of multiple input and output surfaces, such as tabletop displays, wall-mounted displays, and mobile devices. Munin utilizes a service-based model where each device provides one or more dynamically loaded services for input, display, or computation. Using a peer-to-peer model for communication, it leverages IP multicast to replicate the shared state among the peers. Input is handled through a shared event channel that lets input and output devices be fully decoupled. It also provides a data-driven scene graph to delegate rendering to peers, thus creating a robust, fault-tolerant, decentralized system. In this paper, we describe Munin&#39;s general design and architecture, provide several examples of how we are using the framework for ubiquitous analytics and visualization, and present a case study on building a Munin assembly for multidimensional visualization. We also present performance results and anecdotal user feedback for the framework that suggests that combining a service-oriented, data-driven model with middleware support for data sharing and event handling eases the design and execution of high performance distributed visualizations.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2015">2015</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/munin/munin.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Munin: A Peer-to-Peer Middleware for Ubiquitous Analytics and Visualization Spaces<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/eli-raymond-fisher/">Eli Raymond Fisher</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present Munin, a software framework for building ubiquitous analytics environments consisting of multiple input and output surfaces, such as tabletop displays, wall-mounted displays, and mobile devices. Munin utilizes a service-based model where each device provides one or more dynamically loaded services for input, display, or computation. Using a peer-to-peer model for communication, it leverages IP multicast to replicate the shared state among the peers. Input is handled through a shared event channel that lets input and output devices be fully decoupled. It also provides a data-driven scene graph to delegate rendering to peers, thus creating a robust, fault-tolerant, decentralized system. In this paper, we describe Munin&#39;s general design and architecture, provide several examples of how we are using the framework for ubiquitous analytics and visualization, and present a case study on building a Munin assembly for multidimensional visualization. We also present performance results and anecdotal user feedback for the framework that suggests that combining a service-oriented, data-driven model with middleware support for data sharing and event handling eases the design and execution of high performance distributed visualizations.</p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=ZKIXSdUm6-s" style="font-weight: 500"><small>📺 Video</small></a>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #35&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Roberts2014" onClick="copy('Roberts2014')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Roberts2014" style="display: none;">
                        @article{Roberts2014,
title={Visualization Beyond the Desktop --- The Next Big Thing},
author={Jonathan C. Roberts and Panagiotis D. Ritsos and Sriram Karthik Badam and Dominique Brodbeck and Jessie Kennedy and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/beyond-desktop/beyond-desktop.pdf},
year={2014},
date={2014-12-02},
journal={IEEE Computer Graphics &amp; Applications},
volume={34},
number={6},
pages={26--34},
abstract={Visualization is coming of age: with visual depictions being seamlessly integrated into documents and data visualization techniques being used to understand datasets that are ever-growing in size and complexity, the term visualization is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today&#39;s new devices and tomorrows technology. Today, we are interacting with visual depictions through a mouse. Tomorrow, we will be touching, swiping, grasping, feeling, hearing, smelling and even tasting our data. The next big thing is multi-sensory visualization that goes beyond the desktop.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-computer-graphics-and-applications">IEEE Computer Graphics &amp; Applications • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/beyond-desktop/beyond-desktop.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Visualization Beyond the Desktop --- The Next Big Thing<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jonathan-c-roberts/">Jonathan C. Roberts</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/panagiotis-d-ritsos/">Panagiotis D. Ritsos</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/dominique-brodbeck/">Dominique Brodbeck</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jessie-kennedy/">Jessie Kennedy</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Visualization is coming of age: with visual depictions being seamlessly integrated into documents and data visualization techniques being used to understand datasets that are ever-growing in size and complexity, the term visualization is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today&#39;s new devices and tomorrows technology. Today, we are interacting with visual depictions through a mouse. Tomorrow, we will be touching, swiping, grasping, feeling, hearing, smelling and even tasting our data. The next big thing is multi-sensory visualization that goes beyond the desktop.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #36&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2014a" onClick="copy('Badam2014a')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2014a" style="display: none;">
                        @inproceedings{Badam2014a,
title={Tracing and Sketching Performance using Blunt-Tipped Styli on Direct-Touch Tablets},
author={Sriram Karthik Badam and Senthil Chandrasegaran and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/sketch-media/sketch-media.pdf},
year={2014},
date={2014-07-01},
journal={Proceedings of the ACM Conference on Advanced Visual Interfaces},
booktitle={Proceedings of the ACM Conference on Advanced Visual Interfaces},
pages={193--200},
abstract={Direct-touch tablets are quickly replacing traditional pen-and-paper tools in many applications, but not in case of the designer’s sketchbook. In this paper, we explore the tradeoffs inherent in replacing such paper sketchbooks with digital tablets in terms of two major tasks: tracing and free-hand sketching. Given the importance of the pen for sketching, we also study the impact of using a blunt-and-soft-tipped capacitive stylus in tablet settings. We thus conducted experiments to evaluate three sketch media: pen-paper, finger-tablet, and stylus-tablet based on the above tasks. We analyzed the tracing data with respect to speed and accuracy, and the quality of the free-hand sketches through a crowdsourced survey. The pen-paper and stylus-tablet media both performed significantly better than the finger-tablet medium in accuracy, while the pen-paper sketches were significantly rated higher quality compared to both tablet interfaces. A follow-up study comparing the performance of this stylus with a sharp, hard-tip version showed no significant difference in tracing performance, though participants preferred the sharp tip for sketching.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-advanced-visual-interfaces">Proceedings of the ACM Conference on Advanced Visual Interfaces • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/sketch-media/sketch-media.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Tracing and Sketching Performance using Blunt-Tipped Styli on Direct-Touch Tablets<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Direct-touch tablets are quickly replacing traditional pen-and-paper tools in many applications, but not in case of the designer’s sketchbook. In this paper, we explore the tradeoffs inherent in replacing such paper sketchbooks with digital tablets in terms of two major tasks: tracing and free-hand sketching. Given the importance of the pen for sketching, we also study the impact of using a blunt-and-soft-tipped capacitive stylus in tablet settings. We thus conducted experiments to evaluate three sketch media: pen-paper, finger-tablet, and stylus-tablet based on the above tasks. We analyzed the tracing data with respect to speed and accuracy, and the quality of the free-hand sketches through a crowdsourced survey. The pen-paper and stylus-tablet media both performed significantly better than the finger-tablet medium in accuracy, while the pen-paper sketches were significantly rated higher quality compared to both tablet interfaces. A follow-up study comparing the performance of this stylus with a sharp, hard-tip version showed no significant difference in tracing performance, though participants preferred the sharp tip for sketching.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #34&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2014b" onClick="copy('Badam2014b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2014b" style="display: none;">
                        @inproceedings{Badam2014b,
title={PolyChrome: A Cross-Device Framework for Collaborative Web Visualization},
author={Sriram Karthik Badam and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/polychrome/polychrome.pdf},
year={2014},
date={2014-07-01},
booktitle={Proceedings of the ACM Conference on Interactive Tabletops and Surfaces},
journal={Proceedings of the ACM Conference on Interactive Tabletops and Surfaces},
pages={109--118},
abstract={We present PolyChrome, an application framework for creating web-based collaborative visualizations that can span multiple devices. The framework supports (1) co-browsing new web applications as well as legacy websites with no migration costs (i.e., a distributed web browser); (2) an API to develop new web applications that can synchronize the UI state on multiple devices to support synchronous and asynchronous collaboration; and (3) maintenance of state and input events on a server to handle common issues with distributed applications such as consistency management, conflict resolution, and undo operations. We describe PolyChrome&#39;s general design, architecture, and implementation followed by application examples showcasing collaborative web visualizations created using the framework. Finally, we present performance results that suggest that PolyChrome adds minimal overhead compared to single-device applications.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-interactive-tabletops-and-surfaces">Proceedings of the ACM Conference on Interactive Tabletops and Surfaces • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/polychrome/polychrome.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> PolyChrome: A Cross-Device Framework for Collaborative Web Visualization<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present PolyChrome, an application framework for creating web-based collaborative visualizations that can span multiple devices. The framework supports (1) co-browsing new web applications as well as legacy websites with no migration costs (i.e., a distributed web browser); (2) an API to develop new web applications that can synchronize the UI state on multiple devices to support synchronous and asynchronous collaboration; and (3) maintenance of state and input events on a server to handle common issues with distributed applications such as consistency management, conflict resolution, and undo operations. We describe PolyChrome&#39;s general design, architecture, and implementation followed by application examples showcasing collaborative web visualizations created using the framework. Finally, we present performance results that suggest that PolyChrome adds minimal overhead compared to single-device applications.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #31&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Zhao2014" onClick="copy('Zhao2014')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Zhao2014" style="display: none;">
                        @inproceedings{Zhao2014,
title={skWiki: A Multimedia Sketching System for Collaborative Creativity},
author={Zhenpeng Zhao and Sriram Karthik Badam and Senthil Chandrasegaran and Deo Gun Park and Niklas Elmqvist and Lorraine Kisselburgh and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/skwiki/skwiki.pdf},
video={https://www.youtube.com/watch?v=QxtTR14EXFQ},
year={2014},
date={2014-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={1235--1244},
abstract={We present skWiki, a web application framework for collaborative creativity in digital multimedia projects, including text, hand-drawn sketches, and photographs. skWiki overcomes common drawbacks of existing wiki software by providing a rich viewer/editor architecture for all media types that is integrated into the web browser itself, thus avoiding dependence on client-side editors. Instead of files, skWiki uses the concept of paths as trajectories of persistent state over time. This model has intrinsic support for collaborative editing, including cloning, branching, and merging paths edited by multiple contributors. We demonstrate skWiki&#39;s utility using a qualitative, sketching-based user study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/skwiki/skwiki.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> skWiki: A Multimedia Sketching System for Collaborative Creativity<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhenpeng-zhao/">Zhenpeng Zhao</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/deo-gun-park/">Deo Gun Park</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present skWiki, a web application framework for collaborative creativity in digital multimedia projects, including text, hand-drawn sketches, and photographs. skWiki overcomes common drawbacks of existing wiki software by providing a rich viewer/editor architecture for all media types that is integrated into the web browser itself, thus avoiding dependence on client-side editors. Instead of files, skWiki uses the concept of paths as trajectories of persistent state over time. This model has intrinsic support for collaborative editing, including cloning, branching, and merging paths edited by multiple contributors. We demonstrate skWiki&#39;s utility using a qualitative, sketching-based user study.</p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=QxtTR14EXFQ" style="font-weight: 500"><small>📺 Video</small></a>
                      
                  </div>
              </div>
          </li>
    
      
          <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #32&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Fisher2014" onClick="copy('Fisher2014')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Fisher2014" style="display: none;">
                        @article{Fisher2014,
title={Designing Peer-to-Peer Distributed User Interfaces: Case Studies on Building Distributed Applications},
author={Eli Raymond Fisher and Sriram Karthik Badam and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/dui-design/dui-design.pdf},
year={2014},
date={2014-01-01},
journal={International Journal of Human-Computer Studies},
volume={72},
number={1},
pages={100--110},
abstract={Building a distributed user interface (DUI) application should ideally not require any additional effort beyond that necessary to build a non-distributed interface. In practice, however, DUI development is fraught with several technical challenges such as synchronization, resource management, and data transfer. In this paper, we present three case studies on building distributed user interface applications: a distributed media player for multiple displays and controls, a collaborative search system integrating a tabletop and mobile devices, and a multiplayer Tetris game for multi-surface use. While there exist several possible network architectures for such applications, our particular approach focuses on peer-to-peer (P2P) architectures. This focus leads to a number of challenges and opportunities. Drawing from these studies, we derive general challenges for P2P DUI development in terms of design, architecture, and implementation. We conclude with some general guidelines for practical DUI application development using peer-to-peer architectures.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/international-journal-of-human-computer-studies">International Journal of Human-Computer Studies • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/dui-design/dui-design.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Designing Peer-to-Peer Distributed User Interfaces: Case Studies on Building Distributed Applications<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/eli-raymond-fisher/">Eli Raymond Fisher</span></a></div>
                      
                    
                      
                        <div class="badge" style="display: flex; font-weight: 500;">Sriram Karthik Badam</div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Building a distributed user interface (DUI) application should ideally not require any additional effort beyond that necessary to build a non-distributed interface. In practice, however, DUI development is fraught with several technical challenges such as synchronization, resource management, and data transfer. In this paper, we present three case studies on building distributed user interface applications: a distributed media player for multiple displays and controls, a collaborative search system integrating a tabletop and mobile devices, and a multiplayer Tetris game for multi-surface use. While there exist several possible network architectures for such applications, our particular approach focuses on peer-to-peer (P2P) architectures. This focus leads to a number of challenges and opportunities. Drawing from these studies, we derive general challenges for P2P DUI development in terms of design, architecture, and implementation. We conclude with some general guidelines for practical DUI application development using peer-to-peer architectures.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
      
  </ul>
</div>

    </main>
    
    <script>const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),n=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(n).then((()=>{t.innerText="✔ Copied BibTeX",setInterval((()=>{t.innerText="✚ Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}</script>
  </body>
</html>