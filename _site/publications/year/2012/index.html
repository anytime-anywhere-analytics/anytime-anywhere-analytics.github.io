<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    <link rel="stylesheet" href="https://cdn.plyr.io/3.7.8/plyr.css"/> 
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003d73;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'→';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'↓'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}.sticky-header.fixed{display:none}}main{margin:auto;margin-top:25px;padding:.25rem;padding-bottom:0;max-width:120em}.sticky-header{left:0;right:0;position:relative;color:#000}.sticky-header.fixed{margin-top:0;position:fixed;top:40px;width:100%;box-sizing:border-box;z-index:0}.sticky-header.fixed.absolute{position:absolute}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;↗</sup></a>
      </nav>
    </header>
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
  <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; margin-left: -20px; margin-right: -20px; padding-left: 20px; padding-right: 20px; padding-bottom: 20px">
    <h4 style="margin-bottom: 0px; padding-top: 10px">Publications published in</h4>
    <h2 style="margin-top: 0px; margin-bottom: 0px">Year 2012</h2>
  </div>
<ul>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Javed2012c" onClick="copy('Javed2012c')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Javed2012c" style="display: none;">
                      @inproceedings{Javed2012c,
title={GravNav: Using a Gravity Model for Multi-Scale Navigation},
author={Waqas Javed and Sohaib Ghani and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/gravnav/gravnav.pdf},
year={2012},
date={2012-01-01},
journal={Proceedings of the ACM Conference on Advanced Visual Interfaces},
booktitle={Proceedings of the ACM Conference on Advanced Visual Interfaces},
pages={217--224},
abstract={We present gravity navigation (GravNav), a family of multi-scale navigation techniques that use a gravity-inspired model for assisting navigation in large visual 2D spaces based on the interest and salience of visual objects in the space. GravNav is an instance of topology-aware navigation, which makes use of the structure of the visual space to aid navigation. We have performed a controlled study comparing GravNav to standard zoom and pan navigation, with and without variable-rate zoom control. Our results show a significant improvement for GravNav over standard navigation, particularly when coupled with variable-rate zoom. We also report findings on user behavior in multi-scale navigation.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-advanced-visual-interfaces">Proceedings of the ACM Conference on Advanced Visual Interfaces • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/gravnav/gravnav.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">GravNav: Using a Gravity Model for Multi-Scale Navigation</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sohaib-ghani/">Sohaib Ghani</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">We present gravity navigation (GravNav), a family of multi-scale navigation techniques that use a gravity-inspired model for assisting navigation in large visual 2D spaces based on the interest and salience of visual objects in the space. GravNav is an instance of topology-aware navigation, which makes use of the structure of the visual space to aid navigation. We have performed a controlled study comparing GravNav to standard zoom and pan navigation, with and without variable-rate zoom control. Our results show a significant improvement for GravNav over standard navigation, particularly when coupled with variable-rate zoom. We also report findings on user behavior in multi-scale navigation.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Javed2012b" onClick="copy('Javed2012b')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Javed2012b" style="display: none;">
                      @inproceedings{Javed2012b,
title={PolyZoom: Multiscale and Multifocus Exploration in 2D Visual Spaces},
author={Waqas Javed and Sohaib Ghani and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/polyzoom/polyzoom.pdf},
year={2012},
date={2012-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={287--296},
abstract={The most common techniques for navigating in multiscale visual spaces are pan, zoom, and bird’s eye views. However, these techniques are often tedious and cumbersome to use, especially when objects of interest are located far apart. We present the PolyZoom technique where users progressively build hierarchies of focus regions, stacked on each other such that each subsequent level shows a higher magnification. Correlation graphics show the relation between parent and child viewports in the hierarchy. To validate the new technique, we compare it to standard navigation techniques in two user studies, one on multiscale visual search and the other on multifocus interaction. Results show that PolyZoom performs better than current standard techniques. },
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/polyzoom/polyzoom.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">PolyZoom: Multiscale and Multifocus Exploration in 2D Visual Spaces</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sohaib-ghani/">Sohaib Ghani</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">The most common techniques for navigating in multiscale visual spaces are pan, zoom, and bird’s eye views. However, these techniques are often tedious and cumbersome to use, especially when objects of interest are located far apart. We present the PolyZoom technique where users progressively build hierarchies of focus regions, stacked on each other such that each subsequent level shows a higher magnification. Correlation graphics show the relation between parent and child viewports in the hierarchy. To validate the new technique, we compare it to standard navigation techniques in two user studies, one on multiscale visual search and the other on multifocus interaction. Results show that PolyZoom performs better than current standard techniques. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Javed2012a" onClick="copy('Javed2012a')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Javed2012a" style="display: none;">
                      @inproceedings{Javed2012a,
title={Exploring the Design Space of Composite Visualization},
author={Waqas Javed and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/compvis/compvis.pdf},
year={2012},
date={2012-01-01},
journal={Proceedings of the IEEE Pacific Symposium on Visualization},
booktitle={Proceedings of the IEEE Pacific Symposium on Visualization},
pages={1--8},
abstract={We propose the notion of composite visualization views (CVVs) as a theoretical model that unifies the existing coordinated multiple views (CMV) paradigm with other strategies for combining visual representations in the same geometrical space. We identify five such strategies--called CVV design patterns--based on an extensive review of the literature in composite visualization. We go on to show how these design patterns can all be expressed in terms of a design space describing the correlation between two visualizations in terms of spatial mapping as well as the data relationships between items in the visualizations. We also discuss how to use this design space to suggest potential directions for future research.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-ieee-pacific-symposium-on-visualization">Proceedings of the IEEE Pacific Symposium on Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/compvis/compvis.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Exploring the Design Space of Composite Visualization</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">We propose the notion of composite visualization views (CVVs) as a theoretical model that unifies the existing coordinated multiple views (CMV) paradigm with other strategies for combining visual representations in the same geometrical space. We identify five such strategies--called CVV design patterns--based on an extensive review of the literature in composite visualization. We go on to show how these design patterns can all be expressed in terms of a design space describing the correlation between two visualizations in terms of spatial mapping as well as the data relationships between items in the visualizations. We also discuss how to use this design space to suggest potential directions for future research.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Malik2012" onClick="copy('Malik2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Malik2012" style="display: none;">
                      @inproceedings{Malik2012,
title={A Correlative Analysis Process in a Visual Analytics Environment},
author={Abish Malik and Ross Maciejewski and Yun Jang and Whitney Huang and Niklas Elmqvist and David S. Ebert},
url={https://ieeexplore.ieee.org/document/6400491},
year={2012},
date={2012-01-01},
journal={Proceedings of the IEEE Conference on Visual Analytics Science and Technology},
booktitle={Proceedings of the IEEE Conference on Visual Analytics Science and Technology},
pages={33--42},
abstract={Finding patterns and trends in spatial and temporal datasets has been a long studied problem in statistics and different domains of science. This paper presents a visual analytics approach for the interactive exploration and analysis of spatiotemporal correlations among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets, and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson&#39;s product-moment correlation coefficient and factors in the lead or lag between different datasets to detect trends and periodic patterns amongst them.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-ieee-conference-on-visual-analytics-science-and-technology">Proceedings of the IEEE Conference on Visual Analytics Science and Technology • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="https://ieeexplore.ieee.org/document/6400491"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">A Correlative Analysis Process in a Visual Analytics Environment</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/abish-malik/">Abish Malik</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/ross-maciejewski/">Ross Maciejewski</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/yun-jang/">Yun Jang</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/whitney-huang/">Whitney Huang</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Finding patterns and trends in spatial and temporal datasets has been a long studied problem in statistics and different domains of science. This paper presents a visual analytics approach for the interactive exploration and analysis of spatiotemporal correlations among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets, and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson&#39;s product-moment correlation coefficient and factors in the lead or lag between different datasets to detect trends and periodic patterns amongst them.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-McGrath2012" onClick="copy('McGrath2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-McGrath2012" style="display: none;">
                      @inproceedings{McGrath2012,
title={Branch-Explore-Merge: Facilitating Real-Time Revision Control in Collaborative Visual Exploration},
author={Will McGrath and Brian Bowman and David McCallum and Juan-David Hincapie-Ramos and Niklas Elmqvist and Pourang Irani},
url={http://www.umiacs.umd.edu/~elm/projects/bem/bem.pdf},
year={2012},
date={2012-01-01},
journal={Proceedings of the ACM Conference on Interactive Tabletops and Surfaces},
booktitle={Proceedings of the ACM Conference on Interactive Tabletops and Surfaces},
pages={235--244},
abstract={Collaborative work is characterized by participants seamlessly transitioning from working together (coupled) to working alone (decoupled). Groupware should therefore facilitate smoothly varying coupling throughout the entire collaborative session. Towards achieving such transitions for collaborative exploration and search, we propose a protocol based on managing revisions for each collaborator exploring a dataset. The protocol allows participants to diverge from the shared analysis path (branch), study the data independently (explore), and then contribute back their findings onto the shared display (merge). We apply this concept to collaborative search in multidimensional data, and propose an implementation where the public view is a tabletop display and the private views are embedded in handheld tablets. We then use this implementation to perform a qualitative user study involving a real estate dataset. Results show that participants leverage the BEM protocol, spend significant time using their private views (40% to 80% of total task time), and apply public view changes for consultation with collaborators.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-interactive-tabletops-and-surfaces">Proceedings of the ACM Conference on Interactive Tabletops and Surfaces • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/bem/bem.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Branch-Explore-Merge: Facilitating Real-Time Revision Control in Collaborative Visual Exploration</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/will-mcgrath/">Will McGrath</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/brian-bowman/">Brian Bowman</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-mccallum/">David McCallum</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/juan-david-hincapie-ramos/">Juan-David Hincapie-Ramos</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/pourang-irani/">Pourang Irani</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Collaborative work is characterized by participants seamlessly transitioning from working together (coupled) to working alone (decoupled). Groupware should therefore facilitate smoothly varying coupling throughout the entire collaborative session. Towards achieving such transitions for collaborative exploration and search, we propose a protocol based on managing revisions for each collaborator exploring a dataset. The protocol allows participants to diverge from the shared analysis path (branch), study the data independently (explore), and then contribute back their findings onto the shared display (merge). We apply this concept to collaborative search in multidimensional data, and propose an implementation where the public view is a tabletop display and the private views are embedded in handheld tablets. We then use this implementation to perform a qualitative user study involving a real estate dataset. Results show that participants leverage the BEM protocol, spend significant time using their private views (40% to 80% of total task time), and apply public view changes for consultation with collaborators.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Murugappan2012" onClick="copy('Murugappan2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Murugappan2012" style="display: none;">
                      @inproceedings{Murugappan2012,
title={Extended Multitouch: Recovering Touch Posture and Differentiating Users using a Depth Camera},
author={Sundar Murugappan and Vinayak and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/emtouch/emtouch.pdf},
year={2012},
date={2012-01-01},
journal={Proceedings of the ACM Symposium on User Interface Software and Technology},
booktitle={Proceedings of the ACM Symposium on User Interface Software and Technology},
pages={487--496},
abstract={Multitouch surfaces are becoming prevalent, but most existing technologies are only capable of detecting the user’s actual points of contact on the surface and not the identity, posture, and handedness of the user. In this paper, we define the concept of extended multitouch interaction as a richer input modality that includes all of this information. We further present a practical solution to achieve this on tabletop displays based on mounting a single commodity depth camera above a horizontal surface. This will enable us to not only detect when the surface is being touched, but also recover the user’s exact finger and hand posture, as well as distinguish between different users and their handedness. We validate our approach using two user studies, and deploy the technique in a scratchpad tool and in a pen + touch sketch tool.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-symposium-on-user-interface-software-and-technology">Proceedings of the ACM Symposium on User Interface Software and Technology • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/emtouch/emtouch.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Extended Multitouch: Recovering Touch Posture and Differentiating Users using a Depth Camera</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sundar-murugappan/">Sundar Murugappan</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/vinayak/">Vinayak</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Multitouch surfaces are becoming prevalent, but most existing technologies are only capable of detecting the user’s actual points of contact on the surface and not the identity, posture, and handedness of the user. In this paper, we define the concept of extended multitouch interaction as a richer input modality that includes all of this information. We further present a practical solution to achieve this on tabletop displays based on mounting a single commodity depth camera above a horizontal surface. This will enable us to not only detect when the surface is being touched, but also recover the user’s exact finger and hand posture, as well as distinguish between different users and their handedness. We validate our approach using two user studies, and deploy the technique in a scratchpad tool and in a pen + touch sketch tool.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Afzal2012" onClick="copy('Afzal2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Afzal2012" style="display: none;">
                      @article{Afzal2012,
title={Spatial Text Visualization Using Automatic Typographic Maps},
author={Shehzad Afzal and Ross Maciejewski and Yun Jang and Niklas Elmqvist and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/typomapvis/typomapvis.pdf},
year={2012},
date={2012-01-01},
journal={IEEE Computer Graphics and Applications (Proc. Vis/InfoVis 2012)},
volume={18},
number={12},
pages={2556-2564},
abstract={We present a method for automatically building typographic maps that merge text and spatial data into a visual representation where text alone forms the graphical features. We further show how to use this approach to visualize spatial data such as traffic density, crime rate, or demographic data. The technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints. Our sample implementation runs as a Web service, spatializing shape files from the OpenStreetMap project into typographic maps for any region.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-computer-graphics-and-applications-proc-vis-infovis-2012">IEEE Computer Graphics and Applications (Proc. Vis/InfoVis 2012) • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/typomapvis/typomapvis.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Spatial Text Visualization Using Automatic Typographic Maps</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/shehzad-afzal/">Shehzad Afzal</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/ross-maciejewski/">Ross Maciejewski</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/yun-jang/">Yun Jang</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">We present a method for automatically building typographic maps that merge text and spatial data into a visual representation where text alone forms the graphical features. We further show how to use this approach to visualize spatial data such as traffic density, crime rate, or demographic data. The technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints. Our sample implementation runs as a Web service, spatializing shape files from the OpenStreetMap project into typographic maps for any region.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Bowman2012" onClick="copy('Bowman2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Bowman2012" style="display: none;">
                      @article{Bowman2012,
title={Toward Visualization for Games: Theory, Design Space, and Patterns},
author={Brian Bowman and Niklas Elmqvist and T.J. Jankun-Kelly},
url={http://www.umiacs.umd.edu/~elm/projects/visgames/visgames.pdf},
year={2012},
date={2012-01-01},
journal={IEEE Transactions on Visualization and Computer Graphics},
volume={18},
number={11},
pages={1956-1968},
abstract={Electronic games are starting to incorporate in-game telemetry that collects data about player, team, and community performance on a massive scale, and as data begins to accumulate, so does the demand for effectively analyzing this data. In this paper, we use examples from both old and new games of different genres to explore the theory and design space of visualization for games. Drawing on these examples, we define a design space for this novel research topic and use it to formulate design patterns for how to best apply visualization technology to games. We then discuss the implications that this new framework will potentially have on the design and development of game and visualization technology in the future.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization and Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/visgames/visgames.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Toward Visualization for Games: Theory, Design Space, and Patterns</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/brian-bowman/">Brian Bowman</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/t-j-jankun-kelly/">T.J. Jankun-Kelly</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Electronic games are starting to incorporate in-game telemetry that collects data about player, team, and community performance on a massive scale, and as data begins to accumulate, so does the demand for effectively analyzing this data. In this paper, we use examples from both old and new games of different genres to explore the theory and design space of visualization for games. Drawing on these examples, we define a design space for this novel research topic and use it to formulate design patterns for how to best apply visualization technology to games. We then discuss the implications that this new framework will potentially have on the design and development of game and visualization technology in the future.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Elmqvist2012" onClick="copy('Elmqvist2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Elmqvist2012" style="display: none;">
                      @article{Elmqvist2012,
title={Leveraging Multidisciplinarity in a Visual Analytics Graduate Course},
author={Niklas Elmqvist and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/va-education/va-education.pdf},
year={2012},
date={2012-01-01},
journal={IEEE Computer Graphics and Applications},
volume={32},
number={3},
pages={84--87},
abstract={There is a growing demand in engineering, business, science, research, and industry for members with visual analytics expertise, but teaching visual analytics is challenging due to the multidisciplinary nature of the topic matter, the diverse backgrounds of the members, and the corresponding requirements on the instructor. We report some best practices from our experience teaching several offerings of a visual analytics graduate course at Purdue University where we leveraged these multidisciplinary challenges to our advantage instead of attempting to mitigate them.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-computer-graphics-and-applications">IEEE Computer Graphics and Applications • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/va-education/va-education.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Leveraging Multidisciplinarity in a Visual Analytics Graduate Course</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">There is a growing demand in engineering, business, science, research, and industry for members with visual analytics expertise, but teaching visual analytics is challenging due to the multidisciplinary nature of the topic matter, the diverse backgrounds of the members, and the corresponding requirements on the instructor. We report some best practices from our experience teaching several offerings of a visual analytics graduate course at Purdue University where we leveraged these multidisciplinary challenges to our advantage instead of attempting to mitigate them.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-3Ghani2012" onClick="copy('3Ghani2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-3Ghani2012" style="display: none;">
                      @article{3Ghani2012,
title={Perception of Animated Node-Link Diagrams for Dynamic Graphs},
author={Sohaib Ghani and Niklas Elmqvist and Ji Soo Yi},
url={http://www.umiacs.umd.edu/~elm/projects/dyngraph/dyngraph.pdf},
year={2012},
date={2012-01-01},
journal={Computer Graphics Forum},
volume={31},
number={3},
pages={1205--1214},
abstract={Effective visualization of dynamic graphs remains an open research topic, and many state-of-the-art tools use animated node-link diagrams for this purpose. Despite its intuitiveness, the effectiveness of animation in node-link diagrams has been questioned, and several empirical studies have shown that animation is not necessarily superior to static visualizations. However, the exact mechanics of perceiving animated node-link diagrams are still unclear. In this paper, we study the impact of different dynamic graph metrics on user perception of the animation. After deriving candidate visual graph metrics, we perform an exploratory user study where participants are asked to reconstruct the event sequence in animated node-link diagrams. Based on these findings, we conduct a second user study where we investigate the most important visual metrics in depth. Our findings show that node speed and target separation are prominent visual metrics to predict the performance of event sequencing tasks.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/computer-graphics-forum">Computer Graphics Forum • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/dyngraph/dyngraph.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Perception of Animated Node-Link Diagrams for Dynamic Graphs</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sohaib-ghani/">Sohaib Ghani</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/ji-soo-yi/">Ji Soo Yi</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Effective visualization of dynamic graphs remains an open research topic, and many state-of-the-art tools use animated node-link diagrams for this purpose. Despite its intuitiveness, the effectiveness of animation in node-link diagrams has been questioned, and several empirical studies have shown that animation is not necessarily superior to static visualizations. However, the exact mechanics of perceiving animated node-link diagrams are still unclear. In this paper, we study the impact of different dynamic graph metrics on user perception of the animation. After deriving candidate visual graph metrics, we perform an exploratory user study where participants are asked to reconstruct the event sequence in animated node-link diagrams. Based on these findings, we conduct a second user study where we investigate the most important visual metrics in depth. Our findings show that node speed and target separation are prominent visual metrics to predict the performance of event sequencing tasks.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Kim2012" onClick="copy('Kim2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Kim2012" style="display: none;">
                      @article{Kim2012,
title={Embodied Lenses for Collaborative Visual Queries on Tabletop Displays},
author={KyungTae Kim and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/emblens/emblens.pdf},
year={2012},
date={2012-01-01},
journal={Information Visualization},
volume={11},
number={4},
pages={336--355},
abstract={We introduce embodied lenses for visual queries on tabletop surfaces using physical interaction. The lenses are simply thin sheets of paper or transparent foil decorated with fiducial markers, allowing them to be tracked by a diffuse illumination tabletop display. The physical affordance of these embodied lenses allow them to be overlapped, causing composition in the underlying virtual space. We perform a formative evaluation to study users’ conceptual models for overlapping physical lenses. This is followed by a quantitative user study comparing performance for embodied versus purely virtual lenses. Results show that embodied lenses are equally efficient compared to purely virtual lenses, and also support tactile and eyes-free interaction. We then present several examples of the technique, including image layers, map layers, image manipulation, and multidimensional data visualization. The technique is simple, cheap, and can be integrated into many existing tabletop displays.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/emblens/emblens.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Embodied Lenses for Collaborative Visual Queries on Tabletop Displays</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/kyungtae-kim/">KyungTae Kim</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">We introduce embodied lenses for visual queries on tabletop surfaces using physical interaction. The lenses are simply thin sheets of paper or transparent foil decorated with fiducial markers, allowing them to be tracked by a diffuse illumination tabletop display. The physical affordance of these embodied lenses allow them to be overlapped, causing composition in the underlying virtual space. We perform a formative evaluation to study users’ conceptual models for overlapping physical lenses. This is followed by a quantitative user study comparing performance for embodied versus purely virtual lenses. Results show that embodied lenses are equally efficient compared to purely virtual lenses, and also support tactile and eyes-free interaction. We then present several examples of the technique, including image layers, map layers, image manipulation, and multidimensional data visualization. The technique is simple, cheap, and can be integrated into many existing tabletop displays.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Kwon2012" onClick="copy('Kwon2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Kwon2012" style="display: none;">
                      @article{Kwon2012,
title={Evaluating the Role of Time in Investigative Analysis of Document Collections},
author={Bumchul Kwon and Waqas Javed and Sohaib Ghani and Niklas Elmqvist and Ji Soo Yi and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/time-analysis/time-analysis.pdf},
year={2012},
date={2012-01-01},
journal={IEEE Transactions on Visualization and Computer Graphics},
volume={18},
number={11},
pages={199--2004},
abstract={Time is a universal and essential aspect of data in any investigative analysis. It helps analysts establish causality, build storylines from evidence, and reject infeasible hypotheses. For this reason, many investigative analysis tools provide visual representations designed for making sense of temporal data. However, the field of visual analytics still needs more evidence explaining how temporal visualization actually aids the analysis process, as well as design recommendations for how to build these visualizations. To fill this gap, we conducted an insight-based qualitative study to investigate the influence of temporal visualization on investigative analysis. We found that visualizing temporal information helped participants externalize chains of events. Another contribution of our work is the lightweight evaluation approach used to collect, visualize, and analyze insight.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization and Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/time-analysis/time-analysis.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Evaluating the Role of Time in Investigative Analysis of Document Collections</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/bumchul-kwon/">Bumchul Kwon</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sohaib-ghani/">Sohaib Ghani</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/ji-soo-yi/">Ji Soo Yi</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Time is a universal and essential aspect of data in any investigative analysis. It helps analysts establish causality, build storylines from evidence, and reject infeasible hypotheses. For this reason, many investigative analysis tools provide visual representations designed for making sense of temporal data. However, the field of visual analytics still needs more evidence explaining how temporal visualization actually aids the analysis process, as well as design recommendations for how to build these visualizations. To fill this gap, we conducted an insight-based qualitative study to investigate the influence of temporal visualization on investigative analysis. We found that visualizing temporal information helped participants externalize chains of events. Another contribution of our work is the lightweight evaluation approach used to collect, visualize, and analyze insight.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
        <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Madhavan2012" onClick="copy('Madhavan2012')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Madhavan2012" style="display: none;">
                      @article{Madhavan2012,
title={Portfolio Mining},
author={Krishna Madhavan and Mihaela Vorvoreanu and Niklas Elmqvist and Aditya Johri and Naren Ramakrishnan and G. Alan Wang and Ann McKenna},
url={https://ieeexplore.ieee.org/document/6329888},
year={2012},
date={2012-01-01},
journal={IEEE Computer},
volume={45},
number={10},
pages={95--99},
abstract={Portfolio mining facilitates the creation of actionable knowledge, catalyzes innovations, and sustains research communities.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-computer">IEEE Computer • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  <a href="https://ieeexplore.ieee.org/document/6329888"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Portfolio Mining</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/krishna-madhavan/">Krishna Madhavan</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/mihaela-vorvoreanu/">Mihaela Vorvoreanu</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/aditya-johri/">Aditya Johri</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/naren-ramakrishnan/">Naren Ramakrishnan</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/g-alan-wang/">G. Alan Wang</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/ann-mckenna/">Ann McKenna</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Portfolio mining facilitates the creation of actionable knowledge, catalyzes innovations, and sustains research communities.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
    
</ul>
</div>
    </main>
    
    <script>
      const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),o=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(o).then((()=>{t.innerText="✔ Copied BibTeX",setInterval((()=>{t.innerText="✚ Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}var stickyHeaders=function(){var e,t=function(){Array.from(e).forEach((function(t,o){if(t.originalPosition<=window.scrollY){t.classList.add("fixed"),t.style.color="white",t.style.zIndex=1e3,t.style.fontSize="0.8rem",t.style.padding="0.25rem",t.style.paddingLeft="1rem",t.style.paddingBottom="0.4rem",t.style.backgroundColor="#0273A8";var n=e[o+1];if(n){var i=n.originalPosition-t.originalHeight;t.offsetTop>=i&&(t.classList.add("absolute"),t.style.top=i+"px")}}else{t.classList.remove("fixed"),t.style.color="black",t.style.zIndex=-1e3,t.style.fontSize="1.2rem",t.style.padding="0px",t.style.backgroundColor="transparent";var r=e[o-1];r&&window.scrollY<=t.originalPosition-t.originalHeight&&(r.classList.remove("absolute"),r.style.removeProperty("top"))}}))};return{load:function(o){(e=document.querySelectorAll(o)).length>0&&(Array.from(e).forEach((function(e){var t=document.createElement("div");t.className="followWrap",e.parentNode.insertBefore(t,e),t.appendChild(e),e.originalPosition=e.offsetTop,e.originalHeight=e.offsetHeight,t.style.height=e.offsetHeight+"px"})),window.removeEventListener("scroll",t),window.addEventListener("scroll",t))}}}();document.addEventListener("DOMContentLoaded",(function(){stickyHeaders.load(".sticky-header")}));</script>
  </body>
</html>