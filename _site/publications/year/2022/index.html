<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003e75;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'→';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'↓'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}}main{margin:auto;padding:.25rem;padding-bottom:0;max-width:120em}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}.grid-container{display:flex;flex-wrap:wrap;justify-content:center;gap:10px;padding:10px}.grid-item{flex:1 1 300px;max-width:calc(50% - 20px);border-radius:.5rem;box-sizing:border-box;text-align:center;background-color:#003d73;box-shadow:0 5.1px 5.3px rgba(0,0,0,.008),0 17px 17.9px rgba(0,0,0,.012),0 76px 80px rgba(0,0,0,.02);color:#fff}.grid-item>img{border-top-left-radius:.5rem;border-top-right-radius:.5rem;margin-bottom:-10px}@media (max-width:600px){.grid-item{max-width:calc(100% - 20px)}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;↗</sup></a>
      </nav>
    </header>
    
        <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; padding-left: 20px; padding-right: 20px; padding-bottom: 13px; margin-top: 30px; margin-bottom: 0px">
        
            <h4 style="margin-bottom: 0px; padding-top 10px;">Publications published in</h4>
            <h2 style="margin-bottom: 0px; margin-top: 10px;">Year 2022</h2>
        
        </div>
    
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
<ul>
    
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #83&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Clegg2022" onClick="copy('Clegg2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Clegg2022" style="display: none;">
                        @article{Clegg2022,
title={Data Everyday as Community Driven Science: Athletes’ Critical Data Literacy Practices in Collegiate Sports Contexts},
author={Tamara L. Clegg and Keaunna Cleveland and Erianne Weight and Daniel Greene and Niklas Elmqvist},
url={https://onlinelibrary.wiley.com/doi/epdf/10.1002/tea.21842},
year={2022},
date={2022-12-01},
journal={Journal of Research in Science Teaching},
abstract={In this article, we investigate the community-driven science happening organically in elite athletics as a means of engaging a community of learners—collegiate athletes, many of whom come from underrepresented groups—in STEM. We aim to recognize the data literacy practices inherent in sports play and to explore the potential of critical data literacy practices for enabling athletes to leverage data science as a means of addressing systemic racial, equity, and justice issues inherent in sports institutions. We leverage research on critical data literacies as a lens to present case studies of three athletes at an NCAA Division 1 university spanning three different sports. We focus on athletes&#39; experiences as they engage in critical data literacy practices and the ways they welcome, adapt, resist, and critique such engagements. Our findings indicate ways in which athletes (1) readily accept data practices espoused by their coaches and sport, (2) critique and intentionally disengage from such practices, and (3) develop their own new data productions. In order to support community-driven science, our findings point to the critical role of athletics&#39; organizations in promoting athletes&#39; access to, as well as engagement and agency with data practices on their teams.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/journal-of-research-in-science-teaching">Journal of Research in Science Teaching • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/tea.21842" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Data Everyday as Community Driven Science: Athletes’ Critical Data Literacy Practices in Collegiate Sports Contexts<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/tamara-l-clegg/">Tamara L. Clegg</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/keaunna-cleveland/">Keaunna Cleveland</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/erianne-weight/">Erianne Weight</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/daniel-greene/">Daniel Greene</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">In this article, we investigate the community-driven science happening organically in elite athletics as a means of engaging a community of learners—collegiate athletes, many of whom come from underrepresented groups—in STEM. We aim to recognize the data literacy practices inherent in sports play and to explore the potential of critical data literacy practices for enabling athletes to leverage data science as a means of addressing systemic racial, equity, and justice issues inherent in sports institutions. We leverage research on critical data literacies as a lens to present case studies of three athletes at an NCAA Division 1 university spanning three different sports. We focus on athletes&#39; experiences as they engage in critical data literacy practices and the ways they welcome, adapt, resist, and critique such engagements. Our findings indicate ways in which athletes (1) readily accept data practices espoused by their coaches and sport, (2) critique and intentionally disengage from such practices, and (3) develop their own new data productions. In order to support community-driven science, our findings point to the critical role of athletics&#39; organizations in promoting athletes&#39; access to, as well as engagement and agency with data practices on their teams.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #82&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Shin2022b" onClick="copy('Shin2022b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Shin2022b" style="display: none;">
                        @article{Shin2022b,
title={A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data},
author={Sungbok Shin and Sunghyo Chung and Sanghyun Hong and Niklas Elmqvist},
url={http://users.umiacs.umd.edu/~elm/projects/scanner-deeply/scanner-deeply.pdf},
year={2022},
date={2022-10-20},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a Scanner Deeply, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper’s contribution.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/scanner-deeply/scanner-deeply.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sungbok-shin/">Sungbok Shin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sunghyo-chung/">Sunghyo Chung</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sanghyun-hong/">Sanghyun Hong</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a Scanner Deeply, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper’s contribution.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #81&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Newburger2022" onClick="copy('Newburger2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Newburger2022" style="display: none;">
                        @article{Newburger2022,
title={Fitting Bell Curves to Data Distributions using Visualization},
author={Eric Newburger and Michael Correll and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/fitting-bells/fitting-bells.pdf},
year={2022},
date={2022-10-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={idealized probability distributions, such as normal or other curves, lie at the root of confirmatory statistical tests. But how well do people understand these idealized curves? In practical terms, does the human visual system allow us to match sample data distributions with hypothesized population distributions from which those samples might have been drawn? And how do different visualization techniques impact this capability? This paper shares the results of a crowdsourced experiment that tested the ability of respondents to fit normal curves to four different data distribution visualizations: bar histograms, dotplot histograms, strip plots, and boxplots. We find that the crowd can estimate the center (mean) of a distribution with some success and little bias. We also find that people generally overestimate the standard deviation—which we dub the “umbrella effect” because people tend to want to cover the whole distribution using the curve, as if sheltering it from the heavens above—and that strip plots yield the best accuracy.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/fitting-bells/fitting-bells.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Fitting Bell Curves to Data Distributions using Visualization<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/eric-newburger/">Eric Newburger</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/michael-correll/">Michael Correll</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">idealized probability distributions, such as normal or other curves, lie at the root of confirmatory statistical tests. But how well do people understand these idealized curves? In practical terms, does the human visual system allow us to match sample data distributions with hypothesized population distributions from which those samples might have been drawn? And how do different visualization techniques impact this capability? This paper shares the results of a crowdsourced experiment that tested the ability of respondents to fit normal curves to four different data distribution visualizations: bar histograms, dotplot histograms, strip plots, and boxplots. We find that the crowd can estimate the center (mean) of a distribution with some success and little bias. We also find that people generally overestimate the standard deviation—which we dub the “umbrella effect” because people tend to want to cover the whole distribution using the curve, as if sheltering it from the heavens above—and that strip plots yield the best accuracy.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #80&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chundury2022" onClick="copy('Chundury2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chundury2022" style="display: none;">
                        @article{Chundury2022,
title={Contextual In-Situ Help for Visual Data Interfaces},
author={Pramod Chundury and Mehmet Adil Yalcin and Jonathan Crabtree and Anup Mahurkar and Lisa M. Shulman and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/contextual-help/contextual-help.pdf},
year={2022},
date={2022-09-09},
journal={Information Visualization},
abstract={As the complexity of data analysis increases, even well-designed data interfaces must guide experts in transforming their theoretical knowledge into actual features supported by the tool. This challenge is even greater for casual users who are increasingly turning to data analysis to solve everyday problems. To address this challenge, we propose data-driven, contextual, in-situ help features that can be implemented in visual data interfaces. We introduce five modes of help-seeking: (1) contextual help on selected interface elements, (2) topic listing, (3) overview, (4) guided tour, and (5) notifications. The difference between our work and general user interface help systems is that data visualization provide a unique environment for embedding context-dependent data inside on-screen messaging. We demonstrate the usefulness of such contextual help through two case studies of two visual data interfaces: Keshif and POD-Vis. We implemented and evaluated the help modes with two sets of participants, and found that directly selecting user interface elements was the most useful.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/contextual-help/contextual-help.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Contextual In-Situ Help for Visual Data Interfaces<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/pramod-chundury/">Pramod Chundury</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mehmet-adil-yalcin/">Mehmet Adil Yalcin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jonathan-crabtree/">Jonathan Crabtree</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/anup-mahurkar/">Anup Mahurkar</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lisa-m-shulman/">Lisa M. Shulman</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">As the complexity of data analysis increases, even well-designed data interfaces must guide experts in transforming their theoretical knowledge into actual features supported by the tool. This challenge is even greater for casual users who are increasingly turning to data analysis to solve everyday problems. To address this challenge, we propose data-driven, contextual, in-situ help features that can be implemented in visual data interfaces. We introduce five modes of help-seeking: (1) contextual help on selected interface elements, (2) topic listing, (3) overview, (4) guided tour, and (5) notifications. The difference between our work and general user interface help systems is that data visualization provide a unique environment for embedding context-dependent data inside on-screen messaging. We demonstrate the usefulness of such contextual help through two case studies of two visual data interfaces: Keshif and POD-Vis. We implemented and evaluated the help modes with two sets of participants, and found that directly selecting user interface elements was the most useful.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #79&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Patnaik2022" onClick="copy('Patnaik2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Patnaik2022" style="display: none;">
                        @article{Patnaik2022,
title={Sensemaking Sans Power: Interactive Data Visualization Using Color-Changing Ink},
author={Biswaksen Patnaik and Huaishu Peng and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/sense-sans-power/sense-sans-power.pdf},
year={2022},
date={2022-09-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={We present an approach for interactively visualizing data using color-changing inks without the need for electronic displays or computers. Color-changing inks are a family of physical inks that change their color characteristics in response to an external stimulus such as heat, UV light, water, and pressure. Visualizations created using color-changing inks can embed interactivity in printed material without external computational media. In this paper, we survey current color-changing ink technology and then use these findings to derive a framework for how it can be used to construct interactive data representations. We also enumerate the interaction techniques possible using this technology. We then show some examples of how to use color-changing ink to create interactive visualizations on paper. While obviously limited in scope to situations where no power or computing is present, or as a complement to digital displays, our findings can be employed for paper, data physicalization, and embedded visualizations.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/sense-sans-power/sense-sans-power.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Sensemaking Sans Power: Interactive Data Visualization Using Color-Changing Ink<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/biswaksen-patnaik/">Biswaksen Patnaik</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/huaishu-peng/">Huaishu Peng</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present an approach for interactively visualizing data using color-changing inks without the need for electronic displays or computers. Color-changing inks are a family of physical inks that change their color characteristics in response to an external stimulus such as heat, UV light, water, and pressure. Visualizations created using color-changing inks can embed interactivity in printed material without external computational media. In this paper, we survey current color-changing ink technology and then use these findings to derive a framework for how it can be used to construct interactive data representations. We also enumerate the interaction techniques possible using this technology. We then show some examples of how to use color-changing ink to create interactive visualizations on paper. While obviously limited in scope to situations where no power or computing is present, or as a complement to digital displays, our findings can be employed for paper, data physicalization, and embedded visualizations.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #78&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2022" onClick="copy('Badam2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2022" style="display: none;">
                        @article{Badam2022,
title={Integrating Annotations into Multidimensional Visual Dashboards},
author={Sriram Karthik Badam and Senthil Chandrasegaran and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/facetnotes/facetnotes.pdf},
year={2022},
date={2022-05-10},
journal={Information Visualization},
volume={21},
number={3},
pages={270--284},
abstract={Multidimensional data is often visualized using coordinated multiple views in an interactive dashboard. However, unlike in infographics where text is often a central part of the presentation, there is currently little knowledge of how to best integrate text and annotations in a visualization dashboard. In this paper, we explore a technique called FacetNotes for presenting these textual annotations on top of any visualization within a dashboard irrespective of the scale of data shown or the design of visual representation itself. FacetNotes does so by grouping and ordering the textual annotations based on properties of (1) the individual data points associated with the annotations, and (2) the target visual representation on which they should be shown. We present this technique along with a set of user interface features and guidelines to apply it to visualization interfaces. We also demonstrate FacetNotes in a custom visual dashboard interface. Finally, results from a user study of FacetNotes show that the technique improves the scope and complexity of insights developed during visual exploration.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/facetnotes/facetnotes.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Integrating Annotations into Multidimensional Visual Dashboards<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Multidimensional data is often visualized using coordinated multiple views in an interactive dashboard. However, unlike in infographics where text is often a central part of the presentation, there is currently little knowledge of how to best integrate text and annotations in a visualization dashboard. In this paper, we explore a technique called FacetNotes for presenting these textual annotations on top of any visualization within a dashboard irrespective of the scale of data shown or the design of visual representation itself. FacetNotes does so by grouping and ordering the textual annotations based on properties of (1) the individual data points associated with the annotations, and (2) the target visual representation on which they should be shown. We present this technique along with a set of user interface features and guidelines to apply it to visualization interfaces. We also demonstrate FacetNotes in a custom visual dashboard interface. Finally, results from a user study of FacetNotes show that the technique improves the scope and complexity of insights developed during visual exploration.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #77&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Shin2022" onClick="copy('Shin2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Shin2022" style="display: none;">
                        @article{Shin2022,
title={Roslingifier: Semi-Automated Storytelling for Animated Scatterplots},
author={Minjeong Shin and Joohee Kim and Yunha Han and Lexing Xie and Mitchell Whitelaw and Bum Chul Kwon and Sungahn Ko and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/roslingifier/roslingifier.pdf},
year={2022},
date={2022-05-10},
journal={IEEE Transactions on Visualization and Computer Graphics},
abstract={We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization and Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/roslingifier/roslingifier.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Roslingifier: Semi-Automated Storytelling for Animated Scatterplots<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/minjeong-shin/">Minjeong Shin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/joohee-kim/">Joohee Kim</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/yunha-han/">Yunha Han</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lexing-xie/">Lexing Xie</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mitchell-whitelaw/">Mitchell Whitelaw</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/bum-chul-kwon/">Bum Chul Kwon</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sungahn-ko/">Sungahn Ko</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #60&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Hubenschmid2022" onClick="copy('Hubenschmid2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Hubenschmid2022" style="display: none;">
                        @inproceedings{Hubenschmid2022,
title={ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies},
author={Sebastian Hubenschmid and Jonathan Wieland and Daniel Immanuel Fink and Andrea Batch and Johannes Zagermann and Niklas Elmqvist and Harald Reiterer},
url={https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf},
year={2022},
date={2022-05-10},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={24:1--24:20},
publisher={ACM},
address={New York, NY, USA},
abstract={The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sebastian-hubenschmid/">Sebastian Hubenschmid</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jonathan-wieland/">Jonathan Wieland</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/daniel-immanuel-fink/">Daniel Immanuel Fink</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andrea-batch/">Andrea Batch</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/johannes-zagermann/">Johannes Zagermann</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/harald-reiterer/">Harald Reiterer</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #59&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Hoque2022" onClick="copy('Hoque2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Hoque2022" style="display: none;">
                        @inproceedings{Hoque2022,
title={DramatVis Personae: Visual Text Analytics for identifying Social Biases in Creative Writing},
author={Md Naimul Hoque and Bhavya Ghai and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/dvp/dvp.pdf},
year={2022},
date={2022-05-10},
journal={Proceedings of the ACM Conference on Designing Interactive Systems},
booktitle={Proceedings of the ACM Conference on Designing Interactive Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Implicit biases and stereotypes are often pervasive in different forms of creative writing such as novels, screenplays, and children&#39;s books. To understand the kind of biases writers are concerned about and how they mitigate those in their writing, we conducted formative interviews with nine writers. The interviews suggested that despite a writer&#39;s best interest, tracking and managing implicit biases such as a lack of agency, supporting or submissive roles, or harmful language for characters representing marginalized groups is challenging as the story becomes longer and complicated. Based on the interviews, we developed DramatVis Personae (DVP), a visual analytics tool that allows writers to assign social identities to characters, and evaluate how characters and different intersectional social identities are represented in the story. To evaluate DVP, we first conducted think-aloud sessions with three writers and found that DVP is easy-to-use, naturally integrates into the writing process, and could potentially help writers in several critical bias identification tasks. We then conducted a follow-up user study with 11 writers and found that participants could answer questions related to bias detection more efficiently using DVP in comparison to a simple text editor. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-designing-interactive-systems">Proceedings of the ACM Conference on Designing Interactive Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/dvp/dvp.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> DramatVis Personae: Visual Text Analytics for identifying Social Biases in Creative Writing<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/md-naimul-hoque/">Md Naimul Hoque</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/bhavya-ghai/">Bhavya Ghai</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Implicit biases and stereotypes are often pervasive in different forms of creative writing such as novels, screenplays, and children&#39;s books. To understand the kind of biases writers are concerned about and how they mitigate those in their writing, we conducted formative interviews with nine writers. The interviews suggested that despite a writer&#39;s best interest, tracking and managing implicit biases such as a lack of agency, supporting or submissive roles, or harmful language for characters representing marginalized groups is challenging as the story becomes longer and complicated. Based on the interviews, we developed DramatVis Personae (DVP), a visual analytics tool that allows writers to assign social identities to characters, and evaluate how characters and different intersectional social identities are represented in the story. To evaluate DVP, we first conducted think-aloud sessions with three writers and found that DVP is easy-to-use, naturally integrates into the writing process, and could potentially help writers in several critical bias identification tasks. We then conducted a follow-up user study with 11 writers and found that participants could answer questions related to bias detection more efficiently using DVP in comparison to a simple text editor. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #76&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chundury2021" onClick="copy('Chundury2021')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chundury2021" style="display: none;">
                        @article{Chundury2021,
title={Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study},
author={Pramod Chundury and Biswaksen Patnaik and Yasmin Reyazuddin and Christine W. Tang and Jonathan Lazar and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/access-vis/access-vis.pdf},
year={2022},
date={2022-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
volume={28},
number={1},
pages={1084--1094},
abstract={For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&amp;M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&amp;M experts---all of them blind---to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible---using sonification and auralization. However, our experts recommended supporting a combination of senses---sound and touch---to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/access-vis/access-vis.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/pramod-chundury/">Pramod Chundury</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/biswaksen-patnaik/">Biswaksen Patnaik</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/yasmin-reyazuddin/">Yasmin Reyazuddin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/christine-w-tang/">Christine W. Tang</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jonathan-lazar/">Jonathan Lazar</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&amp;M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&amp;M experts---all of them blind---to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible---using sonification and auralization. However, our experts recommended supporting a combination of senses---sound and touch---to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
</ul>
</div>
    </main>
    
    <script>const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),n=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(n).then((()=>{t.innerText="✔ Copied BibTeX",setInterval((()=>{t.innerText="✚ Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}</script>
  </body>
</html>