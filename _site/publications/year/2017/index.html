<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003e75;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'→';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'↓'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}}main{margin:auto;padding:.25rem;padding-bottom:0;max-width:120em}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}.grid-container{display:flex;flex-wrap:wrap;justify-content:center;gap:10px;padding:10px}.grid-item{flex:1 1 300px;max-width:calc(50% - 20px);border-radius:.5rem;box-sizing:border-box;text-align:center;background-color:#003d73;box-shadow:0 5.1px 5.3px rgba(0,0,0,.008),0 17px 17.9px rgba(0,0,0,.012),0 76px 80px rgba(0,0,0,.02);color:#fff}.grid-item>img{border-top-left-radius:.5rem;border-top-right-radius:.5rem;margin-bottom:-10px}@media (max-width:600px){.grid-item{max-width:calc(100% - 20px)}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;↗</sup></a>
      </nav>
    </header>
    
        <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; padding-left: 20px; padding-right: 20px; padding-bottom: 13px; margin-top: 30px; margin-bottom: 0px">
        
            <h4 style="margin-bottom: 0px; padding-top 10px;">Publications published in</h4>
            <h2 style="margin-bottom: 0px; margin-top: 10px;">Year 2017</h2>
        
        </div>
    
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
<ul>
    
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #53&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Batch2017" onClick="copy('Batch2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Batch2017" style="display: none;">
                        @article{Batch2017,
title={The Interactive Visualization Gap in Initial Exploratory Data Analysis},
author={Andrea Batch and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/visgap/visgap.pdf},
year={2017},
date={2017-10-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a “visualization gap” during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/visgap/visgap.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> The Interactive Visualization Gap in Initial Exploratory Data Analysis<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andrea-batch/">Andrea Batch</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a “visualization gap” during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #52&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Park2017" onClick="copy('Park2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Park2017" style="display: none;">
                        @article{Park2017,
title={ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding},
author={Deok Gun Park and Seungyeon Kim and Jurim Lee and Jaegul Choo and Nicholas Diakopoulos and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/conceptvector/conceptvector.pdf},
year={2017},
date={2017-10-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/conceptvector/conceptvector.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/deok-gun-park/">Deok Gun Park</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/seungyeon-kim/">Seungyeon Kim</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jurim-lee/">Jurim Lee</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jaegul-choo/">Jaegul Choo</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/nicholas-diakopoulos/">Nicholas Diakopoulos</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #51&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2017bb" onClick="copy('Badam2017bb')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2017bb" style="display: none;">
                        @article{Badam2017bb,
title={Visfer: Camera-based Visual Data Transfer for Cross-Device Visualization},
author={Sriram Karthik Badam and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/qrvis/visfer.pdf},
year={2017},
date={2017-09-08},
journal={Information Visualization},
abstract={Going beyond the desktop to leverage novel devices—such as smartphones, tablets, or large displays—for visual sensemaking typically requires supporting extraneous operations for device discovery, interaction sharing, and view management. Such operations can be time-consuming and tedious, and distract the user from the actual analysis. Embodied interaction models in these multi-device environments can take advantage of the natural interaction and physicality afforded by multimodal devices and help effectively carry out these operations in visual sensemaking. In this paper, we present cross-device interaction models for visualization spaces, that are embodied in nature, by conducting a user study to elicit actions from participants that could trigger a portrayed effect of sharing visualizations (and therefore information) across devices. We then explore one common interaction style from this design elicitation called Visfer, a technique for effortlessly sharing visualizations across devices using the visual medium. More specifically, this technique involves taking pictures of visualizations, or rather the QR codes augmenting them, on a display using the built-in camera on a handheld device. Our contributions include a conceptual framework for cross-device interaction and the Visfer technique itself, as well as transformation guidelines to exploit the capabilities of each specific device and a web framework for encoding visualization components into animated QR codes, which capture multiple frames of QR codes to embed more information. Beyond this, we also present the results from a performance evaluation for the visual data transfer enabled by Visfer. We end the paper by presenting the application examples of our Visfer framework. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/information-visualization">Information Visualization • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/qrvis/visfer.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Visfer: Camera-based Visual Data Transfer for Cross-Device Visualization<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Going beyond the desktop to leverage novel devices—such as smartphones, tablets, or large displays—for visual sensemaking typically requires supporting extraneous operations for device discovery, interaction sharing, and view management. Such operations can be time-consuming and tedious, and distract the user from the actual analysis. Embodied interaction models in these multi-device environments can take advantage of the natural interaction and physicality afforded by multimodal devices and help effectively carry out these operations in visual sensemaking. In this paper, we present cross-device interaction models for visualization spaces, that are embodied in nature, by conducting a user study to elicit actions from participants that could trigger a portrayed effect of sharing visualizations (and therefore information) across devices. We then explore one common interaction style from this design elicitation called Visfer, a technique for effortlessly sharing visualizations across devices using the visual medium. More specifically, this technique involves taking pictures of visualizations, or rather the QR codes augmenting them, on a display using the built-in camera on a handheld device. Our contributions include a conceptual framework for cross-device interaction and the Visfer technique itself, as well as transformation guidelines to exploit the capabilities of each specific device and a web framework for encoding visualization components into animated QR codes, which capture multiple frames of QR codes to embed more information. Beyond this, we also present the results from a performance evaluation for the visual data transfer enabled by Visfer. We end the paper by presenting the application examples of our Visfer framework. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #50&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Yalcin2017b" onClick="copy('Yalcin2017b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Yalcin2017b" style="display: none;">
                        @article{Yalcin2017b,
title={Keshif: Rapid and Expressive Tabular Data Exploration for Novices},
author={Mehmet Adil Yalcin and Niklas Elmqvist and Benjamin B. Bederson},
url={http://www.umiacs.umd.edu/~elm/projects/keshif/keshif.pdf},
year={2017},
date={2017-05-19},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={General purpose graphical interfaces for data exploration are typically based on manual visualization and interaction specifications. While designing manual specification can be very expressive, it demands high efforts to make effective decisions, therefore reducing exploratory speed. Instead, principled automated designs can increase exploratory speed, decrease learning efforts, help avoid ineffective decisions, and therefore better support data analytics novices. Towards these goals, we present Keshif, a new systematic design for tabular data exploration. To summarize a given dataset, Keshif aggregates records by value within attribute summaries, and visualizes aggregate characteristics using a consistent design based on data types. To reveal data distribution details, Keshif features three complementary linked selections: highlighting, filtering, and comparison. Keshif further increases expressiveness through aggregate metrics, absolute/part-of scale modes, calculated attributes, and saved selections, all working in synchrony. Its automated design approach also simplifies authoring of dashboards composed of summaries and individual records from raw data using fluid interaction. We show examples selected from 160+ datasets from diverse domains. Our study with novices shows that after exploring raw data for 15 minutes, our participants reached close to 30 data insights on average, comparable to other studies with skilled users using more complex tools.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/keshif/keshif.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Keshif: Rapid and Expressive Tabular Data Exploration for Novices<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mehmet-adil-yalcin/">Mehmet Adil Yalcin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/benjamin-b-bederson/">Benjamin B. Bederson</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">General purpose graphical interfaces for data exploration are typically based on manual visualization and interaction specifications. While designing manual specification can be very expressive, it demands high efforts to make effective decisions, therefore reducing exploratory speed. Instead, principled automated designs can increase exploratory speed, decrease learning efforts, help avoid ineffective decisions, and therefore better support data analytics novices. Towards these goals, we present Keshif, a new systematic design for tabular data exploration. To summarize a given dataset, Keshif aggregates records by value within attribute summaries, and visualizes aggregate characteristics using a consistent design based on data types. To reveal data distribution details, Keshif features three complementary linked selections: highlighting, filtering, and comparison. Keshif further increases expressiveness through aggregate metrics, absolute/part-of scale modes, calculated attributes, and saved selections, all working in synchrony. Its automated design approach also simplifies authoring of dashboards composed of summaries and individual records from raw data using fluid interaction. We show examples selected from 160+ datasets from diverse domains. Our study with novices shows that after exploring raw data for 15 minutes, our participants reached close to 30 data insights on average, comparable to other studies with skilled users using more complex tools.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #46&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Yalcin2017" onClick="copy('Yalcin2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Yalcin2017" style="display: none;">
                        @inproceedings{Yalcin2017,
title={Raising the Bars: Evaluating Treemaps vs. Wrapped Bars for Dense Visualization of Sorted Numeric Data},
author={Mehmet Adil Yalcin and Niklas Elmqvist and Benjamin B. Bederson},
url={http://www.umiacs.umd.edu/~elm/projects/raising-bars/RaisingTheBars-GI2017.pdf},
year={2017},
date={2017-05-15},
journal={Proceedings of Graphics Interface},
booktitle={Proceedings of Graphics Interface},
abstract={A standard (single-column) bar chart can effectively visualize a sorted list of numeric records. However, the chart height limits the number of visible records. To show more records, the bars could be made thinner (which could hinder identifying records individually), and scrolling requires interaction to see the overview. Treemaps have been used in practice in non-hierarchical settings for dense visualization of numeric data. Alternatively, we consider wrapped bars, a multi-column bar chart that uses length instead of area to encode numeric values. We compare treemaps and wrapped bars based on their design characteristics, and graphical perception performance for comparison, ranking, and overview tasks using crowdsourced experiments. Our analysis found that wrapped bars perceptually outperform treemaps in all three tasks for dense visualization of non-hierarchical, sorted numeric data.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-graphics-interface">Proceedings of Graphics Interface • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/raising-bars/RaisingTheBars-GI2017.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Raising the Bars: Evaluating Treemaps vs. Wrapped Bars for Dense Visualization of Sorted Numeric Data<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mehmet-adil-yalcin/">Mehmet Adil Yalcin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/benjamin-b-bederson/">Benjamin B. Bederson</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">A standard (single-column) bar chart can effectively visualize a sorted list of numeric records. However, the chart height limits the number of visible records. To show more records, the bars could be made thinner (which could hinder identifying records individually), and scrolling requires interaction to see the overview. Treemaps have been used in practice in non-hierarchical settings for dense visualization of numeric data. Alternatively, we consider wrapped bars, a multi-column bar chart that uses length instead of area to encode numeric values. We compare treemaps and wrapped bars based on their design characteristics, and graphical perception performance for comparison, ranking, and overview tasks using crowdsourced experiments. Our analysis found that wrapped bars perceptually outperform treemaps in all three tasks for dense visualization of non-hierarchical, sorted numeric data.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #45&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chandrasegaran2017" onClick="copy('Chandrasegaran2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chandrasegaran2017" style="display: none;">
                        @inproceedings{Chandrasegaran2017,
title={Merging Sketches for Creative Design Exploration: An Evaluation of Physical and Cognitive Operations},
author={Senthil Chandrasegaran and Sriram Karthik Badam and Ninger Zhou and Zhenpeng Zhao and Lorraine Kisselburgh and Kylie Peppler and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/merge-study/merge-study.pdf},
year={2017},
date={2017-05-15},
journal={Proceedings of Graphics Interface},
booktitle={Proceedings of Graphics Interface},
abstract={Despite its grounding in creativity techniques, merging multiple source sketches to create new ideas has received scant attention in design literature. In this paper, we identify the physical operations that in merging sketch components. We also introduce cognitive operations of reuse, repurpose, refactor, and reinterpret, and explore their relevance to creative design. To examine the relationship of cognitive operations, physical techniques, and creative sketch outcomes, we conducted a qualitative user study where student designers merged existing sketches to generate either an alternative design, or an unrelated new design. We compared two digital selection techniques: freeform selection, and a stroke-cluster-based &quot;object select&quot; technique. The resulting merge sketches were subjected to crowdsourced evaluation of these sketches, and manual coding for the use of cognitive operations. Our findings establish a firm connection between the proposed cognitive operations and the context and outcome of creative tasks. Key findings indicate that reinterpret cognitive operations correlate strongly with creativity in merged sketches, while reuse operations correlate negatively with creativity. Furthermore, freeform selection techniques are preferred significantly by designers. We discuss the empirical contributions of understanding the use of cognitive operations during design exploration, and the practical implications for designing interfaces in digital tools that facilitate creativity in merging sketches. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-graphics-interface">Proceedings of Graphics Interface • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/merge-study/merge-study.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Merging Sketches for Creative Design Exploration: An Evaluation of Physical and Cognitive Operations<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ninger-zhou/">Ninger Zhou</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhenpeng-zhao/">Zhenpeng Zhao</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/kylie-peppler/">Kylie Peppler</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Despite its grounding in creativity techniques, merging multiple source sketches to create new ideas has received scant attention in design literature. In this paper, we identify the physical operations that in merging sketch components. We also introduce cognitive operations of reuse, repurpose, refactor, and reinterpret, and explore their relevance to creative design. To examine the relationship of cognitive operations, physical techniques, and creative sketch outcomes, we conducted a qualitative user study where student designers merged existing sketches to generate either an alternative design, or an unrelated new design. We compared two digital selection techniques: freeform selection, and a stroke-cluster-based &quot;object select&quot; technique. The resulting merge sketches were subjected to crowdsourced evaluation of these sketches, and manual coding for the use of cognitive operations. Our findings establish a firm connection between the proposed cognitive operations and the context and outcome of creative tasks. Key findings indicate that reinterpret cognitive operations correlate strongly with creativity in merged sketches, while reuse operations correlate negatively with creativity. Furthermore, freeform selection techniques are preferred significantly by designers. We discuss the empirical contributions of understanding the use of cognitive operations during design exploration, and the practical implications for designing interfaces in digital tools that facilitate creativity in merging sketches. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #44&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2017" onClick="copy('Badam2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2017" style="display: none;">
                        @inproceedings{Badam2017,
title={Supporting Team-First Visual Analytics through Group Activity Representations},
author={Sriram Karthik Badam and Zehua Zheng and Emily Wall and Alex Endert and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/group-awareness/group-awareness.pdf},
year={2017},
date={2017-05-15},
journal={Proceedings of Graphics Interface},
booktitle={Proceedings of Graphics Interface},
abstract={Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of &quot;team-first&quot; visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-graphics-interface">Proceedings of Graphics Interface • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/group-awareness/group-awareness.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Supporting Team-First Visual Analytics through Group Activity Representations<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zehua-zheng/">Zehua Zheng</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/emily-wall/">Emily Wall</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/alex-endert/">Alex Endert</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of &quot;team-first&quot; visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #49&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2017b" onClick="copy('Badam2017b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2017b" style="display: none;">
                        @article{Badam2017b,
title={Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics},
author={Sriram Karthik Badam and Niklas Elmqvist and Jean-Daniel Fekete},
url={http://www.umiacs.umd.edu/~elm/projects/insightsfeed/insightsfeed.pdf},
year={2017},
date={2017-05-15},
journal={Computer Graphics Forum},
volume={36},
abstract={Progressive visual analytics (PVA) has emerged in recent years to manage the latency of data analysis systems. When analysis is performed progressively, rough estimates of the results are generated quickly and are then improved over time. Analysts can therefore monitor the progression of the results, steer the analysis algorithms, and make early decisions if the estimates provide a convincing picture. In this article, we describe interface design guidelines for helping users understand progressively updating results and make early decisions based on progressive estimates. To illustrate our ideas, we present a prototype PVA tool called InsightsFeed for exploring Twitter data at scale. As validation, we investigate the tradeoffs of our tool when exploring a Twitter dataset in a user study. We report the usage patterns in making early decisions using the user interface, guiding computational methods, and exploring different subsets of the dataset, compared to sequential analysis without progression.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/computer-graphics-forum">Computer Graphics Forum • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/insightsfeed/insightsfeed.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jean-daniel-fekete/">Jean-Daniel Fekete</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Progressive visual analytics (PVA) has emerged in recent years to manage the latency of data analysis systems. When analysis is performed progressively, rough estimates of the results are generated quickly and are then improved over time. Analysts can therefore monitor the progression of the results, steer the analysis algorithms, and make early decisions if the estimates provide a convincing picture. In this article, we describe interface design guidelines for helping users understand progressively updating results and make early decisions based on progressive estimates. To illustrate our ideas, we present a prototype PVA tool called InsightsFeed for exploring Twitter data at scale. As validation, we investigate the tradeoffs of our tool when exploring a Twitter dataset in a user study. We report the usage patterns in making early decisions using the user interface, guiding computational methods, and exploring different subsets of the dataset, compared to sequential analysis without progression.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #48&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chandrasegaran2017c" onClick="copy('Chandrasegaran2017c')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chandrasegaran2017c" style="display: none;">
                        @article{Chandrasegaran2017c,
title={Integrating Visual Analytics Support for Grounded Theory Practice in Qualitative Text Analysis},
author={Senthil Chandrasegaran and Sriram Karthik Badam and Lorraine Kisselburgh and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/gthelper/gthelper.pdf},
year={2017},
date={2017-05-15},
journal={Computer Graphics Forum},
volume={36},
abstract={We present an argument for using visual analytics to aid Grounded Theory methodologies in qualitative data analysis. Grounded theory methods involve the inductive analysis of data to generate novel insights and theoretical constructs. Making sense of unstructured text data is uniquely suited for visual analytics. Using natural language processing techniques such as parts-of-speech tagging, retrieving information content, and topic modeling, different parts of the data can be structured and semantically associated, and interactively explored, thereby providing conceptual depth to the guided discovery process. We review grounded theory methods and identify processes that can be enhanced through visual analytic techniques. Next, we develop an interface for qualitative text analysis, and evaluate our design with qualitative research practitioners who analyze texts with and without visual analytics support. The results of our study suggest how visual analytics can be incorporated into qualitative data analysis tools, and the analytic and interpretive benefits that can result.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/computer-graphics-forum">Computer Graphics Forum • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/gthelper/gthelper.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Integrating Visual Analytics Support for Grounded Theory Practice in Qualitative Text Analysis<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present an argument for using visual analytics to aid Grounded Theory methodologies in qualitative data analysis. Grounded theory methods involve the inductive analysis of data to generate novel insights and theoretical constructs. Making sense of unstructured text data is uniquely suited for visual analytics. Using natural language processing techniques such as parts-of-speech tagging, retrieving information content, and topic modeling, different parts of the data can be structured and semantically associated, and interactively explored, thereby providing conceptual depth to the guided discovery process. We review grounded theory methods and identify processes that can be enhanced through visual analytic techniques. Next, we develop an interface for qualitative text analysis, and evaluate our design with qualitative research practitioners who analyze texts with and without visual analytics support. The results of our study suggest how visual analytics can be incorporated into qualitative data analysis tools, and the analytic and interpretive benefits that can result.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #43&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Zhang2017" onClick="copy('Zhang2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Zhang2017" style="display: none;">
                        @inproceedings{Zhang2017,
title={TopoGroups: Context-Preserving Visual Illustration of Multi-Scale Spatial Aggregates},
author={Jiawei Zhang and Abish Malik and Benjamin Ahlbrand and Niklas Elmqvist and Ross Maciejewski and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/topogroups/topogroups.pdf},
year={2017},
date={2017-05-08},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={2940--2951},
publisher={ACM},
abstract={Spatial datasets, such as tweets in a geographic area, often exhibit different distribution patterns at multiple levels of scale, such as live updates about events occurring in very specific locations on the map. Navigating in such multi-scale data-rich spaces is often inefficient, requires users to choose between overview or detail information, and does not support identifying spatial patterns at varying scales. In this paper, we propose TopoGroups, a novel context-preserving technique that aggregates spatial data into hierarchical clusters to improve exploration and navigation at multiple spatial scales. The technique uses a boundary distortion algorithm to minimize the visual clutter caused by overlapping aggregates. Our user study explores multiple visual encoding strategies for TopoGroups including color, transparency, shading, and shapes in order to convey the hierarchical and statistical information of the geographical aggregates at different scales.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/topogroups/topogroups.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> TopoGroups: Context-Preserving Visual Illustration of Multi-Scale Spatial Aggregates<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jiawei-zhang/">Jiawei Zhang</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/abish-malik/">Abish Malik</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/benjamin-ahlbrand/">Benjamin Ahlbrand</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ross-maciejewski/">Ross Maciejewski</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Spatial datasets, such as tweets in a geographic area, often exhibit different distribution patterns at multiple levels of scale, such as live updates about events occurring in very specific locations on the map. Navigating in such multi-scale data-rich spaces is often inefficient, requires users to choose between overview or detail information, and does not support identifying spatial patterns at varying scales. In this paper, we propose TopoGroups, a novel context-preserving technique that aggregates spatial data into hierarchical clusters to improve exploration and navigation at multiple spatial scales. The technique uses a boundary distortion algorithm to minimize the visual clutter caused by overlapping aggregates. Our user study explores multiple visual encoding strategies for TopoGroups including color, transparency, shading, and shapes in order to convey the hierarchical and statistical information of the geographical aggregates at different scales.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #42&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Piya2017" onClick="copy('Piya2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Piya2017" style="display: none;">
                        @inproceedings{Piya2017,
title={Co-3Deator: A Team-First Collaborative 3D Design ideation Tool},
author={Cecil Piya and Vinayak and Senthil Chandrasegaran and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/co3deator/co3deator.pdf},
year={2017},
date={2017-05-08},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={6581--6592},
abstract={We present CO-3DEATOR, a sketch-based collaborative 3D modeling system based on the notion of “team-first” ideation tools, where the needs and processes of the entire design team come before that of an individual designer. Co-3Deator includes two specific team-first features: a concept component hierarchy which provides a design representation suitable for multi-level sharing and reusing of design information, and a collaborative design explorer for storing, viewing, and accessing hierarchical design data during collaborative design activities. We conduct two controlled user studies, one with individual designers to elicit the form and functionality of the collaborative design explorer, and the other with design teams to evaluate the utility of the concept component hierarchy and design explorer towards collaborative design ideation. Our results support our rationale for both of the proposed team-first collaboration mechanisms and suggest further ways to streamline collaborative design.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/co3deator/co3deator.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Co-3Deator: A Team-First Collaborative 3D Design ideation Tool<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/cecil-piya/">Cecil Piya</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/vinayak/">Vinayak</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present CO-3DEATOR, a sketch-based collaborative 3D modeling system based on the notion of “team-first” ideation tools, where the needs and processes of the entire design team come before that of an individual designer. Co-3Deator includes two specific team-first features: a concept component hierarchy which provides a design representation suitable for multi-level sharing and reusing of design information, and a collaborative design explorer for storing, viewing, and accessing hierarchical design data during collaborative design activities. We conduct two controlled user studies, one with individual designers to elicit the form and functionality of the collaborative design explorer, and the other with design teams to evaluate the utility of the concept component hierarchy and design explorer towards collaborative design ideation. Our results support our rationale for both of the proposed team-first collaboration mechanisms and suggest further ways to streamline collaborative design.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
     <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Journal Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #47&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chandrasegaran2017b" onClick="copy('Chandrasegaran2017b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chandrasegaran2017b" style="display: none;">
                        @article{Chandrasegaran2017b,
title={VizScribe: A Visual Analytics Approach to Understand Designer Behavior},
author={Senthil Chandrasegaran and Sriram Karthik Badam and Lorraine Kisselburgh and Kylie Peppler and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/vizscribe/vizscribe.pdf},
year={2017},
date={2017-01-02},
journal={International Journal of Human-Computer Interaction},
volume={100},
pages={66--80},
abstract={Design protocol analysis is a technique to understand designers’ cognitive processes by analyzing sequences of observations on their behavior. These observations typically use audio, video, and transcript data in order to gain insights into the designer&#39;s behavior and the design process. The recent availability of sophisticated sensing technology has made such data highly multimodal, requiring more flexible protocol analysis tools. To address this need, we present VizScribe, a visual analytics framework that employs multiple coordinated multiple views that enable the viewing of such data from different perspectives. VizScribe allows designers to create, customize, and extend interactive visualizations for design protocol data such as video, transcripts, sketches, sensor data, and user logs. User studies where design researchers used VizScribe for protocol analysis indicated that the linked views and interactive navigation offered by VizScribe afforded the researchers multiple, useful ways to approach and interpret such multimodal data.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/international-journal-of-human-computer-interaction">International Journal of Human-Computer Interaction • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/vizscribe/vizscribe.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> VizScribe: A Visual Analytics Approach to Understand Designer Behavior<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/kylie-peppler/">Kylie Peppler</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Design protocol analysis is a technique to understand designers’ cognitive processes by analyzing sequences of observations on their behavior. These observations typically use audio, video, and transcript data in order to gain insights into the designer&#39;s behavior and the design process. The recent availability of sophisticated sensing technology has made such data highly multimodal, requiring more flexible protocol analysis tools. To address this need, we present VizScribe, a visual analytics framework that employs multiple coordinated multiple views that enable the viewing of such data from different perspectives. VizScribe allows designers to create, customize, and extend interactive visualizations for design protocol data such as video, transcripts, sketches, sensor data, and user logs. User studies where design researchers used VizScribe for protocol analysis indicated that the linked views and interactive navigation offered by VizScribe afforded the researchers multiple, useful ways to approach and interpret such multimodal data.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
</ul>
</div>
    </main>
    
    <script>const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),n=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(n).then((()=>{t.innerText="✔ Copied BibTeX",setInterval((()=>{t.innerText="✚ Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}</script>
  </body>
</html>