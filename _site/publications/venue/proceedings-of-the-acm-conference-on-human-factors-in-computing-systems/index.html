<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003e75;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'→';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'↓'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}}main{margin:auto;padding:.25rem;padding-bottom:0;max-width:120em}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}.grid-container{display:flex;flex-wrap:wrap;justify-content:center;gap:10px;padding:10px}.grid-item{flex:1 1 300px;max-width:calc(50% - 20px);border-radius:.5rem;box-sizing:border-box;text-align:center;background-color:#003d73;box-shadow:0 5.1px 5.3px rgba(0,0,0,.008),0 17px 17.9px rgba(0,0,0,.012),0 76px 80px rgba(0,0,0,.02);color:#fff}.grid-item>img{border-top-left-radius:.5rem;border-top-right-radius:.5rem;margin-bottom:-10px}@media (max-width:600px){.grid-item{max-width:calc(100% - 20px)}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;↗</sup></a>
      </nav>
    </header>
    
        <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; padding-left: 20px; padding-right: 20px; padding-bottom: 13px; margin-top: 30px; margin-bottom: 0px">
        
            <h4 style="margin-bottom: 0px; padding-top 10px;">Publications published at</h4>
            <h2 style="margin-bottom: 0px; margin-top: 10px;">Proceedings of the ACM Conference on Human Factors in Computing Systems</h2>
        
        </div>
    
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
  <ul>
    
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #64&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Raghunandan2023" onClick="copy('Raghunandan2023')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Raghunandan2023" style="display: none;">
                        @inproceedings{Raghunandan2023,
title={Code Code Evolution: Understanding How People Change Data Science Notebooks Over Time},
author={Deepthi Raghunandan and Aayushi Roy and Shenzhi Shi and Niklas Elmqvist and Leilani Battle},
url={https://users.umiacs.umd.edu/~elm/projects/cce/cce.pdf},
year={2023},
date={2023-04-24},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA },
abstract={Sensemaking is the iterative process of identifying, extracting, and explaining insights from data, where each iteration is referred to as the “sensemaking loop.” However, little is known about how sensemaking behavior evolves from exploration and explanation during this process. This gap limits our ability to understand the full scope of sensemaking, which in turn inhibits the design of tools that support the process. We contribute the first mixed-method to characterize how sensemaking evolves within computational notebooks. We study 2,574 Jupyter notebooks mined from GitHub by identifying data science notebooks that have undergone significant iterations, presenting a regression model that automatically characterizes sensemaking activity, and using this regression model to calculate and analyze shifts in activity across GitHub versions. Our results show that notebook authors participate in various sensemaking tasks over time, such as annotation, branching analysis, and documentation. We use our insights to recommend extensions to current notebook environments.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2023">2023</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/cce/cce.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Code Code Evolution: Understanding How People Change Data Science Notebooks Over Time<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/deepthi-raghunandan/">Deepthi Raghunandan</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/aayushi-roy/">Aayushi Roy</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/shenzhi-shi/">Shenzhi Shi</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/leilani-battle/">Leilani Battle</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Sensemaking is the iterative process of identifying, extracting, and explaining insights from data, where each iteration is referred to as the “sensemaking loop.” However, little is known about how sensemaking behavior evolves from exploration and explanation during this process. This gap limits our ability to understand the full scope of sensemaking, which in turn inhibits the design of tools that support the process. We contribute the first mixed-method to characterize how sensemaking evolves within computational notebooks. We study 2,574 Jupyter notebooks mined from GitHub by identifying data science notebooks that have undergone significant iterations, presenting a regression model that automatically characterizes sensemaking activity, and using this regression model to calculate and analyze shifts in activity across GitHub versions. Our results show that notebook authors participate in various sensemaking tasks over time, such as annotation, branching analysis, and documentation. We use our insights to recommend extensions to current notebook environments.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #63&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Hoque2023" onClick="copy('Hoque2023')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Hoque2023" style="display: none;">
                        @inproceedings{Hoque2023,
title={Accessible Data Representation with Natural Sound},
author={Md Naimul Hoque and Md Ehtesham-Ul-Haque and Niklas Elmqvist and Syed Masum Billah},
url={https://users.umiacs.umd.edu/~elm/projects/susurrus/susurrus.pdf},
year={2023},
date={2023-04-24},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Sonification translates data into non-speech audio. Such auditory representations can make data visualization accessible to people who are blind or have low vision (BLV). This paper presents a sonification method for translating common data visualization into a blend of natural sounds. We hypothesize that people&#39;s familiarity with sounds drawn from nature, such as birds singing in a forest, and their ability to listen to these sounds in parallel, will enable BLV users to perceive multiple data points being sonified at the same time. Informed by an extensive literature review and a preliminary study with 5 BLV participants, we designed an accessible data representation tool, Susurrus, that combines our sonification method with other accessibility features, such as keyboard interaction and text-to-speech feedback. Finally, we conducted a user study with 12 BLV participants and report the potential and application of natural sounds for sonification compared to existing sonification tools.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2023">2023</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/susurrus/susurrus.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Accessible Data Representation with Natural Sound<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/md-naimul-hoque/">Md Naimul Hoque</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/md-ehtesham-ul-haque/">Md Ehtesham-Ul-Haque</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/syed-masum-billah/">Syed Masum Billah</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Sonification translates data into non-speech audio. Such auditory representations can make data visualization accessible to people who are blind or have low vision (BLV). This paper presents a sonification method for translating common data visualization into a blend of natural sounds. We hypothesize that people&#39;s familiarity with sounds drawn from nature, such as birds singing in a forest, and their ability to listen to these sounds in parallel, will enable BLV users to perceive multiple data points being sonified at the same time. Informed by an extensive literature review and a preliminary study with 5 BLV participants, we designed an accessible data representation tool, Susurrus, that combines our sonification method with other accessibility features, such as keyboard interaction and text-to-speech feedback. Finally, we conducted a user study with 12 BLV participants and report the potential and application of natural sounds for sonification compared to existing sonification tools.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #62&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Shin2023" onClick="copy('Shin2023')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Shin2023" style="display: none;">
                        @inproceedings{Shin2023,
title={Perceptual Pat: A Virtual Human Visual System for Iterative Visualization Design},
author={Sungbok Shin and Sanghyun Hong and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/perceptual-pat/perceptual-pat.pdf},
year={2023},
date={2023-04-24},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Designing a visualization is often a process of iterative refinement where the designer improves a chart over time by adding features, improving encodings, and fixing mistakes. However, effective design requires external critique and evaluation. Unfortunately, such critique is not always available on short notice and evaluation can be costly. To address this need, we present Perceptual Pat, an extensible suite of AI and computer vision techniques that forms a virtual human visual system for supporting iterative visualization design. The system analyzes snapshots of a visualization using an extensible set of filters—including gaze maps, text recognition, color analysis, etc—and generates a report summarizing the findings. The web-based Pat Design Lab provides a version tracking system that enables the designer to track improvements over time. We validate Perceptual Pat using a longitudinal qualitative study involving 4 professional visualization designers that used the tool over a few days to design a new visualization},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2023">2023</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/perceptual-pat/perceptual-pat.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Perceptual Pat: A Virtual Human Visual System for Iterative Visualization Design<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sungbok-shin/">Sungbok Shin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sanghyun-hong/">Sanghyun Hong</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Designing a visualization is often a process of iterative refinement where the designer improves a chart over time by adding features, improving encodings, and fixing mistakes. However, effective design requires external critique and evaluation. Unfortunately, such critique is not always available on short notice and evaluation can be costly. To address this need, we present Perceptual Pat, an extensible suite of AI and computer vision techniques that forms a virtual human visual system for supporting iterative visualization design. The system analyzes snapshots of a visualization using an extensible set of filters—including gaze maps, text recognition, color analysis, etc—and generates a report summarizing the findings. The web-based Pat Design Lab provides a version tracking system that enables the designer to track improvements over time. We validate Perceptual Pat using a longitudinal qualitative study involving 4 professional visualization designers that used the tool over a few days to design a new visualization</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #61&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Saffo2023" onClick="copy('Saffo2023')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Saffo2023" style="display: none;">
                        @inproceedings{Saffo2023,
title={Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms},
author={David Saffo and Andrea Batch and Cody Dunne and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/vrxd/vrxd.pdf},
year={2023},
date={2023-04-24},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user&#39;s 3D workspace. To address this, we propose the &#39;eyes-and-shoes&#39; principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A ✚ Copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on https://osf.io/wgprb/. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2023">2023</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/vrxd/vrxd.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/david-saffo/">David Saffo</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andrea-batch/">Andrea Batch</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/cody-dunne/">Cody Dunne</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user&#39;s 3D workspace. To address this, we propose the &#39;eyes-and-shoes&#39; principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A ✚ Copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on https://osf.io/wgprb/. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #60&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Hubenschmid2022" onClick="copy('Hubenschmid2022')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Hubenschmid2022" style="display: none;">
                        @inproceedings{Hubenschmid2022,
title={ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies},
author={Sebastian Hubenschmid and Jonathan Wieland and Daniel Immanuel Fink and Andrea Batch and Johannes Zagermann and Niklas Elmqvist and Harald Reiterer},
url={https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf},
year={2022},
date={2022-05-10},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={24:1--24:20},
publisher={ACM},
address={New York, NY, USA},
abstract={The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sebastian-hubenschmid/">Sebastian Hubenschmid</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jonathan-wieland/">Jonathan Wieland</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/daniel-immanuel-fink/">Daniel Immanuel Fink</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andrea-batch/">Andrea Batch</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/johannes-zagermann/">Johannes Zagermann</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/harald-reiterer/">Harald Reiterer</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #57&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Batch2019" onClick="copy('Batch2019')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Batch2019" style="display: none;">
                        @inproceedings{Batch2019,
title={Scents and Sensibility: Evaluating Information Olfactation},
author={Andrea Batch and Biswaksen Patnaik and Moses Akazue and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/info-olfac/scents-sense.pdf},
year={2020},
date={2020-10-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Olfaction---the sense of smell---is one of the least explored of the human senses for conveying abstract information. In this paper, we conduct a comprehensive perceptual experiment on information olfactation: the use of olfactory and crossmodal sensory marks and channels to convey data. More specifically, following the example from graphical perception studies, we design an experiment that studies the perceptual accuracy of four cross-modal sensory channels---scent type, scent intensity, airflow, and temperature---for conveying three different types of data---nominal, ordinal, and quantitative. We also present details of a 24-scent multi-sensory display},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2020">2020</a></small>
                  </div>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/info-olfac/scents-sense.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Scents and Sensibility: Evaluating Information Olfactation<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andrea-batch/">Andrea Batch</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/biswaksen-patnaik/">Biswaksen Patnaik</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/moses-akazue/">Moses Akazue</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Olfaction---the sense of smell---is one of the least explored of the human senses for conveying abstract information. In this paper, we conduct a comprehensive perceptual experiment on information olfactation: the use of olfactory and crossmodal sensory marks and channels to convey data. More specifically, following the example from graphical perception studies, we design an experiment that studies the perceptual accuracy of four cross-modal sensory channels---scent type, scent intensity, airflow, and temperature---for conveying three different types of data---nominal, ordinal, and quantitative. We also present details of a 24-scent multi-sensory display</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #55&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Chidambaram2019" onClick="copy('Chidambaram2019')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Chidambaram2019" style="display: none;">
                        @inproceedings{Chidambaram2019,
title={Shape Structuralizer: Design, Fabrication and Exploring Structually-Sound Scaffolded Constructions using 3D Mesh Models},
author={Subramanian Chidambaram and Yunbo Zhang and Venkatraghavan Sundararajan and Ana M. Villanueva and Niklas Elmqvist and Karthik Ramani},
url={https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2019/02/Shape-Structuralizer-Design-Fabrication-and-User-driven-Iterative-Refinement-of-3D-Mesh-Models.pdf},
year={2019},
date={2019-05-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={663:1--663:12},
publisher={ACM},
address={New York, NY, USA},
abstract={Current Computer-Aided Design (CAD) tools lack proper support for guiding novice users towards designs ready for fabrication. We propose Shape Structuralizer (SS), an interactive design support system that repurposes surface models into structural constructions using rods and custom 3D-printed joints. Shape Structuralizer embeds a recommendation system that computationally supports the user during design ideation by providing design suggestions on local refinements of the design. This strategy enables novice users to choose designs that both satisfy stress constraints as well as their personal design intent. The interactive guidance enables users to repurpose existing surface mesh models, analyze them in-situ for stress and displacement constraints, add movable joints to increase functionality, and attach a customized appearance. This also empowers novices to fabricate even complex constructs while ensuring structural soundness. We validate the Shape Structuralizer tool with a qualitative user study where we observed that even novice users were able to generate a large number of structurally safe designs for fabrication.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  </div>
                  <a href="https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2019/02/Shape-Structuralizer-Design-Fabrication-and-User-driven-Iterative-Refinement-of-3D-Mesh-Models.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Shape Structuralizer: Design, Fabrication and Exploring Structually-Sound Scaffolded Constructions using 3D Mesh Models<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/subramanian-chidambaram/">Subramanian Chidambaram</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/yunbo-zhang/">Yunbo Zhang</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/venkatraghavan-sundararajan/">Venkatraghavan Sundararajan</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ana-m-villanueva/">Ana M. Villanueva</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Current Computer-Aided Design (CAD) tools lack proper support for guiding novice users towards designs ready for fabrication. We propose Shape Structuralizer (SS), an interactive design support system that repurposes surface models into structural constructions using rods and custom 3D-printed joints. Shape Structuralizer embeds a recommendation system that computationally supports the user during design ideation by providing design suggestions on local refinements of the design. This strategy enables novice users to choose designs that both satisfy stress constraints as well as their personal design intent. The interactive guidance enables users to repurpose existing surface mesh models, analyze them in-situ for stress and displacement constraints, add movable joints to increase functionality, and attach a customized appearance. This also empowers novices to fabricate even complex constructs while ensuring structural soundness. We validate the Shape Structuralizer tool with a qualitative user study where we observed that even novice users were able to generate a large number of structurally safe designs for fabrication.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #54&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Mylvarapu2019" onClick="copy('Mylvarapu2019')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Mylvarapu2019" style="display: none;">
                        @inproceedings{Mylvarapu2019,
title={Ranked-List Visualization: A Graphical Perception Study},
author={Pranathi Mylavarapu and Mehmet Adil Yalcin and Xan Gregg and Niklas Elmqvist},
url={http://users.umiacs.umd.edu/~elm/projects/ranked-list/ranked-list.pdf},
year={2019},
date={2019-05-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={192:1--192:12},
publisher={ACM},
address={New York, NY, USA},
abstract={Visualization of ranked lists is a common occurrence, but many in-the-wild solutions fly in the face of vision science and visualization wisdom. For example, treemaps and bubble charts are commonly used for this purpose, despite the fact that the data is not hierarchical and that length is easier to perceive than area. Furthermore, several new visual representations have recently been suggested in this area, including wrapped bars, packed bars, piled bars, and Zvinca plots. To quantify the differences and trade-offs for these ranked-list visualizations, we here report on a crowdsourced graphical perception study involving six such visual representations, including the ubiquitous scrolled barchart, in three tasks: ranking (assessing a single item), comparison (two items), and average (assessing global distribution). Results show that wrapped bars may be the best choice for visualizing ranked lists, and that treemaps are surprisingly accurate despite the use of area rather than length to represent value.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  </div>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/ranked-list/ranked-list.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Ranked-List Visualization: A Graphical Perception Study<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/pranathi-mylavarapu/">Pranathi Mylavarapu</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mehmet-adil-yalcin/">Mehmet Adil Yalcin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/xan-gregg/">Xan Gregg</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Visualization of ranked lists is a common occurrence, but many in-the-wild solutions fly in the face of vision science and visualization wisdom. For example, treemaps and bubble charts are commonly used for this purpose, despite the fact that the data is not hierarchical and that length is easier to perceive than area. Furthermore, several new visual representations have recently been suggested in this area, including wrapped bars, packed bars, piled bars, and Zvinca plots. To quantify the differences and trade-offs for these ranked-list visualizations, we here report on a crowdsourced graphical perception study involving six such visual representations, including the ubiquitous scrolled barchart, in three tasks: ranking (assessing a single item), comparison (two items), and average (assessing global distribution). Results show that wrapped bars may be the best choice for visualizing ranked lists, and that treemaps are surprisingly accurate despite the use of area rather than length to represent value.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #53&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Horak2019" onClick="copy('Horak2019')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Horak2019" style="display: none;">
                        @inproceedings{Horak2019,
title={Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups},
author={Tom Horak and Andreas Mathisen and Clemens Nylandsted Klokmose and Raimund Dachselt and Niklas Elmqvist},
url={http://users.umiacs.umd.edu/~elm/projects/vistribute/vistribute.pdf},
year={2019},
date={2019-05-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={616:1--616:13},
publisher={ACM},
address={New York, NY, USA},
abstract={We present Vistribute, a framework for the automatic distribution of visualizations and UI components across multiple heterogeneous devices. Our framework consists of three parts: (i) a design space considering properties and relationships of interactive visualizations, devices, and user preferences in multi-display environments; (ii) specific heuristics incorporating these dimensions for guiding the distribution for a given interface and device ensemble; and (iii) a web-based implementation instantiating these heuristics to automatically generate a distribution as well as providing interaction mechanisms for user-defined adaptations. In contrast to existing UI distribution systems, we are able to infer all required information by analyzing the visualizations and devices without relying on additional input provided by users or programmers. In a qualitative study, we let experts create their own distributions and rate both other manual distributions and our automatic ones. We found that all distributions provided comparable quality, hence validating our framework.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  </div>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/vistribute/vistribute.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/tom-horak/">Tom Horak</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/andreas-mathisen/">Andreas Mathisen</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/clemens-nylandsted-klokmose/">Clemens Nylandsted Klokmose</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/raimund-dachselt/">Raimund Dachselt</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present Vistribute, a framework for the automatic distribution of visualizations and UI components across multiple heterogeneous devices. Our framework consists of three parts: (i) a design space considering properties and relationships of interactive visualizations, devices, and user preferences in multi-display environments; (ii) specific heuristics incorporating these dimensions for guiding the distribution for a given interface and device ensemble; and (iii) a web-based implementation instantiating these heuristics to automatically generate a distribution as well as providing interaction mechanisms for user-defined adaptations. In contrast to existing UI distribution systems, we are able to infer all required information by analyzing the visualizations and devices without relying on additional input provided by users or programmers. In a qualitative study, we let experts create their own distributions and rate both other manual distributions and our automatic ones. We found that all distributions provided comparable quality, hence validating our framework.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #48&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Zhang2018" onClick="copy('Zhang2018')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Zhang2018" style="display: none;">
                        @inproceedings{Zhang2018,
title={TopoText: Context-Preserving Semantic Exploration Across Multiple Spatial Scales},
author={Jiawei Zhang and Chittayong Surakitbanharn and Niklas Elmqvist and Ross Maciejewski and Zhenyu Quan and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/topotext/topotext.pdf},
year={2018},
date={2018-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
abstract={TopoText is a context-preserving technique for visualizing semantic data for multi-scale spatial aggregates to gain insight into spatial phenomena. Conventional exploration requires users to navigate across multiple scales but only presents the information related to the current scale. This limitation potentially adds more steps of interaction and cognitive overload to the users. TopoText renders multi-scale aggregates into a single visual display combining novel text-based encoding and layout methods that draw labels along the boundary or filled within the aggregates. The text itself not only summarizes the semantics at each individual scale, but also indicates the spatial coverage of the aggregates and their underlying hierarchical},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/topotext/topotext.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> TopoText: Context-Preserving Semantic Exploration Across Multiple Spatial Scales<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jiawei-zhang/">Jiawei Zhang</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/chittayong-surakitbanharn/">Chittayong Surakitbanharn</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ross-maciejewski/">Ross Maciejewski</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhenyu-quan/">Zhenyu Quan</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">TopoText is a context-preserving technique for visualizing semantic data for multi-scale spatial aggregates to gain insight into spatial phenomena. Conventional exploration requires users to navigate across multiple scales but only presents the information related to the current scale. This limitation potentially adds more steps of interaction and cognitive overload to the users. TopoText renders multi-scale aggregates into a single visual display combining novel text-based encoding and layout methods that draw labels along the boundary or filled within the aggregates. The text itself not only summarizes the semantics at each individual scale, but also indicates the spatial coverage of the aggregates and their underlying hierarchical</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #47&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Horak2018" onClick="copy('Horak2018')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Horak2018" style="display: none;">
                        @inproceedings{Horak2018,
title={When David Meets Goliath: Combining Smartwatches with a Large Vertical Display for Visual Data Exploration},
author={Tom Horak and Sriram Karthik Badam and Niklas Elmqvist and Raimund Dachselt},
url={http://www.umiacs.umd.edu/~elm/projects/david-goliath/david-goliath.pdf},
year={2018},
date={2018-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
abstract={We explore the combination of smartwatches and a large interactive display to support visual data analysis. These two extremes of interactive surfaces are increasingly popular, but feature different characteristics—display and input modalities, personal/public use, performance, and portability. In this paper, we first identify possible roles for both devices and the interplay between them through an example scenario. We then propose a conceptual framework to enable analysts to explore data items, track interaction histories, and alter visualization configurations through mechanisms using both devices in combination. We validate an implementation of our framework through a formative evaluation and a user study. The results show that this device combination, compared to just a large display, allows users to develop complex insights more fluidly by leveraging the roles of the two devices. Finally, we report on the interaction patterns and interplay between the devices for visual exploration as observed during our study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/david-goliath/david-goliath.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> When David Meets Goliath: Combining Smartwatches with a Large Vertical Display for Visual Data Exploration<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/tom-horak/">Tom Horak</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/raimund-dachselt/">Raimund Dachselt</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We explore the combination of smartwatches and a large interactive display to support visual data analysis. These two extremes of interactive surfaces are increasingly popular, but feature different characteristics—display and input modalities, personal/public use, performance, and portability. In this paper, we first identify possible roles for both devices and the interplay between them through an example scenario. We then propose a conceptual framework to enable analysts to explore data items, track interaction histories, and alter visualization configurations through mechanisms using both devices in combination. We validate an implementation of our framework through a formative evaluation and a user study. The results show that this device combination, compared to just a large display, allows users to develop complex insights more fluidly by leveraging the roles of the two devices. Finally, we report on the interaction patterns and interplay between the devices for visual exploration as observed during our study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #43&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Zhang2017" onClick="copy('Zhang2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Zhang2017" style="display: none;">
                        @inproceedings{Zhang2017,
title={TopoGroups: Context-Preserving Visual Illustration of Multi-Scale Spatial Aggregates},
author={Jiawei Zhang and Abish Malik and Benjamin Ahlbrand and Niklas Elmqvist and Ross Maciejewski and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/topogroups/topogroups.pdf},
year={2017},
date={2017-05-08},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={2940--2951},
publisher={ACM},
abstract={Spatial datasets, such as tweets in a geographic area, often exhibit different distribution patterns at multiple levels of scale, such as live updates about events occurring in very specific locations on the map. Navigating in such multi-scale data-rich spaces is often inefficient, requires users to choose between overview or detail information, and does not support identifying spatial patterns at varying scales. In this paper, we propose TopoGroups, a novel context-preserving technique that aggregates spatial data into hierarchical clusters to improve exploration and navigation at multiple spatial scales. The technique uses a boundary distortion algorithm to minimize the visual clutter caused by overlapping aggregates. Our user study explores multiple visual encoding strategies for TopoGroups including color, transparency, shading, and shapes in order to convey the hierarchical and statistical information of the geographical aggregates at different scales.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/topogroups/topogroups.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> TopoGroups: Context-Preserving Visual Illustration of Multi-Scale Spatial Aggregates<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jiawei-zhang/">Jiawei Zhang</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/abish-malik/">Abish Malik</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/benjamin-ahlbrand/">Benjamin Ahlbrand</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ross-maciejewski/">Ross Maciejewski</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Spatial datasets, such as tweets in a geographic area, often exhibit different distribution patterns at multiple levels of scale, such as live updates about events occurring in very specific locations on the map. Navigating in such multi-scale data-rich spaces is often inefficient, requires users to choose between overview or detail information, and does not support identifying spatial patterns at varying scales. In this paper, we propose TopoGroups, a novel context-preserving technique that aggregates spatial data into hierarchical clusters to improve exploration and navigation at multiple spatial scales. The technique uses a boundary distortion algorithm to minimize the visual clutter caused by overlapping aggregates. Our user study explores multiple visual encoding strategies for TopoGroups including color, transparency, shading, and shapes in order to convey the hierarchical and statistical information of the geographical aggregates at different scales.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #42&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Piya2017" onClick="copy('Piya2017')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Piya2017" style="display: none;">
                        @inproceedings{Piya2017,
title={Co-3Deator: A Team-First Collaborative 3D Design ideation Tool},
author={Cecil Piya and Vinayak and Senthil Chandrasegaran and Niklas Elmqvist and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/co3deator/co3deator.pdf},
year={2017},
date={2017-05-08},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={6581--6592},
abstract={We present CO-3DEATOR, a sketch-based collaborative 3D modeling system based on the notion of “team-first” ideation tools, where the needs and processes of the entire design team come before that of an individual designer. Co-3Deator includes two specific team-first features: a concept component hierarchy which provides a design representation suitable for multi-level sharing and reusing of design information, and a collaborative design explorer for storing, viewing, and accessing hierarchical design data during collaborative design activities. We conduct two controlled user studies, one with individual designers to elicit the form and functionality of the collaborative design explorer, and the other with design teams to evaluate the utility of the concept component hierarchy and design explorer towards collaborative design ideation. Our results support our rationale for both of the proposed team-first collaboration mechanisms and suggest further ways to streamline collaborative design.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/co3deator/co3deator.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Co-3Deator: A Team-First Collaborative 3D Design ideation Tool<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/cecil-piya/">Cecil Piya</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/vinayak/">Vinayak</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present CO-3DEATOR, a sketch-based collaborative 3D modeling system based on the notion of “team-first” ideation tools, where the needs and processes of the entire design team come before that of an individual designer. Co-3Deator includes two specific team-first features: a concept component hierarchy which provides a design representation suitable for multi-level sharing and reusing of design information, and a collaborative design explorer for storing, viewing, and accessing hierarchical design data during collaborative design activities. We conduct two controlled user studies, one with individual designers to elicit the form and functionality of the collaborative design explorer, and the other with design teams to evaluate the utility of the concept component hierarchy and design explorer towards collaborative design ideation. Our results support our rationale for both of the proposed team-first collaboration mechanisms and suggest further ways to streamline collaborative design.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #39&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Park2016" onClick="copy('Park2016')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Park2016" style="display: none;">
                        @inproceedings{Park2016,
title={Supporting Comment Moderators in identifying High Quality Online News Comments},
author={Deok Gun Park and Simranjit Singh and Nicholas Diakopoulos and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/commentiq/commentiq.pdf},
year={2016},
date={2016-05-05},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={1114--1125},
abstract={Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2016">2016</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/commentiq/commentiq.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Supporting Comment Moderators in identifying High Quality Online News Comments<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/deok-gun-park/">Deok Gun Park</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/simranjit-singh/">Simranjit Singh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/nicholas-diakopoulos/">Nicholas Diakopoulos</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #38&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Badam2016" onClick="copy('Badam2016')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Badam2016" style="display: none;">
                        @inproceedings{Badam2016,
title={TimeFork: Interactive Prediction of Time Series},
author={Sriram Karthik Badam and Jieqiong Zhao and Shivalik Sen and Niklas Elmqvist and David S. Ebert},
url={http://www.umiacs.umd.edu/~elm/projects/timefork/timefork.pdf},
year={2016},
date={2016-05-05},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={5409--5420},
abstract={We present TimeFork, an interactive prediction technique to support users predicting the future of time-series data, such as in financial, scientific, or medical domains. TimeFork combines visual representations of multiple time series with prediction information generated by computational models. Using this method, analysts engage in a back-and-forth dialogue with the computational model by alternating between manually predicting future changes through interaction and letting the model automatically determine the most likely outcomes, to eventually come to a common prediction using the model. This computer-supported prediction approach allows for harnessing the user’s knowledge of factors influencing future behavior, as well as sophisticated computational models drawing on past performance. To validate the TimeFork technique, we conducted a user study in a stock market prediction game. We present evidence of improved performance for participants using TimeFork compared to fully manual or fully automatic predictions, and characterize qualitative usage patterns observed during the user study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2016">2016</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/timefork/timefork.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> TimeFork: Interactive Prediction of Time Series<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jieqiong-zhao/">Jieqiong Zhao</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/shivalik-sen/">Shivalik Sen</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/david-s-ebert/">David S. Ebert</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present TimeFork, an interactive prediction technique to support users predicting the future of time-series data, such as in financial, scientific, or medical domains. TimeFork combines visual representations of multiple time series with prediction information generated by computational models. Using this method, analysts engage in a back-and-forth dialogue with the computational model by alternating between manually predicting future changes through interaction and letting the model automatically determine the most likely outcomes, to eventually come to a common prediction using the model. This computer-supported prediction approach allows for harnessing the user’s knowledge of factors influencing future behavior, as well as sophisticated computational models drawing on past performance. To validate the TimeFork technique, we conducted a user study in a stock market prediction game. We present evidence of improved performance for participants using TimeFork compared to fully manual or fully automatic predictions, and characterize qualitative usage patterns observed during the user study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #33&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Benjamin2014" onClick="copy('Benjamin2014')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Benjamin2014" style="display: none;">
                        @inproceedings{Benjamin2014,
title={Juxtapoze: supporting serendipity and creative expression in clipart compositions},
author={William Benjamin and Senthil Chandrasegaran and Devarajan Ramanujan and Niklas Elmqvist and SVN Vishwanathan and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/juxtapoze/juxtapoze.pdf},
year={2014},
date={2014-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={341--350},
abstract={Juxtapoze is a clipart composition workflow that supports creative expression and serendipitous discoveries in the shape domain. We achieve creative expression by supporting a workflow of searching, editing, and composing: the user queries the shape database using strokes, selects the desired search result, and finally modifies the selected image before composing it into the overall drawing. Serendipitous discovery of shapes is facilitated by allowing multiple exploration channels, such as doodles, shape filtering, and relaxed search. Results from a qualitative evaluation show that Juxtapoze makes the process of creating image compositions enjoyable and supports creative expression and serendipity.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/juxtapoze/juxtapoze.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Juxtapoze: supporting serendipity and creative expression in clipart compositions<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/william-benjamin/">William Benjamin</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/devarajan-ramanujan/">Devarajan Ramanujan</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/svn-vishwanathan/">SVN Vishwanathan</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Juxtapoze is a clipart composition workflow that supports creative expression and serendipitous discoveries in the shape domain. We achieve creative expression by supporting a workflow of searching, editing, and composing: the user queries the shape database using strokes, selects the desired search result, and finally modifies the selected image before composing it into the overall drawing. Serendipitous discovery of shapes is facilitated by allowing multiple exploration channels, such as doodles, shape filtering, and relaxed search. Results from a qualitative evaluation show that Juxtapoze makes the process of creating image compositions enjoyable and supports creative expression and serendipity.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #31&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Zhao2014" onClick="copy('Zhao2014')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Zhao2014" style="display: none;">
                        @inproceedings{Zhao2014,
title={skWiki: A Multimedia Sketching System for Collaborative Creativity},
author={Zhenpeng Zhao and Sriram Karthik Badam and Senthil Chandrasegaran and Deo Gun Park and Niklas Elmqvist and Lorraine Kisselburgh and Karthik Ramani},
url={http://www.umiacs.umd.edu/~elm/projects/skwiki/skwiki.pdf},
video={https://www.youtube.com/watch?v=QxtTR14EXFQ},
year={2014},
date={2014-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={1235--1244},
abstract={We present skWiki, a web application framework for collaborative creativity in digital multimedia projects, including text, hand-drawn sketches, and photographs. skWiki overcomes common drawbacks of existing wiki software by providing a rich viewer/editor architecture for all media types that is integrated into the web browser itself, thus avoiding dependence on client-side editors. Instead of files, skWiki uses the concept of paths as trajectories of persistent state over time. This model has intrinsic support for collaborative editing, including cloning, branching, and merging paths edited by multiple contributors. We demonstrate skWiki&#39;s utility using a qualitative, sketching-based user study.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2014">2014</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/skwiki/skwiki.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> skWiki: A Multimedia Sketching System for Collaborative Creativity<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/zhenpeng-zhao/">Zhenpeng Zhao</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sriram-karthik-badam/">Sriram Karthik Badam</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/senthil-chandrasegaran/">Senthil Chandrasegaran</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/deo-gun-park/">Deo Gun Park</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/lorraine-kisselburgh/">Lorraine Kisselburgh</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/karthik-ramani/">Karthik Ramani</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present skWiki, a web application framework for collaborative creativity in digital multimedia projects, including text, hand-drawn sketches, and photographs. skWiki overcomes common drawbacks of existing wiki software by providing a rich viewer/editor architecture for all media types that is integrated into the web browser itself, thus avoiding dependence on client-side editors. Instead of files, skWiki uses the concept of paths as trajectories of persistent state over time. This model has intrinsic support for collaborative editing, including cloning, branching, and merging paths edited by multiple contributors. We demonstrate skWiki&#39;s utility using a qualitative, sketching-based user study.</p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=QxtTR14EXFQ" style="font-weight: 500"><small>📺 Video</small></a>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #29&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Javed2012b" onClick="copy('Javed2012b')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Javed2012b" style="display: none;">
                        @inproceedings{Javed2012b,
title={PolyZoom: Multiscale and Multifocus Exploration in 2D Visual Spaces},
author={Waqas Javed and Sohaib Ghani and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/polyzoom/polyzoom.pdf},
year={2012},
date={2012-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={287--296},
abstract={The most common techniques for navigating in multiscale visual spaces are pan, zoom, and bird’s eye views. However, these techniques are often tedious and cumbersome to use, especially when objects of interest are located far apart. We present the PolyZoom technique where users progressively build hierarchies of focus regions, stacked on each other such that each subsequent level shows a higher magnification. Correlation graphics show the relation between parent and child viewports in the hierarchy. To validate the new technique, we compare it to standard navigation techniques in two user studies, one on multiscale visual search and the other on multifocus interaction. Results show that PolyZoom performs better than current standard techniques. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2012">2012</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/polyzoom/polyzoom.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> PolyZoom: Multiscale and Multifocus Exploration in 2D Visual Spaces<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/sohaib-ghani/">Sohaib Ghani</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">The most common techniques for navigating in multiscale visual spaces are pan, zoom, and bird’s eye views. However, these techniques are often tedious and cumbersome to use, especially when objects of interest are located far apart. We present the PolyZoom technique where users progressively build hierarchies of focus regions, stacked on each other such that each subsequent level shows a higher magnification. Correlation graphics show the relation between parent and child viewports in the hierarchy. To validate the new technique, we compare it to standard navigation techniques in two user studies, one on multiscale visual search and the other on multifocus interaction. Results show that PolyZoom performs better than current standard techniques. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #24&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Dragicevic2011" onClick="copy('Dragicevic2011')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Dragicevic2011" style="display: none;">
                        @inproceedings{Dragicevic2011,
title={Temporal Distortion for Animated Transitions},
author={Pierre Dragicevic and Anastasia Bezerianos and Waqas Javed and Niklas Elmqvist and Jean-Daniel Fekete},
url={http://www.umiacs.umd.edu/~elm/projects/timedistort/timedistort.pdf},
year={2011},
date={2011-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={2009-2018},
abstract={Animated transitions are popular in many visual applications but they can be difficult to follow, especially when many objects move at the same time. One informal design guideline for creating effective animated transitions has long been the use of slow-in/slow-out pacing, but no empirical data exist to support this practice. We remedy this by studying object tracking performance under different conditions of temporal distortion, i.e., constant speed transitions, slow-in/slow-out, fast-in/fast-out, and an adaptive technique that slows down the visually complex parts of the animation. Slow-in/slow-out outperformed other techniques, but we saw technique differences depending on the type of visual transition.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2011">2011</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/timedistort/timedistort.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Temporal Distortion for Animated Transitions<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/pierre-dragicevic/">Pierre Dragicevic</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/anastasia-bezerianos/">Anastasia Bezerianos</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jean-daniel-fekete/">Jean-Daniel Fekete</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Animated transitions are popular in many visual applications but they can be difficult to follow, especially when many objects move at the same time. One informal design guideline for creating effective animated transitions has long been the use of slow-in/slow-out pacing, but no empirical data exist to support this practice. We remedy this by studying object tracking performance under different conditions of temporal distortion, i.e., constant speed transitions, slow-in/slow-out, fast-in/fast-out, and an adaptive technique that slows down the visually complex parts of the animation. Slow-in/slow-out outperformed other techniques, but we saw technique differences depending on the type of visual transition.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #19&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Kwon2011" onClick="copy('Kwon2011')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Kwon2011" style="display: none;">
                        @inproceedings{Kwon2011,
title={Direct Manipulation Through Surrogate Objects},
author={Bumchul Kwon and Waqas Javed and Niklas Elmqvist and Ji Soo Yi},
url={http://www.umiacs.umd.edu/~elm/projects/surrogate/surrogate.pdf},
year={2011},
date={2011-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={627-636},
abstract={Direct manipulation has had major influence on interface design since it was proposed by Shneiderman in 1982. Although directness generally benefits users, direct manipulation also has weaknesses. In some cases, such as when a user needs to manipulate small, attribute-rich objects or multiple objects simultaneously, indirect manipulation may be more efficient at the cost of directness or intuitiveness of the interaction. Several techniques have been developed over the years to address these issues, but these are all isolated and limited efforts with no coherent underlying principle. We propose the notion of Surrogate Interaction that ties together a large subset of these techniques through the use of a surrogate object that allow users to interact with the surrogate instead of the domain object. We believe that formalizing this family of interaction techniques will provide an additional and powerful interface design alternative for interaction designers, as well as uncover opportunities for future research. },
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2011">2011</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/surrogate/surrogate.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Direct Manipulation Through Surrogate Objects<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/bumchul-kwon/">Bumchul Kwon</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/waqas-javed/">Waqas Javed</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/ji-soo-yi/">Ji Soo Yi</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Direct manipulation has had major influence on interface design since it was proposed by Shneiderman in 1982. Although directness generally benefits users, direct manipulation also has weaknesses. In some cases, such as when a user needs to manipulate small, attribute-rich objects or multiple objects simultaneously, indirect manipulation may be more efficient at the cost of directness or intuitiveness of the interaction. Several techniques have been developed over the years to address these issues, but these are all isolated and limited efforts with no coherent underlying principle. We propose the notion of Surrogate Interaction that ties together a large subset of these techniques through the use of a surrogate object that allow users to interact with the surrogate instead of the domain object. We believe that formalizing this family of interaction techniques will provide an additional and powerful interface design alternative for interaction designers, as well as uncover opportunities for future research. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #16&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Elmqvist2009a" onClick="copy('Elmqvist2009a')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Elmqvist2009a" style="display: none;">
                        @inproceedings{Elmqvist2009a,
title={Motion-Pointing: Target Selection using Elliptical Motions},
author={Jean-Daniel Fekete and Niklas Elmqvist and Yves Guiard},
url={http://www.umiacs.umd.edu/~elm/projects/motionpointing/motionpointing.pdf},
year={2009},
date={2009-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={289--298},
abstract={We present a novel method called motion-pointing for selecting a set of visual items, such as push-buttons or radio-buttons, without actually pointing to them. Instead, each potential target displays an animated point we call the driver. To select a specific item, the user only has to imitate the motion of its driver using the input device. Once the motion has been recognized by the system, the user can confirm the selection to trigger the action. We consider cyclic motions on an elliptic trajectory with a specific period, and study the most effective methods for real-time matching such a trajectory, as well as the range of parameters a human can reliably reproduce. We then show how to implement motion-pointing in real applications using an interaction technique we call move-and-stroke. Finally, we measure the input throughput and error rate of move-and-stroke in a controlled experiment. We show that the selection time is linearly proportional to the number of input bits conveyed up to 6 bits, confirming that motion-pointing is a practical input method.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2009">2009</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/motionpointing/motionpointing.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Motion-Pointing: Target Selection using Elliptical Motions<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jean-daniel-fekete/">Jean-Daniel Fekete</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/yves-guiard/">Yves Guiard</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">We present a novel method called motion-pointing for selecting a set of visual items, such as push-buttons or radio-buttons, without actually pointing to them. Instead, each potential target displays an animated point we call the driver. To select a specific item, the user only has to imitate the motion of its driver using the input device. Once the motion has been recognized by the system, the user can confirm the selection to trigger the action. We consider cyclic motions on an elliptic trajectory with a specific period, and study the most effective methods for real-time matching such a trajectory, as well as the range of parameters a human can reliably reproduce. We then show how to implement motion-pointing in real applications using an interaction technique we call move-and-stroke. Finally, we measure the input throughput and error rate of move-and-stroke in a controlled experiment. We show that the selection time is linearly proportional to the number of input bits conveyed up to 6 bits, confirming that motion-pointing is a practical input method.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #14&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Elmqvist2008d" onClick="copy('Elmqvist2008d')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Elmqvist2008d" style="display: none;">
                        @inproceedings{Elmqvist2008d,
title={Mélange: Space Folding for Multi-Focus Interaction},
author={Niklas Elmqvist and Nathalie Henry Riche and Yann Riche and Jean-Daniel Fekete},
url={http://www.umiacs.umd.edu/~elm/projects/melange/melange.pdf},
video={https://www.youtube.com/watch?v=I1KiO1iZ1DI},
year={2008},
date={2008-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={1333--1342},
abstract={Interaction and navigation in large geometric spaces typically require a sequence of pan and zoom actions. This strategy is often ineffective and cumbersome, especially when trying to study several distant objects. We propose a new distortion technique that folds the intervening space to guarantee visibility of multiple focus regions. The folds themselves show contextual information and support unfolding and paging interactions. Compared to previous work, our method provides more context and distance awareness. We conducted a study comparing the space-folding technique to existing approaches, and found that participants performed significantly better with the new technique.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2008">2008</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/melange/melange.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Mélange: Space Folding for Multi-Focus Interaction<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/nathalie-henry-riche/">Nathalie Henry Riche</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/yann-riche/">Yann Riche</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/jean-daniel-fekete/">Jean-Daniel Fekete</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Interaction and navigation in large geometric spaces typically require a sequence of pan and zoom actions. This strategy is often ineffective and cumbersome, especially when trying to study several distant objects. We propose a new distortion technique that folds the intervening space to guarantee visibility of multiple focus regions. The folds themselves show contextual information and support unfolding and paging interactions. Compared to previous work, our method provides more context and distance awareness. We conducted a study comparing the space-folding technique to existing approaches, and found that participants performed significantly better with the new technique.</p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=I1KiO1iZ1DI" style="font-weight: 500"><small>📺 Video</small></a>
                      
                  </div>
              </div>
          </li>
       
        <li style="display: flex; margin-bottom: 1rem;">
                <div>
                  <div style="padding: 0.5rem; display: flex; flex-direction: column; color: black">
                    <div style="display: flex; flex-direction: row; gap: 0.25rem;">
                      <div style="display: flex; font-weight: 600">
                        Conference Paper
                      </div>
                      <div style="display: flex; font-weight: 600">
                      #13&nbsp;
                      </div>
                                          <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem; width: fit-content;" class="clickable badge inverted" id="bibtex-button-Elmqvist2008c" onClick="copy('Elmqvist2008c')">
                      Copy BibTeX
                    </button>
                    <div id="bibtex-Elmqvist2008c" style="display: none;">
                        @inproceedings{Elmqvist2008c,
title={Evaluating Motion Constraints for 3D Wayfinding in Immersive and Desktop Virtual Environments},
author={Niklas Elmqvist and Mihail Eduard Tudoreanu and Philippas Tsigas},
url={http://www.umiacs.umd.edu/~elm/projects/motcon/motcon.pdf},
video={https://www.youtube.com/watch?v=LRVTyoeuhpo},
year={2008},
date={2008-01-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={1769--1778},
abstract={Motion constraints providing guidance for 3D navigation have recently been suggested as a way of offloading some of the cognitive effort of traversing complex 3D environments on a computer. We present findings from an evaluation of the benefits of this practice where users achieved significantly better results in memory recall and performance when given access to such a guidance method. The study was conducted on both standard desktop computers with mouse and keyboard, as well as on an immersive CAVE system. Interestingly, our results also show that the improvements were more dramatic for desktop users than for CAVE users, even outperforming the latter. Furthermore, the study indicates that allowing the users to retain local control over the navigation on the desktop platform helps them in familiarizing themselves with the 3D world.},
keywords={},
}
                    </div>

                  </div>
                    <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2008">2008</a></small>
                  </div>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/motcon/motcon.pdf" style="display: inline-flex; flex-direction: row;">
                    <h3 style="margin: 0.15rem; margin-bottom: 0.3rem"><span style="text-decoration-color: transparent; font-size: 0.8rem; padding: 4px; padding-left: 8px; padding-right: 4px; border-radius: 4px; background-color:  #F40F02 ; color:  white ;">pdf&nbsp;</span> Evaluating Motion Constraints for 3D Wayfinding in Immersive and Desktop Virtual Environments<sup style="opacity: 0.5">&nbsp;↗</sup></h3>
                  </a>
                  <div style="margin: 0.15rem; line-height: 1.3; display: flex; gap: 6px; flex-wrap: wrap;">
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/mihail-eduard-tudoreanu/">Mihail Eduard Tudoreanu</span></a></div>
                      
                    
                      
                        <div><a class="badge" style="display: flex; font-weight: 500" href="/publications/members/philippas-tsigas/">Philippas Tsigas</span></a></div>
                      
                    
                  </div>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.95rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725;">Motion constraints providing guidance for 3D navigation have recently been suggested as a way of offloading some of the cognitive effort of traversing complex 3D environments on a computer. We present findings from an evaluation of the benefits of this practice where users achieved significantly better results in memory recall and performance when given access to such a guidance method. The study was conducted on both standard desktop computers with mouse and keyboard, as well as on an immersive CAVE system. Interestingly, our results also show that the improvements were more dramatic for desktop users than for CAVE users, even outperforming the latter. Furthermore, the study indicates that allowing the users to retain local control over the navigation on the desktop platform helps them in familiarizing themselves with the 3D world.</p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=LRVTyoeuhpo" style="font-weight: 500"><small>📺 Video</small></a>
                      
                  </div>
              </div>
          </li>
       
</ul>
</div>
    </main>
    
    <script>const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),n=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(n).then((()=>{t.innerText="✔ Copied BibTeX",setInterval((()=>{t.innerText="✚ Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}</script>
  </body>
</html>