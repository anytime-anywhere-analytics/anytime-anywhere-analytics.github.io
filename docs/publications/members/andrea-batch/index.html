<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    <link rel="stylesheet" href="https://cdn.plyr.io/3.7.8/plyr.css"/> 
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003d73;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'→';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'↓'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}.sticky-header.fixed{display:none}}main{margin:auto;margin-top:25px;padding:.25rem;padding-bottom:0;max-width:120em}.sticky-header{left:0;right:0;position:relative;color:#000}.sticky-header.fixed{margin-top:0;position:fixed;top:40px;width:100%;box-sizing:border-box;z-index:0}.sticky-header.fixed.absolute{position:absolute}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;↗</sup></a>
      </nav>
    </header>
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
  <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; margin-left: -20px; margin-right: -20px; padding-left: 20px; padding-right: 20px; padding-bottom: 20px">
    <h4 style="margin-bottom: 0px; padding-top: 10px">Publications coauthored by</h4>
    <h2 style="margin-top: 0px; margin-bottom: 0px">Andrea Batch</h2>
  </div>
  <ul>
      
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Saffo2023" onClick="copy('Saffo2023')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Saffo2023" style="display: none;">
                      @inproceedings{Saffo2023,
title={Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms},
author={David Saffo and Andrea Batch and Cody Dunne and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/vrxd/vrxd.pdf},
year={2023},
date={2023-04-24},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user&#39;s 3D workspace. To address this, we propose the &#39;eyes-and-shoes&#39; principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A ✚ Copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on https://osf.io/wgprb/. },
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2023">2023</a></small>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/vrxd/vrxd.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-saffo/">David Saffo</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/cody-dunne/">Cody Dunne</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user&#39;s 3D workspace. To address this, we propose the &#39;eyes-and-shoes&#39; principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A ✚ Copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on https://osf.io/wgprb/. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Batch2023" onClick="copy('Batch2023')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Batch2023" style="display: none;">
                      @article{Batch2023,
title={uxSense: Supporting User Experience Analysis with Visualization and Computer Vision},
author={Andrea Batch and Yipeng Ji and Mingming Fan and Jian Zhao and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/uxsense/uxsense.pdf},
year={2023},
date={2023-02-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose UXSENSE, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2023">2023</a></small>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/uxsense/uxsense.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">uxSense: Supporting User Experience Analysis with Visualization and Computer Vision</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/yipeng-ji/">Yipeng Ji</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/mingming-fan/">Mingming Fan</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jian-zhao/">Jian Zhao</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose UXSENSE, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Hubenschmid2022" onClick="copy('Hubenschmid2022')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Hubenschmid2022" style="display: none;">
                      @inproceedings{Hubenschmid2022,
title={ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies},
author={Sebastian Hubenschmid and Jonathan Wieland and Daniel Immanuel Fink and Andrea Batch and Johannes Zagermann and Niklas Elmqvist and Harald Reiterer},
url={https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf},
year={2022},
date={2022-05-10},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={24:1--24:20},
publisher={ACM},
address={New York, NY, USA},
abstract={The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2022">2022</a></small>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sebastian-hubenschmid/">Sebastian Hubenschmid</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jonathan-wieland/">Jonathan Wieland</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/daniel-immanuel-fink/">Daniel Immanuel Fink</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/johannes-zagermann/">Johannes Zagermann</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/harald-reiterer/">Harald Reiterer</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Batch2019" onClick="copy('Batch2019')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Batch2019" style="display: none;">
                      @inproceedings{Batch2019,
title={Scents and Sensibility: Evaluating Information Olfactation},
author={Andrea Batch and Biswaksen Patnaik and Moses Akazue and Niklas Elmqvist},
url={https://users.umiacs.umd.edu/~elm/projects/info-olfac/scents-sense.pdf},
year={2020},
date={2020-10-01},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
publisher={ACM},
address={New York, NY, USA},
abstract={Olfaction---the sense of smell---is one of the least explored of the human senses for conveying abstract information. In this paper, we conduct a comprehensive perceptual experiment on information olfactation: the use of olfactory and crossmodal sensory marks and channels to convey data. More specifically, following the example from graphical perception studies, we design an experiment that studies the perceptual accuracy of four cross-modal sensory channels---scent type, scent intensity, airflow, and temperature---for conveying three different types of data---nominal, ordinal, and quantitative. We also present details of a 24-scent multi-sensory display},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems • <a class="secondary" style="font-weight: 500" href="/publications/year/2020">2020</a></small>
                  <a href="https://users.umiacs.umd.edu/~elm/projects/info-olfac/scents-sense.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Scents and Sensibility: Evaluating Information Olfactation</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/biswaksen-patnaik/">Biswaksen Patnaik</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/moses-akazue/">Moses Akazue</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Olfaction---the sense of smell---is one of the least explored of the human senses for conveying abstract information. In this paper, we conduct a comprehensive perceptual experiment on information olfactation: the use of olfactory and crossmodal sensory marks and channels to convey data. More specifically, following the example from graphical perception studies, we design an experiment that studies the perceptual accuracy of four cross-modal sensory channels---scent type, scent intensity, airflow, and temperature---for conveying three different types of data---nominal, ordinal, and quantitative. We also present details of a 24-scent multi-sensory display</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Batch2020" onClick="copy('Batch2020')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Batch2020" style="display: none;">
                      @article{Batch2020,
title={There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics},
author={Andrea Batch and Andrew Cunningham and Maxime Cordeil and Niklas Elmqvist and Tim Dwyer and Bruce H. Thomas and Kim Marriott},
url={http://users.umiacs.umd.edu/~elm/projects/nospoon/nospoon.pdf},
year={2020},
date={2020-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
volume={26},
number={1},
abstract={Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2020">2020</a></small>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/nospoon/nospoon.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/andrew-cunningham/">Andrew Cunningham</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/maxime-cordeil/">Maxime Cordeil</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/tim-dwyer/">Tim Dwyer</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/bruce-h-thomas/">Bruce H. Thomas</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/kim-marriott/">Kim Marriott</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Patnaik2019" onClick="copy('Patnaik2019')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Patnaik2019" style="display: none;">
                      @article{Patnaik2019,
title={Information Olfactation: Harnessing Scent to Convey Data},
author={Biswaksen Patnaik and Andrea Batch and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/info-olfac/info-olfac.pdf},
year={2019},
date={2019-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present VISCENT: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/info-olfac/info-olfac.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Information Olfactation: Harnessing Scent to Convey Data</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/biswaksen-patnaik/">Biswaksen Patnaik</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present VISCENT: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Batch2018" onClick="copy('Batch2018')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Batch2018" style="display: none;">
                      @inproceedings{Batch2018,
title={Gesture and Action Discovery for Evaluating Virtual Environments with Semi-Supervised Segmentation of Telemetry Records},
author={Andrea Batch and Hanuma Teja Maddali and Kyungjun Lee and Niklas Elmqvist},
url={http://users.umiacs.umd.edu/~elm/projects/hceye/vr-telemetry.pdf},
year={2018},
date={2018-12-10},
journal={Proceedings of the IEEE Conference on Artificial Intelligence &amp; Virtual Reality},
booktitle={Proceedings of the IEEE Conference on Artificial Intelligence &amp; Virtual Reality},
pages={1--10},
publisher={IEEE},
abstract={In this paper, we propose a novel pipeline for semi-supervised behavioral coding of videos of users testing a device or interface, with an eye toward human-computer interaction evaluation for virtual reality. Our system applies existing statistical techniques for time-series classification, including e-divisive change point detection and &quot;Symbolic Aggregate approXimation&quot; (SAX) with agglomerative hierarchical clustering, to 3D pose telemetry data. These techniques create classes of short segments of single-person video data–short actions of potential interest called &quot;micro-gestures.&quot; A long short-term memory (LSTM) layer then learns these micro-gestures from pose features generated purely from video via a pretrained OpenPose convolutional neural network (CNN) to predict their occurrence in unlabeled test videos. We present and discuss the results from testing our system on the single user pose videos of the CMU Panoptic Dataset. },
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-ieee-conference-on-artificial-intelligence-and-virtual-reality">Proceedings of the IEEE Conference on Artificial Intelligence &amp; Virtual Reality • <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/hceye/vr-telemetry.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Gesture and Action Discovery for Evaluating Virtual Environments with Semi-Supervised Segmentation of Telemetry Records</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/hanuma-teja-maddali/">Hanuma Teja Maddali</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/kyungjun-lee/">Kyungjun Lee</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">In this paper, we propose a novel pipeline for semi-supervised behavioral coding of videos of users testing a device or interface, with an eye toward human-computer interaction evaluation for virtual reality. Our system applies existing statistical techniques for time-series classification, including e-divisive change point detection and &quot;Symbolic Aggregate approXimation&quot; (SAX) with agglomerative hierarchical clustering, to 3D pose telemetry data. These techniques create classes of short segments of single-person video data–short actions of potential interest called &quot;micro-gestures.&quot; A long short-term memory (LSTM) layer then learns these micro-gestures from pose features generated purely from video via a pretrained OpenPose convolutional neural network (CNN) to predict their occurrence in unlabeled test videos. We present and discuss the results from testing our system on the single user pose videos of the CMU Panoptic Dataset. </p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Gold2018" onClick="copy('Gold2018')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Gold2018" style="display: none;">
                      @inproceedings{Gold2018,
title={Clinical Concept Value Sets and Interoperability in Health Data Analytics},
author={Sigfried Gold and Andrea Batch and Robert McClure and Guoqian Jiang and Hadi Kharrazi and Rishi Saripalle and Vojtech Huser and Chunhua Weng and Nancy Roderer and Ana Szarfman and Niklas Elmqvist and David Gotz},
url={https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371254/},
year={2018},
date={2018-11-03},
journal={Proceedings of the Annual AMIA Symposium},
booktitle={Proceedings of the Annual AMIA Symposium},
abstract={This paper focuses on value sets as an essential component in the health analytics ecosystem. We discuss shared repositories of reusable value sets and offer recommendations for their further development and adoption. In order to motivate these contributions, we explain how value sets fit into specific analytic tasks and the health analytics landscape more broadly; their growing importance and ubiquity with the advent of Common Data Models, Distributed Research Networks, and the availability of higher order, reusable analytic resources like electronic phenotypes and electronic clinical quality measures; the formidable barriers to value set reuse; and our introduction of a concept-agnostic orientation to vocabulary collections. The costs of ad hoc value set management and the benefits of value set reuse are described or implied throughout. Our standards, infrastructure, and design recommendations are not systematic or comprehensive but invite further work to support value set reuse for health analytics},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-annual-amia-symposium">Proceedings of the Annual AMIA Symposium • <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371254/"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Clinical Concept Value Sets and Interoperability in Health Data Analytics</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sigfried-gold/">Sigfried Gold</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/robert-mcclure/">Robert McClure</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/guoqian-jiang/">Guoqian Jiang</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/hadi-kharrazi/">Hadi Kharrazi</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/rishi-saripalle/">Rishi Saripalle</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/vojtech-huser/">Vojtech Huser</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/chunhua-weng/">Chunhua Weng</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/nancy-roderer/">Nancy Roderer</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/ana-szarfman/">Ana Szarfman</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/david-gotz/">David Gotz</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">This paper focuses on value sets as an essential component in the health analytics ecosystem. We discuss shared repositories of reusable value sets and offer recommendations for their further development and adoption. In order to motivate these contributions, we explain how value sets fit into specific analytic tasks and the health analytics landscape more broadly; their growing importance and ubiquity with the advent of Common Data Models, Distributed Research Networks, and the availability of higher order, reusable analytic resources like electronic phenotypes and electronic clinical quality measures; the formidable barriers to value set reuse; and our introduction of a concept-agnostic orientation to vocabulary collections. The costs of ad hoc value set management and the benefits of value set reuse are described or implied throughout. Our standards, infrastructure, and design recommendations are not systematic or comprehensive but invite further work to support value set reuse for health analytics</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Batch2017" onClick="copy('Batch2017')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Batch2017" style="display: none;">
                      @article{Batch2017,
title={The Interactive Visualization Gap in Initial Exploratory Data Analysis},
author={Andrea Batch and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/visgap/visgap.pdf},
year={2017},
date={2017-10-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a “visualization gap” during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics • <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/visgap/visgap.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">The Interactive Visualization Gap in Initial Exploratory Data Analysis</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Andrea Batch</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a “visualization gap” during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
  </ul>
</div>

    </main>
    
    <script>
      const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),o=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(o).then((()=>{t.innerText="✔ Copied BibTeX",setInterval((()=>{t.innerText="✚ Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}var stickyHeaders=function(){var e,t=function(){Array.from(e).forEach((function(t,o){if(t.originalPosition<=window.scrollY){t.classList.add("fixed"),t.style.color="white",t.style.zIndex=1e3,t.style.fontSize="0.8rem",t.style.padding="0.25rem",t.style.paddingLeft="1rem",t.style.paddingBottom="0.4rem",t.style.backgroundColor="#0273A8";var n=e[o+1];if(n){var i=n.originalPosition-t.originalHeight;t.offsetTop>=i&&(t.classList.add("absolute"),t.style.top=i+"px")}}else{t.classList.remove("fixed"),t.style.color="black",t.style.zIndex=-1e3,t.style.fontSize="1.2rem",t.style.padding="0px",t.style.backgroundColor="transparent";var r=e[o-1];r&&window.scrollY<=t.originalPosition-t.originalHeight&&(r.classList.remove("absolute"),r.style.removeProperty("top"))}}))};return{load:function(o){(e=document.querySelectorAll(o)).length>0&&(Array.from(e).forEach((function(e){var t=document.createElement("div");t.className="followWrap",e.parentNode.insertBefore(t,e),t.appendChild(e),e.originalPosition=e.offsetTop,e.originalHeight=e.offsetHeight,t.style.height=e.offsetHeight+"px"})),window.removeEventListener("scroll",t),window.addEventListener("scroll",t))}}}();document.addEventListener("DOMContentLoaded",(function(){stickyHeaders.load(".sticky-header")}));</script>
  </body>
</html>