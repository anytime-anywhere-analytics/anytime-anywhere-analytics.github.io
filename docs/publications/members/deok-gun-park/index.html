<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:locale" content="en">
    <meta property="og:site_name" content="CA3">
    <meta property="og:title" content="Center for Anytime Anywhere Analytics">
    <meta property="og:url" content="https://ca3.au.dk/">
    <meta property="og:type" content="website">
    <meta property="og:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta property="og:image:url" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta property="og:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:title" content="Center for Anytime Anywhere Analytics">
    <meta name="twitter:url" content="https://ca3.au.dk/">
    <meta name="twitter:description" content="Home of the Center for Anytime Anywhere Analytics in the Web">
    <meta name="twitter:image" content="https://ca3.au.dk//assets/share-image.jpg">
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:site" content="@@ca3">
    <meta name="twitter:dnt" content="on">
    <title>Center for Anytime Anywhere Analytics</title>
    <link rel="icon" type="image/png" href="/assets/icon.png">
    <link rel="dns-prefetch" href="https://ca3.au.dk/">
    <link rel="preconnect" href="https://ca3.au.dk/">
    <link rel="me" href="https://ca3.au.dk/" type="text/html">
    <link rel="me" href="mailto:elm@cs.au.dk">
    <link rel="stylesheet" href="https://cdn.plyr.io/3.7.8/plyr.css"/> 
    <style>
      :root{--publication-length:0;--grid-maxWidth:120rem;--grid-gutter:3rem;--font-size:1rem;--line-height:1.55;--font-family-sans:system-ui,--apple-system,sans-serif;--font-family-mono:monaco,"Consolas","Lucida Console",monospace}@media only screen and (min-width:768px){div.plyr{border-radius:.5rem!important}}body{font-family:var(--font-family-sans);font-size:var(--font-size);line-height:var(--line-height);margin:0;padding:0}a{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#003d73;transition:all .25s}a.secondary{font-weight:700;text-underline-offset:3px;text-decoration-color:hsla(199,98%,33%,.229);color:#0273a8;transition:all .25s}a.secondary:hover{color:rgba(2,115,168,.705)}a:hover{color:hsla(208,100%,23%,.705)}code,pre{font-family:var(--font-family-mono)}.container{max-width:var(--grid-maxWidth);margin:0 auto;width:96%;padding:0 calc(var(--grid-gutter)/ 2)}.row{display:flex;flex-flow:row wrap;justify-content:flex-start;margin-left:calc(var(--grid-gutter)/ -2);margin-right:calc(var(--grid-gutter)/ -2)}.row.reverse{flex-direction:row-reverse}.col{flex:1}.col,[class*=" col-"],[class^=col-]{margin:0 calc(var(--grid-gutter)/ 2) calc(var(--grid-gutter)/ 2)}.col-1{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}@media screen and (max-width:599px){.container{width:100%}.col,[class*=col-],[class^=col-]{flex:0 1 100%;max-width:100%}}@media screen and (min-width:900px){.col-1-md{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-md{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-md{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-md{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-md{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-md{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-md{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-md{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-md{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-md{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-md{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-md{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}@media screen and (min-width:1200px){.col-1-lg{flex:0 0 calc((100% / (12/1)) - var(--grid-gutter));max-width:calc((100% / (12/1)) - var(--grid-gutter))}.col-2-lg{flex:0 0 calc((100% / (12/2)) - var(--grid-gutter));max-width:calc((100% / (12/2)) - var(--grid-gutter))}.col-3-lg{flex:0 0 calc((100% / (12/3)) - var(--grid-gutter));max-width:calc((100% / (12/3)) - var(--grid-gutter))}.col-4-lg{flex:0 0 calc((100% / (12/4)) - var(--grid-gutter));max-width:calc((100% / (12/4)) - var(--grid-gutter))}.col-5-lg{flex:0 0 calc((100% / (12/5)) - var(--grid-gutter));max-width:calc((100% / (12/5)) - var(--grid-gutter))}.col-6-lg{flex:0 0 calc((100% / (12/6)) - var(--grid-gutter));max-width:calc((100% / (12/6)) - var(--grid-gutter))}.col-7-lg{flex:0 0 calc((100% / (12/7)) - var(--grid-gutter));max-width:calc((100% / (12/7)) - var(--grid-gutter))}.col-8-lg{flex:0 0 calc((100% / (12/8)) - var(--grid-gutter));max-width:calc((100% / (12/8)) - var(--grid-gutter))}.col-9-lg{flex:0 0 calc((100% / (12/9)) - var(--grid-gutter));max-width:calc((100% / (12/9)) - var(--grid-gutter))}.col-10-lg{flex:0 0 calc((100% / (12/10)) - var(--grid-gutter));max-width:calc((100% / (12/10)) - var(--grid-gutter))}.col-11-lg{flex:0 0 calc((100% / (12/11)) - var(--grid-gutter));max-width:calc((100% / (12/11)) - var(--grid-gutter))}.col-12-lg{flex:0 0 calc((100% / (12/12)) - var(--grid-gutter));max-width:calc((100% / (12/12)) - var(--grid-gutter))}}ul{list-style:none;padding-left:0}summary{cursor:pointer;user-select:none;-moz-user-select:none;-webkit-user-select:none}details>summary{list-style:none}summary::-webkit-details-marker{display:none}details>summary::before{content:'‚Üí';font-size:.79rem;padding-right:5.25px;cursor:pointer}details[open]>summary::before{font-size:.79rem;padding-right:5px;content:'‚Üì'}.small{width:80px;height:80px}.clickable.badge,a.badge{font-size:.8rem;background-color:hsla(208,67%,74%,.478);border:solid 1px hsla(208,67%,74%,.741);padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.clickable.badge:active,.clickable.badge:focus,.clickable.badge:hover,a.badge:hover{color:#fff;background-color:#075eab;border-color:#003d73}.inverted.clickable.badge{font-size:.75rem;color:#fff;background-color:#075eab;border-color:#003d73;padding:.3rem;padding-left:.7rem;padding-right:.7rem;border-radius:.25rem;text-decoration:none;transition:all .25s ease-in-out}.inverted.clickable.badge:active,.inverted.clickable.badge:focus,.inverted.clickable.badge:hover{color:#fff;background-color:#003e75}.badge{cursor:pointer;font-size:.8rem;background-color:#f4f4f4;border:solid 1px #ddd;padding:.25rem;padding-left:.4rem;padding-right:.4rem;border-radius:.25rem}div.avatar{background-color:#003d73;color:#fff;text-align:center;vertical-align:center;margin-right:20px;border-radius:.5rem}.text-avatar{color:#fff;display:flex;align-items:center;justify-content:center}div.avatar>span{font-size:1.8rem}.person-container{display:grid;grid-template-columns:repeat(auto-fill,minmax(300px,1fr));grid-gap:20px;padding:0;padding-top:.8rem}.person-member{display:flex;align-items:start!important;margin-bottom:5px;padding-bottom:20px}img{object-fit:cover;background-color:hsla(208,100%,23%,.366)}.person-member img{width:150px;height:150px;border:solid 1px hsla(208,100%,23%,.242);border-radius:.5rem;margin-right:20px;object-fit:cover;transition:all .25s ease-in-out}.person-member img:hover{width:150px;height:150px;margin-right:20px;object-fit:cover}.person-info h4{margin:0 0 5px 0}.person-info p{margin:0;color:#666}header{display:flex;position:fixed;zIndex:2000;padding-left:1rem;align-items:center;top:0;width:100%;height:42px;background-color:#003d73;z-index:1000}.flexible-space{display:flex;flex-grow:1}.brand-text{color:#fff;font-size:smaller;font-weight:600}nav{display:flex;gap:1rem;padding-right:2rem}nav>a{display:flex;font-size:.8rem;font-weight:600;color:rgba(255,255,255,.5)}a:active,a:focus,nav>a:hover{color:#fff}.hamburger-menu{display:none;flex-direction:column;cursor:pointer;padding-right:2rem}.hamburger-menu .line{width:15px;height:2px;background-color:#fff;margin:2px 0}@media screen and (min-width:769px){#navbar{display:flex}}@media screen and (max-width:768px){#navbar{display:none}.hamburger-menu{display:flex}nav{z-index:2000;position:absolute;top:42px;left:0;right:10px;padding-left:1rem;padding-bottom:1rem;background-color:#003d73;flex-direction:column;width:100%}nav a{width:100%;text-align:center}.sticky-header.fixed{display:none}}main{margin:auto;margin-top:25px;padding:.25rem;padding-bottom:0;max-width:120em}.sticky-header{left:0;right:0;position:relative;color:#000}.sticky-header.fixed{margin-top:0;position:fixed;top:40px;width:100%;box-sizing:border-box;z-index:0}.sticky-header.fixed.absolute{position:absolute}.hero-jumbotron{margin-top:.8rem;display:flex;flex-direction:row;justify-content:center}.hero-logo{display:flex;flex-direction:column;width:150px;padding:1rem;padding-bottom:0}.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:3.5rem;padding-bottom:20px;font-weight:700;color:#003d73}.hero-description-detail{text-align:center;font-size:2rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}@media screen and (min-width:769px){.hero-description{padding-left:20%;padding-right:20%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600}.hero-description-detail{text-align:center;font-size:1.5rem;font-weight:300;line-height:1.5}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:1rem;font-weight:300;line-height:1.5}}@media screen and (max-width:768px){.hero-description{padding-left:5%;padding-right:5%}.hero-description-title{text-align:center;font-size:2.5rem;padding-bottom:20px;font-weight:600;line-height:1.25}.hero-description-detail{text-align:center;font-size:1.125rem;line-height:1.75;font-weight:300}.hero-description-detail-funding{padding-top:20px;padding-bottom:20px;text-align:center;font-size:.8rem;line-height:1.5;font-weight:300}}</style>
  </head>
  <body>
    <header>
      <div class="brand-text">
        <a href="/" style="text-decoration: none; color: white;">Center for Anytime Anywhere Analytics</a>
      </div>
      <div class="flexible-space"></div>
      <div class="hamburger-menu" onclick="toggleMenu()">
          <div class="line"></div>
          <div class="line"></div>
          <div class="line"></div>
      </div>
      <nav id="navbar">
          <a href="/">Home</a>
          <a href="/members">Members</a>
          <a href="/publications">Publications</a>
          <a href="/open-positions">Open positions</a>
          <a href="https://github.com/orgs/anytime-anywhere-analytics/" target="_blank">Github <sup>&nbsp;‚Üó</sup></a>
      </nav>
    </header>
    <main>
      <div style="margin:auto; padding:1rem; padding-top: 0px; max-width:120em;">
  <div style="display: flex; flex-direction: column; background-color: #0273A8; color: white; margin-left: -20px; margin-right: -20px; padding-left: 20px; padding-right: 20px; padding-bottom: 20px">
    <h4 style="margin-bottom: 0px; padding-top: 10px">Publications coauthored by</h4>
    <h2 style="margin-top: 0px; margin-bottom: 0px">Deok Gun Park</h2>
  </div>
  <ul>
      
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Chalbi2020" onClick="copy('Chalbi2020')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Chalbi2020" style="display: none;">
                      @article{Chalbi2020,
title={Common Fate for Animated Transitions in Visualization},
author={Amira Chalbi and Jacob Ritchie and Deok Gun Park and Jungu Choi and Nicolas Roussel and Niklas Elmqvist and Fanny Chevalier},
url={http://users.umiacs.umd.edu/~elm/projects/common-fate/common-fate.pdf},
year={2020},
date={2020-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
volume={26},
number={1},
abstract={The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion &gt; (dynamic luminance, size, luminance); dynamic size &gt; (dynamic luminance, position); and dynamic luminance &gt; size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics ‚Ä¢ <a class="secondary" style="font-weight: 500" href="/publications/year/2020">2020</a></small>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/common-fate/common-fate.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Common Fate for Animated Transitions in Visualization</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/amira-chalbi/">Amira Chalbi</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jacob-ritchie/">Jacob Ritchie</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Deok Gun Park</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jungu-choi/">Jungu Choi</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/nicolas-roussel/">Nicolas Roussel</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/fanny-chevalier/">Fanny Chevalier</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion &gt; (dynamic luminance, size, luminance); dynamic size &gt; (dynamic luminance, position); and dynamic luminance &gt; size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Choi2019" onClick="copy('Choi2019')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Choi2019" style="display: none;">
                      @article{Choi2019,
title={Visualizing for the Non‚ÄêVisual: Enabling the Visually Impaired to Use Visualization},
author={Jinho Choi and Sanghun Jung and Deok Gun Park and Jaegul Choo and Niklas Elmqvist},
url={http://users.umiacs.umd.edu/~elm/projects/vis4nonvisual/vis4nonvisual.pdf},
year={2019},
date={2019-06-01},
journal={Computer Graphics Forum},
volume={38},
number={3},
pages={249--260},
abstract={The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep‚Äêneural‚Äênetwork‚Äêbased approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back‚Äêend algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/computer-graphics-forum">Computer Graphics Forum ‚Ä¢ <a class="secondary" style="font-weight: 500" href="/publications/year/2019">2019</a></small>
                  <a href="http://users.umiacs.umd.edu/~elm/projects/vis4nonvisual/vis4nonvisual.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Visualizing for the Non‚ÄêVisual: Enabling the Visually Impaired to Use Visualization</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jinho-choi/">Jinho Choi</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/sanghun-jung/">Sanghun Jung</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Deok Gun Park</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jaegul-choo/">Jaegul Choo</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep‚Äêneural‚Äênetwork‚Äêbased approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back‚Äêend algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Park2018" onClick="copy('Park2018')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Park2018" style="display: none;">
                      @article{Park2018,
title={ATOM: A Grammar for Unit Visualization},
author={Deok Gun Park and Steven M. Drucker and Roland Fernandez and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/atom/atom.pdf},
year={2018},
date={2018-01-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark---a visual unit---during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user&#39;s mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics ‚Ä¢ <a class="secondary" style="font-weight: 500" href="/publications/year/2018">2018</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/atom/atom.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">ATOM: A Grammar for Unit Visualization</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Deok Gun Park</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/steven-m-drucker/">Steven M. Drucker</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/roland-fernandez/">Roland Fernandez</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark---a visual unit---during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user&#39;s mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Park2017" onClick="copy('Park2017')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Park2017" style="display: none;">
                      @article{Park2017,
title={ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding},
author={Deok Gun Park and Seungyeon Kim and Jurim Lee and Jaegul Choo and Nicholas Diakopoulos and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/conceptvector/conceptvector.pdf},
year={2017},
date={2017-10-01},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
abstract={Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics ‚Ä¢ <a class="secondary" style="font-weight: 500" href="/publications/year/2017">2017</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/conceptvector/conceptvector.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Deok Gun Park</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/seungyeon-kim/">Seungyeon Kim</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jurim-lee/">Jurim Lee</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jaegul-choo/">Jaegul Choo</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/nicholas-diakopoulos/">Nicholas Diakopoulos</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Park2016" onClick="copy('Park2016')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Park2016" style="display: none;">
                      @inproceedings{Park2016,
title={Supporting Comment Moderators in identifying High Quality Online News Comments},
author={Deok Gun Park and Simranjit Singh and Nicholas Diakopoulos and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/commentiq/commentiq.pdf},
year={2016},
date={2016-05-05},
booktitle={Proceedings of the ACM Conference on Human Factors in Computing Systems},
journal={Proceedings of the ACM Conference on Human Factors in Computing Systems},
pages={1114--1125},
abstract={Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/proceedings-of-the-acm-conference-on-human-factors-in-computing-systems">Proceedings of the ACM Conference on Human Factors in Computing Systems ‚Ä¢ <a class="secondary" style="font-weight: 500" href="/publications/year/2016">2016</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/commentiq/commentiq.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">Supporting Comment Moderators in identifying High Quality Online News Comments</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <span class="badge" style="font-weight: 500;">Deok Gun Park</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/simranjit-singh/">Simranjit Singh</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/nicholas-diakopoulos/">Nicholas Diakopoulos</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.</p>
                      </details>
                      
                  </div>
              </div>
          </li>
      
          <li style="display: flex; margin-bottom: 1rem;">
              <div style="padding: 0.5rem;">
                  <button style="cursor:copy; border-radius: 2rem; font-size: 0.68rem;" class="clickable badge inverted" id="bibtex-button-Choi2015" onClick="copy('Choi2015')">
                    Copy BibTeX
                </button>
                  <div id="bibtex-Choi2015" style="display: none;">
                      @article{Choi2015,
title={VisDock: A Toolkit for Cross-Cutting Interactions in Visualization},
author={Jungu Choi and Deok Gun Park and Yuetling Wong and Eli Raymond Fisher and Niklas Elmqvist},
url={http://www.umiacs.umd.edu/~elm/projects/visdock/visdock.pdf},
video={https://www.youtube.com/watch?v=LUC-nGR-fOk},
year={2015},
date={2015-03-21},
journal={IEEE Transactions on Visualization &amp; Computer Graphics},
volume={21},
number={9},
pages={1087--1100},
abstract={Standard user applications provide a range of cross-cutting interaction techniques that are common to virtually all such tools: selection, filtering, navigation, layer management, and cut-and-paste.We present VisDock, a JavaScript mixin library that provides a core set of these cross-cutting interaction techniques for visualization, including selection (lasso, paths, shape selection, etc), layer management (visibility, transparency, set operations, etc), navigation (pan, zoom, overview, magnifying lenses, etc), and annotation (point-based, region-based, data-space based, etc). To showcase the utility of the library, we have released it as Open Source and integrated it with a large number of existing web-based visualizations. Furthermore, we have evaluated VisDock using qualitative studies with both developers utilizing the toolkit to build new web-based visualizations, as well as with end-users utilizing it to explore movie ratings data. Results from these studies highlight the usability and effectiveness of the toolkit from both developer and end-user perspectives.},
keywords={},
}
                  </div>
                <small><a class="secondary" style="font-weight: 500" href="/publications/venue/ieee-transactions-on-visualization-and-computer-graphics">IEEE Transactions on Visualization &amp; Computer Graphics ‚Ä¢ <a class="secondary" style="font-weight: 500" href="/publications/year/2015">2015</a></small>
                  <a href="http://www.umiacs.umd.edu/~elm/projects/visdock/visdock.pdf"><h3 style="margin: 0.15rem; margin-bottom: 0.3rem">VisDock: A Toolkit for Cross-Cutting Interactions in Visualization</h3></a>
                  <p style="margin: 0.15rem; line-height: 1.9">
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/jungu-choi/">Jungu Choi</span></a>
                      
                    
                      
                        <span class="badge" style="font-weight: 500;">Deok Gun Park</span>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/yuetling-wong/">Yuetling Wong</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/eli-raymond-fisher/">Eli Raymond Fisher</span></a>
                      
                    
                      
                        <a class="badge" style="font-weight: 500" href="/publications/members/niklas-elmqvist/">Niklas Elmqvist</span></a>
                      
                    
                  </p>
                  <div style="display: inline-flex; gap: 1rem;">
                      <details>
                          <summary style="margin-top:1px"><small style="text-decoration: underline; text-decoration-color: rgba(0,0,0,0.15); text-underline-offset: 3px;">Click to read abstract</small></summary>
                          <p class="badge" style="font-size: 0.9rem; background-color: hsla(208, 100%, 23%, 0.0625); border-color: hsla(208, 100%, 23%, 0.125); padding: 2rem; margin-top: 6px; border-radius: 0px; margin-left: -2rem; margin-right: -2rem; line-height: 1.725">Standard user applications provide a range of cross-cutting interaction techniques that are common to virtually all such tools: selection, filtering, navigation, layer management, and cut-and-paste.We present VisDock, a JavaScript mixin library that provides a core set of these cross-cutting interaction techniques for visualization, including selection (lasso, paths, shape selection, etc), layer management (visibility, transparency, set operations, etc), navigation (pan, zoom, overview, magnifying lenses, etc), and annotation (point-based, region-based, data-space based, etc). To showcase the utility of the library, we have released it as Open Source and integrated it with a large number of existing web-based visualizations. Furthermore, we have evaluated VisDock using qualitative studies with both developers utilizing the toolkit to build new web-based visualizations, as well as with end-users utilizing it to explore movie ratings data. Results from these studies highlight the usability and effectiveness of the toolkit from both developer and end-user perspectives.</p>
                      </details>
                      
                          <a href="https://www.youtube.com/watch?v=LUC-nGR-fOk" style="font-weight: 500"><small>üì∫ Video</small></a>
                      
                  </div>
              </div>
          </li>
      
  </ul>
</div>

    </main>
    
    <script>
      const copy=e=>{var t=document.getElementById(`bibtex-button-${e}`),o=document.getElementById(`bibtex-${e}`).textContent.trim();navigator.clipboard.writeText(o).then((()=>{t.innerText="‚úî Copied BibTeX",setInterval((()=>{t.innerText="‚úö Copy BibTeX"}),1e3)})).catch((e=>{console.error("Failed to copy: ",e)}))};function toggleMenu(){var e=document.getElementById("navbar");"none"===e.style.display||""===e.style.display?e.style.display="flex":e.style.display="none"}var stickyHeaders=function(){var e,t=function(){Array.from(e).forEach((function(t,o){if(t.originalPosition<=window.scrollY){t.classList.add("fixed"),t.style.color="white",t.style.zIndex=1e3,t.style.fontSize="0.8rem",t.style.padding="0.25rem",t.style.paddingLeft="1rem",t.style.paddingBottom="0.4rem",t.style.backgroundColor="#0273A8";var n=e[o+1];if(n){var i=n.originalPosition-t.originalHeight;t.offsetTop>=i&&(t.classList.add("absolute"),t.style.top=i+"px")}}else{t.classList.remove("fixed"),t.style.color="black",t.style.zIndex=-1e3,t.style.fontSize="1.2rem",t.style.padding="0px",t.style.backgroundColor="transparent";var r=e[o-1];r&&window.scrollY<=t.originalPosition-t.originalHeight&&(r.classList.remove("absolute"),r.style.removeProperty("top"))}}))};return{load:function(o){(e=document.querySelectorAll(o)).length>0&&(Array.from(e).forEach((function(e){var t=document.createElement("div");t.className="followWrap",e.parentNode.insertBefore(t,e),t.appendChild(e),e.originalPosition=e.offsetTop,e.originalHeight=e.offsetHeight,t.style.height=e.offsetHeight+"px"})),window.removeEventListener("scroll",t),window.addEventListener("scroll",t))}}}();document.addEventListener("DOMContentLoaded",(function(){stickyHeaders.load(".sticky-header")}));</script>
  </body>
</html>