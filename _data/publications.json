[
    {
        "type": "article",
        "id": "elmqvist2023DataAnalyticsAnywhere",
        "title": "Data Analytics Anywhere and Everywhere",
        "author": [
            "Niklas Elmqvist"
        ],
        "abstract": "Mobile, ubiquitous, and immersive computing appear poised to transform visualization, data science, and data-driven decision making.",
        "url": "https://doi.org/10.1145/3584858",
        "_url_type": "doi",
        "year": 2023,
        "date": "2023-01-01T00:00:00.000Z",
        "journal": "Communications of the ACM",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": 66,
        "number": 12,
        "pages": "52--63",
        "doi": "10.1145/3584858",
        "issn": "0001-0782",
        "language": "English",
        "keywords": [],
        "_type_counter": 86
    },
    {
        "type": "inproceedings",
        "id": "berger2024ScientistsCodeProgramming",
        "title": "Scientists and Code: Programming as a Tool",
        "author": [
            "Caroline Berger",
            "Matthew Christian Demuth Lutze",
            "Niklas Elmqvist",
            "Magnus Madsen",
            "Clemens Nylandsted Klokmose"
        ],
        "year": 2024,
        "date": "2024-01-19T00:00:00.000Z",
        "booktitle": "Proceedings of the 14th Annual Workshop on the Intersection of HCI and PL (PLATEAU 2024)",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "abstract": "Many scientists use programming to analyze their data. In this paper, we explore the computational ecosystem of scientists and their socio-technical system of computing through a human-centered approach. By employing contextual inquiry techniques with nine scientists drawn from fields such as theoretical physics, biomedical science, and entomology, we learned that programming is a tool for scientists, and as such the output is more important than the code itself. We found that during analysis, scientists often write code to create plots, and then compare these plots to assess the match of output to their expectation. Participants used ChatGPT while coding. We also found that scientists' programming tools and practices often limit their analysis. Finally, based on a combined human-computer interaction and programming language analysis, we identify drivers and blockers of scientists' work. Our findings uncover opportunities for the design of programming tools and languages.",
        "language": "English",
        "keywords": [
            "end-user programming",
            "programming practices",
            "tool design",
            "contextual inquiry"
        ],
        "_type_counter": 65
    },
    {
        "type": "article",
        "id": "shin2024RealitySituationSurvey",
        "title": "The Reality of the Situation: A Survey of Situated Analytics",
        "author": [
            "Sungbok Shin",
            "Andrea Batch",
            "Peter W. S. Butcher",
            "Panagiotis D. Ritsos",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3285546",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 8,
        "pages": "5147--5164",
        "doi": "10.1109/TVCG.2023.3285546",
        "issn": "1077-2626",
        "abstract": "The advent of low-cost, accessible, and high-performance augmented reality (AR) has shed light on a situated form of analytics where in-situ visualizations embedded in the real world can facilitate sensemaking based on the user's physical location. In this work, we identify prior literature in this emerging field with a focus on situated analytics. After collecting 47 relevant situated analytics systems, we classify them using a taxonomy of three dimensions: situating triggers, view situatedness, and data depiction. We then identify four archetypical patterns in our classification using an ensemble cluster analysis. We also assess the level which these systems support the sensemaking process. Finally, we discuss insights and design guidelines that we learned from our analysis.",
        "language": "English",
        "keywords": [
            "augmented reality",
            "data visualization",
            "immersive analytics",
            "situated analytics",
            "situated visualization"
        ],
        "_type_counter": 87
    },
    {
        "type": "article",
        "id": "kim2024TowardsVisualizationThumbnail",
        "title": "Towards Visualization Thumbnail Designs That Entice Reading Data-Driven Articles",
        "author": [
            "Hwiyeon Kim",
            "Joohee Kim",
            "Yunha Han",
            "Hwajung Hong",
            "Kwon, Oh Sang",
            "Park, Young Woo",
            "Niklas Elmqvist",
            "Sungahn Ko",
            "Kwon, Bum Chul"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3278304",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 8,
        "pages": "4825--4840",
        "doi": "10.1109/TVCG.2023.3278304",
        "issn": "1077-2626",
        "abstract": "As online news increasingly include data journalism, there is a corresponding increase in the incorporation of visualization in article thumbnail images. However, little research exists on the design rationale for visualization thumbnails, such as resizing, cropping, simplifying, and embellishing charts that appear within the body of the associated article. Therefore, in this paper we aim to understand these design choices and determine what makes a visualization thumbnail inviting and interpretable. To this end, we first survey visualization thumbnails collected online and discuss visualization thumbnail practices with data journalists and news graphics designers. Based on the survey and discussion results, we then define a design space for visualization thumbnails and conduct a user study with four types of visualization thumbnails derived from the design space. The study results indicate that different chart components play different roles in attracting reader attention and enhancing reader understandability of the visualization thumbnails. We also find various thumbnail design strategies for effectively combining the charts' components, such as a data summary with highlights and data labels, and a visual legend with text labels and Human Recognizable Objects (HROs), into thumbnails. Ultimately, we distill our findings into design implications that allow effective visualization thumbnail designs for data-rich news articles. Our work can thus be seen as a first step toward providing structured guidance on how to design compelling thumbnails for data stories.",
        "language": "English",
        "keywords": [
            "data journalism",
            "data stories",
            "data-driven storytelling",
            "online news",
            "thumbnail images",
            "visualization"
        ],
        "_type_counter": 88
    },
    {
        "type": "article",
        "id": "hoque2024DataopsyScalableFluid",
        "title": "Dataopsy: Scalable and Fluid Visual Exploration using Aggregate Query Sculpting",
        "author": [
            "Md Naimul Hoque",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3326594",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 1,
        "pages": "186--196",
        "doi": "10.1109/TVCG.2023.3326594",
        "issn": "1077-2626",
        "abstract": "We present aggregate query sculpting (AQS), a faceted visual query technique for large-scale multidimensional data. As a 'born scalable' query technique, AQS starts visualization with a single visual mark representing an aggregation of the entire dataset. The user can then progressively explore the dataset through a sequence of operations abbreviated as P6: pivot (facet an aggregate based on an attribute), partition (lay out a facet in space), peek (see inside a subset using an aggregate visual representation), pile (merge two or more subsets), project (extracting a subset into a new substrate), and prune (discard an aggregate not currently of interest). We validate AQS with Dataopsy, a prototype implementation of AQS that has been designed for fluid interaction on desktop and touch-based mobile devices. We demonstrate AQS and Dataopsy using two case studies and three application examples.",
        "language": "English",
        "keywords": [
            "multidimensional data visualization",
            "multivariate graphs",
            "visual exploration",
            "visual queries"
        ],
        "_type_counter": 89
    },
    {
        "type": "article",
        "id": "raghunandan2024LodestarSupportingRapid",
        "title": "Lodestar: Supporting Rapid Prototyping of Data Science Workflows through Data-Driven Analysis Recommendations",
        "author": [
            "Deepthi Raghunandan",
            "Zhe Cui",
            "Kartik Krishnan",
            "Segen Tirfe",
            "Shenzhi Shi",
            "Tejaswi Darshan Shrestha",
            "Leilani Battle",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1177/14738716231190429",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "publisher": "SAGE Publications Inc.",
        "address": "Thousand Oaks, CA, USA",
        "volume": 23,
        "number": 1,
        "pages": "21--39",
        "doi": "10.1177/14738716231190429",
        "issn": "1473-8716",
        "abstract": "Keeping abreast of current trends, technologies, and best practices in visualization and data analysis is becoming increasingly difficult, especially for fledgling data scientists. In this paper, we propose lodestar, an interactive computational notebook that allows users to quickly explore and construct new data science workflows by selecting from a list of automated analysis recommendations. We derive our recommendations from directed graphs of known analysis states, with two input sources: one manually curated from online data science tutorials, and another extracted through semi-automatic analysis of a corpus of over 6000 Jupyter notebooks. We validated Lodestar through three separate user studies: first a formative evaluation involving novices learning data science using the tool. We used the feedback from this study to improve the tool. This was followed by a summative study involving both new and returning participants from the formative evaluation to test the efficacy of our improvements. We also engaged professional data scientists in an expert review assessing the utility of the different recommendations. Overall, our results suggest that both novice and professional users find Lodestar useful for rapidly creating data science workflows.",
        "language": "English",
        "keywords": [
            "computational notebook",
            "data science",
            "Markov chain",
            "Python",
            "visualization recommendation"
        ],
        "_type_counter": 90
    },
    {
        "type": "article",
        "id": "devalk2024RiversideDesignStudy",
        "title": "Riverside: A Design Study on Visualization for Situation Awareness in Cybersecurity",
        "author": [
            "Kaitlyn DeValk",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1177/14738716231189220",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "publisher": "SAGE Publications Inc.",
        "address": "Thousand Oaks, CA, USA",
        "volume": 23,
        "number": 1,
        "pages": "40--66",
        "doi": "10.1177/14738716231189220",
        "issn": "1473-8716",
        "abstract": "Real-time situation awareness is a key challenge of cybersecurity defense. Visual analytics has been utilized for this purpose, but existing tools tend to require detailed knowledge about the network, which can be challenging in large-scale, production networks. We conducted an interview study involving 24 security professionals to gather requirements for the design, development, and evaluation of visualization to aid situation awareness in cybersecurity. Using these findings, we designed a visualization tool -- called RIVERSIDE -- for providing a real-time view of the dynamically changing computer network to support situation awareness. We evaluated Riverside in a user study involving 10 participants. Participants were placed in an incident response scenario that tasked them to identify malicious activity on a network. 20\\% of the users identified all attack component, while an additional 40\\% only missed one component.",
        "language": "English",
        "keywords": [
            "cybersecurity situation awareness",
            "evaluation",
            "network security visualization"
        ],
        "_type_counter": 91
    },
    {
        "type": "article",
        "id": "chundury2024TactualPlotSpatializingData",
        "title": "TactualPlot: Spatializing Data as Sound Using Sensory Substitution for Touchscreen Accessibility",
        "author": [
            "Pramod Chundury",
            "Yasmin Reyazuddin",
            "Jordan, J. Bern",
            "Jonathan Lazar",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3326937",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 1,
        "pages": "836--846",
        "doi": "10.1109/TVCG.2023.3326937",
        "issn": "1077-2626",
        "abstract": "Tactile graphics are one of the best ways for a blind person to perceive a chart using touch, but their fabrication is often costly, time-consuming, and does not lend itself to dynamic exploration. Refreshable haptic displays tend to be expensive and thus unavailable to most blind individuals. We propose TactualPlot, an approach to sensory substitution where touch interaction yields auditory (sonified) feedback. The technique relies on embodied cognition for spatial awareness - i.e., individuals can perceive 2D touch locations of their fingers with reference to other 2D locations such as the relative locations of other fingers or chart characteristics that are visualized on touchscreens. Combining touch and sound in this way yields a scalable data exploration method for scatterplots where the data density under the user's fingertips is sampled. The sample regions can optionally be scaled based on how quickly the user moves their hand. Our development of TactualPlot was informed by formative design sessions with a blind collaborator, whose practice while using tactile scatterplots caused us to expand the technique for multiple fingers. We present results from an evaluation comparing our TactualPlot interaction technique to tactile graphics printed on swell touch paper.",
        "language": "English",
        "keywords": [
            "accessibility",
            "crossmodal interaction",
            "multimodal interaction",
            "sonification",
            "visualization"
        ],
        "_type_counter": 92
    },
    {
        "type": "article",
        "id": "newburger2024VisualizationAccordingStatisticians",
        "title": "Visualization According to Statisticians: An Interview Study on the Role of Visualization for Inferential Statistics",
        "author": [
            "Eric Newburger",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3326521",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 1,
        "pages": "230--239",
        "doi": "10.1109/TVCG.2023.3326521",
        "issn": "1077-2626",
        "abstract": "Statisticians are not only one of the earliest professional adopters of data visualization, but also some of its most prolific users. Understanding how these professionals utilize visual representations in their analytic process may shed light on best practices for visual sensemaking. We present results from an interview study involving 18 professional statisticians (19.7 years average in the profession) on three aspects: (1) their use of visualization in their daily analytic work; (2) their mental models of inferential statistical processes; and (3) their design recommendations for how to best represent statistical inferences. Interview sessions consisted of discussing inferential statistics, eliciting participant sketches of suitable visual designs, and finally, a design intervention with our proposed visual designs. We analyzed interview transcripts using thematic analysis and open coding, deriving thematic codes on statistical mindset, analytic process, and analytic toolkit. The key findings for each aspect are as follows: (1) statisticians make extensive use of visualization during all phases of their work (and not just when reporting results); (2) their mental models of inferential methods tend to be mostly visually based; and (3) many statisticians abhor dichotomous thinking. The latter suggests that a multi-faceted visual display of inferential statistics that includes a visual indicator of analytically important effect sizes may help to balance the attributed epistemic power of traditional statistical testing with an awareness of the uncertainty of sensemaking.",
        "language": "English",
        "keywords": [
            "inferential statistics",
            "qualitative interview study",
            "statistical visualization",
            "thematic coding"
        ],
        "_type_counter": 93
    },
    {
        "type": "article",
        "id": "batch2024WizualizationHardMagic",
        "title": "Wizualization: A 'Hard Magic' Visualization System for Immersive and Ubiquitous Analytics",
        "author": [
            "Andrea Batch",
            "Peter W. S. Butcher",
            "Panagiotis D. Ritsos",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3326580",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 1,
        "pages": "507--517",
        "doi": "10.1109/TVCG.2023.3326580",
        "issn": "1077-2626",
        "abstract": "What if magic could be used as an effective metaphor to perform data visualization and analysis using speech and gestures while mobile and on-the-go? In this paper, we introduce Wizualization, a visual analytics system for eXtended Reality (XR) that enables an analyst to author and interact with visualizations using such a magic system through gestures, speech commands, and touch interaction. Wizualization is a rendering system for current XR headsets that comprises several components: a cross-device (or Arcane Focuses) infrastructure for signalling and view control (Weave), a code notebook (Spellbook), and a grammar of graphics for XR (Optomancy). The system offers users three modes of input: gestures, spoken commands, and materials. We demonstrate Wizualization and its components using a motivating scenario on collaborative data analysis of pandemic data across time and space.",
        "language": "English",
        "keywords": [
            "gestural interaction",
            "immersive analytics",
            "situated analytics",
            "ubiquitous analytics",
            "voice interaction"
        ],
        "_type_counter": 94
    },
    {
        "type": "article",
        "id": "butcher2024IsNativeNaive",
        "title": "Is Native Naive? Comparing Native Game Engines and WebXR as Immersive Analytics Development Platforms",
        "author": [
            "Peter W. S. Butcher",
            "Andrea Batch",
            "David Saffo",
            "Blair Macintyre",
            "Niklas Elmqvist",
            "Panagiotis D. Ritsos"
        ],
        "url": "https://doi.org/10.1109/MCG.2024.3367422",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Computer Graphics and Applications",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 44,
        "number": 3,
        "pages": "91--98",
        "doi": "10.1109/MCG.2024.3367422",
        "issn": "0272-1716",
        "abstract": "Native game engines have long been the 3-D development platform of choice for research in mixed and augmented reality. For this reason, they have also been adopted in many immersive visualization and immersive analytics systems and toolkits. However, with the rapid improvements of WebXR and related open technologies, this choice may not always be optimal for future visualization research. In this article, we investigate common assumptions about native game engines versus WebXR and find that while native engines still have an advantage in many areas, WebXR is rapidly catching up and is superior for many immersive analytics applications.",
        "language": "English",
        "keywords": [
            "immersive analytics",
            "augmented reality",
            "WebXR",
            "game engines",
            "visualization"
        ],
        "_type_counter": 95
    },
    {
        "type": "inproceedings",
        "id": "hoque2024HaLLMarkEffect",
        "title": "The HaLLMark Effect: Supporting Provenance and Transparent Use of Large Language Models in Writing with Interactive Visualization",
        "author": [
            "Hoque, Md Naimul",
            "Tasfa Mashiat",
            "Bhavya Ghai",
            "Cecilia Shelton",
            "Fanny Chevalier",
            "Kari Kraus",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1145/3613904.3641895",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-11T00:00:00.000Z",
        "booktitle": "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "pages": "1--17",
        "doi": "10.1145/3613904.3641895",
        "abstract": "The use of Large Language Models (LLMs) for writing has sparked controversy both among readers and writers. On one hand, writers are concerned that LLMs will deprive them of agency and ownership, and readers are concerned about spending their time on text generated by soulless machines. On the other hand, AI-assistance can improve writing as long as writers can conform to publisher policies, and as long as readers can be assured that a text has been verifed by a human. We argue that a system that captures the provenance of interaction with an LLM can help writers retain their agency, conform to policies, and communicate their use of AI to publishers and readers transparently. Thus we propose HaLLMark, a tool for visualizing the writer's interaction with the LLM. We evaluated HaLLMark with 13 creative writers, and found that it helped them retain a sense of control and ownership of the text.",
        "series": "CHI '24",
        "articleno": "285",
        "numpages": 17,
        "location": "Honolulu, HI, USA",
        "language": "English",
        "keywords": [
            "agency",
            "co-writing",
            "creative writing",
            "LLMs",
            "visualization"
        ],
        "_type_counter": 66
    },
    {
        "type": "inproceedings",
        "id": "patnaik2024VisTorch",
        "title": "VisTorch: Interacting with Situated Visualizations using Handheld Projectors",
        "author": [
            "Biswaksen Patnaik",
            "Huaishu Peng",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1145/3613904.3642857",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-11T00:00:00.000Z",
        "booktitle": "Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "pages": "1--15",
        "doi": "10.1145/3613904.3642857",
        "abstract": "Spatial data is best analyzed in situ, but existing mixed reality technologies can be bulky, expensive, or unsuitable for collaboration. We present VisTorch: a handheld device for projected situated analytics consisting of a pico-projector, a multi-spectrum camera, and a touch surface. VisTorch enables viewing charts situated in physical space by simply pointing the device at a surface to reveal visualizations in that location. We evaluated the approach using both a user study and an expert review. In the former, we asked 20 participants to first organize charts in space and then refer to these charts to answer questions. We observed three spatial and one temporal pattern in participant analyses. In the latter, four experts---a museum designer, a statistical software developer, a theater stage designer, and an environmental educator---utilized VisTorch to derive practical usage scenarios. Results from our study showcase the utility of situated visualizations for memory and recall.",
        "series": "CHI '24",
        "articleno": "603",
        "numpages": 15,
        "location": "Honolulu, HI, USA",
        "language": "English",
        "keywords": [
            "augmented reality",
            "immersive analytics",
            "situated visualization",
            "ubiquitous analytics"
        ],
        "_type_counter": 67
    },
    {
        "type": "article",
        "id": "batch2024uxSense",
        "title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision",
        "author": [
            "Andrea Batch",
            "Yipeng Ji",
            "Mingming Fan",
            "Jian Zhao",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2023.3241581",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 30,
        "number": 7,
        "pages": "3841--3856",
        "doi": "10.1109/TVCG.2023.3241581",
        "issn": "1077-2626",
        "abstract": "Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose UXSENSE, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",
        "language": "English",
        "keywords": [
            "computer vision",
            "deep learning",
            "evaluation",
            "machine learning",
            "video analytics",
            "visual analytics",
            "visualization"
        ],
        "_type_counter": 96
    },
    {
        "type": "inproceedings",
        "id": "chundury2024UnderstandingVisualization",
        "title": "Understanding the Visualization and Analytics Needs of Blind and Low-Vision Professionals",
        "author": [
            "Pramod Chundury",
            "Urja Thakkar",
            "Yasmin Reyazuddin",
            "Jordan, J. Bern",
            "Niklas Elmqvist",
            "Jonathan Lazar"
        ],
        "url": "https://doi.org/10.1145/3663548.3688496",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-28T00:00:00.000Z",
        "booktitle": "Proceedings of the 26th International ACM SIGACCESS Conference on Computers and Accessibility",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "pages": "1--17",
        "doi": "10.1145/3663548.3688496",
        "abstract": "Inclusivity for blind and low vision (BLV) professionals in data science and analytics is limited by a gap in understanding their unique data analysis needs. We contribute to the literature by reporting on a two-step online survey delving into the experiences and challenges faced by BLV individuals engaged in data-related roles. Our fndings highlight that despite expertise in programming and GUI-based analysis tools, BLV professionals faced accessibility issues at various points in the data analysis pipeline---issues ranging from data loading and transformation, availability and compatibility of data tools with assistive technology, and visualization authoring. The prevalent use of tools such as Excel, Python, and SAS alongside heavy reliance on assistive technologies highlights persistent accessibility challenges. Furthermore, frequent collaboration with sighted colleagues indicates compromised independence. These results underscore the urgent need for \"born accessible\" tools that ensure the inclusivity and autonomy of BLV professionals in the feld of data science.",
        "series": "ASSETS '24",
        "articleno": "63",
        "numpages": 17,
        "location": "St. John's, NL, Canada",
        "language": "English",
        "keywords": [
            "accessible data visualization",
            "blind and low vision users",
            "inclusive data analysis"
        ],
        "_type_counter": 68
    },
    {
        "type": "article",
        "id": "mane2024WichitaCombatFlight",
        "title": "``Wichita 1-1, Fox Three''---The Role of 3D Telemetry Analysis in Combat Flight Simulation",
        "author": [
            "Mane, Sourabh V.",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1145/3677114",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM on Human-Computer Interaction",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "volume": 8,
        "number": "CHI PLAY",
        "pages": "1--28",
        "doi": "10.1145/3677114",
        "issn": "2573-0142",
        "abstract": "Analyzing 3D telemetry data collected from competitive video games on the Internet can support players in improving performance as well as spectators in viewing data-driven narratives of the gameplay. In this paper, we conduct an in-depth qualitative study on the use of telemetry analysis by embedding over several weeks in a virtual F-14A Tomcat squadron in the multiplayer combat flight simulator DCS World (DCS) (2008). Based on formative interviews with DCS pilots, we design a web-based game analytics framework for rendering 3D telemetry from the flight simulator in a live 3D player, incorporating many of the data displays and visualizations requested by the participants. We then evaluate the framework with real mission data from several air-to-air engagements involving the virtual squadron. Our findings highlight the key role of 3D telemetry playback in competitive multiplayer gaming.",
        "articleno": "346",
        "numpages": 28,
        "language": "English",
        "keywords": [
            "3D telemetry",
            "computer games",
            "game analytics",
            "multiplayer games",
            "post-mortem analysis"
        ],
        "_type_counter": 97
    },
    {
        "type": "article",
        "id": "ondov2024VisualizingMultilayer",
        "title": "Visualizing Multilayer Spatiotemporal Epidemiological Data with Animated Geocircles",
        "author": [
            "Brian Ondov",
            "Harsh B. Patel",
            "Ai Te Kuo",
            "John Kastner",
            "Yunheng Han",
            "Hong Wei",
            "Niklas Elmqvist",
            "Hanan Samet"
        ],
        "url": "https://doi.org/10.1093/jamia/ocae234",
        "_url_type": "doi",
        "year": 2024,
        "date": "2024-01-01T00:00:00.000Z",
        "journal": "Journal of the American Medical Informatics Association",
        "publisher": "Oxford University Press",
        "address": "Oxford, UK",
        "volume": 31,
        "number": 11,
        "pages": "2507--2518",
        "doi": "10.1093/jamia/ocae234",
        "issn": "1067-5027",
        "abstract": "Objective: The COVID-19 pandemic emphasized the value of geospatial visual analytics for both epidemiologists and the general public. However, systems struggled to encode temporal and geospatial trends of multiple, potentially interacting variables, such as active cases, deaths, and vaccinations. We sought to ask (1) how epidemiologists interact with visual analytics tools, (2) how multiple, time-varying, geospatial variables can be conveyed in a unified view, and (3) how complex spatiotemporal encodings affect utility for both experts and non-experts. Materials and Methods: We propose encoding variables with animated, concentric, hollow circles, allowing multiple variables via color encoding and avoiding occlusion problems, and we implement this method in a browser-based tool called CoronaViz. We conduct task-based evaluations with non-experts, as well as in-depth interviews and observational sessions with epidemiologists, covering a range of tools and encodings. Results: Sessions with epidemiologists confirmed the importance of multivariate, spatiotemporal queries and the utility of CoronaViz for answering them, while providing direction for future development. Non-experts tasked with performing spatiotemporal queries unanimously preferred animation to multi-view dashboards. Discussion: We find that conveying complex, multivariate data necessarily involves trade-offs. Yet, our studies suggest the importance of complementary visualization strategies, with our animated multivariate spatiotemporal encoding filling important needs for exploration and presentation. Conclusion: CoronaViz's unique ability to convey multiple, time-varying, geospatial variables makes it both a valuable addition to interactive COVID-19 dashboards and a platform for empowering experts and the public during future disease outbreaks. CoronaViz is open-source and a live instance is freely hosted at http://coronaviz.umiacs.io.",
        "language": "English",
        "keywords": [
            "COVID-19",
            "data visualization",
            "epidemiology",
            "geographic information systems"
        ],
        "_type_counter": 98
    },
    {
        "type": "article",
        "id": "dhanoa2025AgenticVisualization",
        "title": "Agentic Visualization: Extracting Agent-based Design Patterns from Visualization Systems",
        "author": [
            "Vaishali Dhanoa",
            "Anton Sutje Wolter",
            "Gabriela Molina León",
            "Hans-Jörg Schulz",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/MCG.2025.3012345",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Computer Graphics and Applications",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "doi": "10.1109/MCG.2025.3012345",
        "issn": "0272-1716",
        "abstract": "Autonomous agents powered by Large Language Models are transforming AI, creating an imperative for the visualization area. However, our field's focus on a human in the sensemaking loop raises critical questions about autonomy, delegation, and coordination for such agentic visualization that preserve human agency while amplifying analytical capabilities. This paper addresses these questions by reinterpreting existing visualization systems with semi-automated or fully automatic AI components through an agentic lens. Based on this analysis, we extract a collection of design patterns for agentic visualization, including agentic roles, communication, and coordination. These patterns provide a foundation for future agentic visualization systems that effectively harness AI agents while maintaining human insight and control.",
        "note": "To appear",
        "language": "English",
        "keywords": [
            "agentic visualization",
            "design patterns",
            "autonomous agents",
            "large language models",
            "human-AI collaboration"
        ],
        "_type_counter": 99
    },
    {
        "type": "article",
        "id": "borowski2025DashSpace",
        "title": "DashSpace: A Live Collaborative Platform for Immersive and Ubiquitous Analytics",
        "author": [
            "Marcel Borowski",
            "Peter W. S. Butcher",
            "Kristensen, Janus Bager",
            "Jonas Oxenboll Petersen",
            "Panagiotis D. Ritsos",
            "Clemens Nylandsted Klokmose",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2025.3537679",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 31,
        "number": 10,
        "pages": "7034--7047",
        "doi": "10.1109/TVCG.2025.3537679",
        "issn": "1077-2626",
        "abstract": "We introduce DashSpace, a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar. To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants---whose presence is shown using 3D avatars and webcam feeds---to interact with the shared space synchronously, both co-located and remotely. We present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.",
        "language": "English",
        "keywords": [
            "augmented reality",
            "collaborative visualization",
            "extended reality",
            "immersive analytics",
            "ubiquitous analytics",
            "web-based technologies"
        ],
        "_type_counter": 100
    },
    {
        "type": "article",
        "id": "shin2025Drillboards",
        "title": "Drillboards: Adaptive Visualization Dashboards for Dynamic Personalization of Visualization Experiences",
        "author": [
            "Sungbok Shin",
            "Inyoup Na",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2025.3542606",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 31,
        "number": 10,
        "pages": "7196--7210",
        "doi": "10.1109/TVCG.2025.3542606",
        "issn": "1077-2626",
        "abstract": "We present drillboards, a technique for adaptive visualization dashboards consisting of a hierarchy of coordinated charts that the user can drill down to reach a desired level of detail depending on their expertise, interest, and desired effort. This functionality allows different users to personalize the same dashboard to their specific needs and expertise. The technique is based on a formal vocabulary of chart representations and rules for merging multiple charts of different types and data into single composite representations. The drillboard hierarchy is created by iteratively applying these rules starting from a baseline dashboard, with each consecutive operation yielding a new dashboard with fewer charts and progressively more abstract and simplified views. We also present an authoring tool for building drillboards and show how it can be applied to an agricultural dataset with hundreds of expert users. Our evaluation asked three domain experts to author drillboards for their own datasets, which we then showed to casual end-users with favorable outcomes.",
        "language": "English",
        "keywords": [
            "adaptive dashboards",
            "dashboards",
            "hierarchical aggregates",
            "information visualization",
            "personalization",
            "visualization"
        ],
        "_type_counter": 101
    },
    {
        "type": "article",
        "id": "shin2025Visualizationary",
        "title": "Visualizationary: Automating Design Feedback for Visualization Designers Using Large Language Models",
        "author": [
            "Sungbok Shin",
            "Sanghyun Hong",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2025.3579700",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 31,
        "number": 10,
        "pages": "8796--8813",
        "doi": "10.1109/TVCG.2025.3579700",
        "issn": "1077-2626",
        "abstract": "Interactive visualization editors empower users to author visualizations without writing code, but do not provide guidance on the art and craft of effective visual communication. In this article, we explore the potential of using an off-the-shelf large language models (LLMs) to provide actionable and customized feedback to visualization designers. Our implementation, Visualizationary, demonstrates how ChatGPT can be used for this purpose through two key components: a preamble of visualization design guidelines and a suite of perceptual filters that extract salient metrics from a visualization image. We present findings from a longitudinal user study involving 13 visualization designers - 6 novices, 4 intermediates, and 3 experts - who authored a new visualization from scratch over several days. Our results indicate that providing guidance in natural language via an LLM can aid even seasoned designers in refining their visualizations.",
        "language": "English",
        "keywords": [
            "visualization design",
            "design critique",
            "feedback",
            "human-centered AI",
            "large language models"
        ],
        "_type_counter": 102
    },
    {
        "type": "article",
        "id": "srinivasan2025AttentionAware",
        "title": "Attention-Aware Visualization: Tracking and Responding to User Perception Over Time",
        "author": [
            "Arvind Srinivasan",
            "Johannes Ellemose",
            "Peter W. S. Butcher",
            "Panagiotis D. Ritsos",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2024.3456300",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 31,
        "number": 1,
        "pages": "1017--1027",
        "doi": "10.1109/TVCG.2024.3456300",
        "issn": "1077-2626",
        "abstract": "We propose the notion of attention-aware visualizations (AAVs) that track the user's perception of a visual representation over time and feed this information back to the visualization. Such context awareness is particularly useful for ubiquitous and immersive analytics where knowing which embedded visualizations the user is looking at can be used to make visualizations react appropriately to the user's attention: for example, by highlighting data the user has not yet seen. We can separate the approach into three components: (1) measuring the user's gaze on a visualization and its parts; (2) tracking the user's attention over time; and (3) reactively modifying the visual representation based on the current attention metric. In this paper, we present two separate implementations of AAV: a 2D data-agnostic method for web-based visualizations that can use an embodied eyetracker to capture the user's gaze, and a 3D data-aware one that uses the stencil buffer to track the visibility of each individual mark in a visualization. Both methods provide similar mechanisms for accumulating attention over time and changing the appearance of marks in response. We also present results from a qualitative evaluation studying visual feedback and triggering mechanisms for capturing and revisualizing attention.",
        "language": "English",
        "keywords": [
            "attention tracking",
            "eyetracking",
            "immersive analytics",
            "post-WIMP interaction",
            "ubiquitous analytics"
        ],
        "_type_counter": 103
    },
    {
        "type": "article",
        "id": "kim2025DGComics",
        "title": "DG Comics: Semi-Automatically Authoring Graph Comics for Dynamic Graphs",
        "author": [
            "Joohee Kim",
            "Hyunwook Lee",
            "Nguyen, Duc M.",
            "Minjeong Shin",
            "Kwon, Bum Chul",
            "Sungahn Ko",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1109/TVCG.2024.3456340",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 31,
        "number": 1,
        "pages": "973--983",
        "doi": "10.1109/TVCG.2024.3456340",
        "issn": "1077-2626",
        "abstract": "Comics are an effective method for sequential data-driven storytelling, especially for dynamic graphs - graphs whose vertices and edges change over time. However, manually creating such comics is currently time-consuming, complex, and error-prone. In this paper, we propose DG COMICS, a novel comic authoring tool for dynamic graphs that allows users to semi-automatically build and annotate comics. The tool uses a newly developed hierarchical clustering algorithm to segment consecutive snapshots of dynamic graphs while preserving their chronological order. It also presents rich information on both individuals and communities extracted from dynamic graphs in multiple views, where users can explore dynamic graphs and choose what to tell in comics. For evaluation, we provide an example and report the results of a user study and an expert review.",
        "language": "English",
        "keywords": [
            "data-driven storytelling",
            "dynamic graphs",
            "graph comics",
            "narrative visualization"
        ],
        "_type_counter": 104
    },
    {
        "type": "article",
        "id": "dhanoa2025DTour",
        "title": "D-Tour: Semi-Automatic Generation of Interactive Guided Tours for Visualization Dashboard Onboarding",
        "author": [
            "Vaishali Dhanoa",
            "Andreas Hinterreiter",
            "Vanessa Fediuk",
            "Niklas Elmqvist",
            "Eduard Gröller",
            "Marc Streit"
        ],
        "url": "https://doi.org/10.1109/TVCG.2024.3456347",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "publisher": "IEEE Computer Society",
        "address": "Los Alamitos, CA, USA",
        "volume": 31,
        "number": 1,
        "pages": "721--731",
        "doi": "10.1109/TVCG.2024.3456347",
        "issn": "1077-2626",
        "abstract": "Onboarding a user to a visualization dashboard entails explaining its various components, including the chart types used, the data loaded, and the interactions available. Authoring such an onboarding experience is time-consuming and requires significant knowledge and little guidance on how best to complete this task. Depending on their levels of expertise, end users being onboarded to a new dashboard can be either confused and overwhelmed or disinterested and disengaged. We propose interactive dashboard tours (D-Tours) as semi-automated onboarding experiences that preserve the agency of users with various levels of expertise to keep them interested and engaged. Our interactive tours concept draws from open-world game design to give the user freedom in choosing their path through onboarding. We have implemented the concept in a tool called D-TOUR PROTOTYPE, which allows authors to craft custom interactive dashboard tours from scratch or using automatic templates. Automatically generated tours can still be customized to use different media (e.g., video, audio, and highlighting) or new narratives to produce an onboarding experience tailored to an individual user. We demonstrate the usefulness of interactive dashboard tours through use cases and expert interviews. Our evaluation shows that authors found the automation in the D-Tour Prototype helpful and time-saving, and users found the created tours engaging and intuitive.",
        "note": "Supplemental materials: https://osf.io/6fbjp/",
        "language": "English",
        "keywords": [
            "dashboards",
            "interactive tours",
            "onboarding",
            "open-world games",
            "storytelling",
            "tutorial"
        ],
        "_type_counter": 105
    },
    {
        "type": "inproceedings",
        "id": "patnaik2025Datamancer",
        "title": "Datamancer: Bimanual Gesture Interaction in Multi-Display Ubiquitous Analytics Environments",
        "author": [
            "Biswaksen Patnaik",
            "Marcel Borowski",
            "Huaishu Peng",
            "Clemens Nylandsted Klokmose",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1145/3706598.3713123",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-26T00:00:00.000Z",
        "booktitle": "Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "pages": "1--19",
        "doi": "10.1145/3706598.3713123",
        "abstract": "We introduce Datamancer, a wearable device enabling bimanual gesture interaction across multi-display ubiquitous analytics environments. Datamancer addresses the gap in gesture-based interaction within data visualization settings, where current methods are often constrained by limited interaction spaces or the need for installing bulky tracking setups. Datamancer integrates a finger-mounted pinhole camera and a chest-mounted gesture sensor, allowing seamless selection and manipulation of visualizations on distributed displays. By pointing to a display, users can acquire the display and engage in various interactions, such as panning, zooming, and selection, using both hands. Our contributions include (1) an investigation of the design space of gestural interaction for physical ubiquitous analytics environments; (2) a prototype implementation of the Datamancer system that realizes this model; and (3) an evaluation of the prototype through demonstration of application scenarios, an expert review, and a user study.",
        "series": "CHI '25",
        "articleno": "626",
        "numpages": 19,
        "location": "Yokohama, Japan",
        "language": "English",
        "keywords": [
            "augmented reality",
            "gestural interaction",
            "immersive analytics",
            "situated analytics",
            "ubiquitous analytics",
            "visualizations"
        ],
        "_type_counter": 69
    },
    {
        "type": "misc",
        "id": "elmqvist2025ParticipatoryAI",
        "title": "Participatory AI: A Scandinavian Approach to Human-Centered AI",
        "author": [
            "Niklas Elmqvist",
            "Eve Hoggan",
            "Hans-Jörg Schulz",
            "Marianne Graves Petersen",
            "Peter Dalsgaard",
            "Ira Assent",
            "Olav W. Bertelsen",
            "Akhil Arora",
            "Kaj Gronbaek",
            "Susanne Bodker",
            "Clemens Nylandsted Klokmose",
            "Rachel Charlotte Smith",
            "Sebastian Hubenschmid",
            "Christoph A. Johns",
            "Gabriela Molina León",
            "Anton Wolter",
            "Johannes Ellemose",
            "Vaishali Dhanoa",
            "Simon Aagaard Enni",  
            "Mille Skovhus Lunding",
            "Karl-Emil Kjaer Bilstrup",
            "Juan Sánchez Esquivel",
            "Luke Connelly",
            "Rafael Pablos Sarabia",
            "Morten Birk",
            "Joachim Nyborg",
            "Stefanie Zollmann",
            "Tobias Langlotz",
            "Meredith Chou",
            "Jens Emil Gronbaek",
            "Michael Wessely",
            "Yijing Jiang",
            "Caroline Berger",
            "Duosi Dai",
            "Michael Mose Biskjaer",
            "Germán Leiva",
            "Jonas Frich",
            "Eva Eriksson",
            "Kim Halskov",
            "Thorbjorn Mikkelsen",
            "Nearchos Potamitis",
            "Michel Yildirim",
            "Arvind Srinivasan",
            "Jeanette Falk",
            "Nanna Inie",
            "Ole Sejer Iversen",
            "Hugo Andersson"
        ],
        "url": "https://arxiv.org/abs/2505.19101",
        "_url_type": "arxiv",
        "year": 2025,
        "date": "2025-01-01T00:00:00.000Z",
        "abstract": "AI's transformative impact on work, education, and everyday life makes it as much a political artifact as a technological one. Current AI models are opaque, centralized, and overly generic. The algorithmic automation they provide threatens human agency and democratic values in both workplaces and daily life. To confront such challenges, we turn to Scandinavian Participatory Design (PD), which was devised in the 1970s to face a similar threat from mechanical automation. In the PD tradition, technology is seen not just as an artifact, but as a locus of democracy. Drawing from this tradition, we propose Participatory AI as a PD approach to human-centered AI that applies five PD principles to four design challenges for algorithmic automation. We use concrete case studies to illustrate how to treat AI models less as proprietary products and more as shared socio-technical systems that enhance rather than diminish human agency, human dignity, and human values.",
        "howpublished": "arXiv preprint arXiv:2505.19101",
        "eprint": "2505.19101",
        "archiveprefix": "arXiv",
        "primaryclass": "cs.HC",
        "language": "English",
        "keywords": [
            "participatory design",
            "human-centered AI",
            "democratic values",
            "algorithmic automation"
        ],
        "_type_counter": 1
    },
    {
        "type": "inproceedings",
        "id": "borowski2025Spatialstrates",
        "title": "Spatialstrates: Cross-Reality Collaboration through Spatial Hypermedia",
        "author": [
            "Marcel Borowski",
            "Jens Emil Gronbaek",
            "Peter W. S. Butcher",
            "Panagiotis D. Ritsos",
            "Clemens Nylandsted Klokmose",
            "Niklas Elmqvist"
        ],
        "url": "https://doi.org/10.1145/3746059.3747708",
        "_url_type": "doi",
        "year": 2025,
        "date": "2025-01-28T00:00:00.000Z",
        "booktitle": "Proceedings of the 38th Annual ACM Symposium on User Interface Software and Technology",
        "publisher": "Association for Computing Machinery",
        "address": "New York, NY, USA",
        "pages": "1--17",
        "doi": "10.1145/3746059.3747708",
        "abstract": "Consumer-level XR hardware now enables immersive spatial computing, yet most knowledge work remains confined to traditional 2D desktop environments. These worlds exist in isolation: writing emails or editing presentations favors desktop interfaces, while viewing 3D simulations or architectural models benefits from immersive environments. We address this fragmentation by combining spatial hypermedia, shareable dynamic media, and cross-reality computing to provide (1) composability of heterogeneous content and of nested information spaces through spatial transclusion, (2) pervasive cooperation across heterogeneous devices and platforms, and (3) congruent spatial representations despite underlying environmental differences. Our implementation, the Spatialstrates platform, embodies these principles using standard web technologies to bridge 2D desktop and 3D immersive environments. Through four scenarios---collaborative brainstorming, architectural design, molecular science visualization, and immersive analytics---we demonstrate how Spatialstrates enables collaboration between desktop 2D and immersive 3D contexts, allowing users to select the most appropriate interface for each task while maintaining collaborative capabilities.",
        "series": "UIST '25",
        "articleno": "129",
        "numpages": 17,
        "location": "Busan, Republic of Korea",
        "language": "English",
        "keywords": [
            "cross-reality",
            "hypermedia",
            "augmented reality",
            "extended reality",
            "collaboration",
            "2D/3D integration"
        ],
        "_type_counter": 70
    },
    {
        "type": "inproceedings",
        "id": "Raghunandan2023",
        "title": "Code Code Evolution: Understanding How People Change Data Science Notebooks Over Time",
        "author": [
            "Deepthi Raghunandan",
            "Aayushi Roy",
            "Shenzhi Shi",
            "Niklas Elmqvist",
            "Leilani Battle"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/cce/cce.pdf",
        "_url_type": "pdf",
        "year": 2023,
        "date": "2023-04-24T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "publisher": "ACM",
        "address": "New York, NY, USA ",
        "abstract": "Sensemaking is the iterative process of identifying, extracting, and explaining insights from data, where each iteration is referred to as the \u201csensemaking loop.\u201d However, little is known about how sensemaking behavior evolves from exploration and explanation during this process. This gap limits our ability to understand the full scope of sensemaking, which in turn inhibits the design of tools that support the process. We contribute the first mixed-method to characterize how sensemaking evolves within computational notebooks. We study 2,574 Jupyter notebooks mined from GitHub by identifying data science notebooks that have undergone significant iterations, presenting a regression model that automatically characterizes sensemaking activity, and using this regression model to calculate and analyze shifts in activity across GitHub versions. Our results show that notebook authors participate in various sensemaking tasks over time, such as annotation, branching analysis, and documentation. We use our insights to recommend extensions to current notebook environments.",
        "keywords": [],
        "_type_counter": 64
    },
    {
        "type": "inproceedings",
        "id": "Hoque2023",
        "title": "Accessible Data Representation with Natural Sound",
        "author": [
            "Md Naimul Hoque",
            "Md Ehtesham-Ul-Haque",
            "Niklas Elmqvist",
            "Syed Masum Billah"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/susurrus/susurrus.pdf",
        "_url_type": "pdf",
        "year": 2023,
        "date": "2023-04-24T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Sonification translates data into non-speech audio. Such auditory representations can make data visualization accessible to people who are blind or have low vision (BLV). This paper presents a sonification method for translating common data visualization into a blend of natural sounds. We hypothesize that people's familiarity with sounds drawn from nature, such as birds singing in a forest, and their ability to listen to these sounds in parallel, will enable BLV users to perceive multiple data points being sonified at the same time. Informed by an extensive literature review and a preliminary study with 5 BLV participants, we designed an accessible data representation tool, Susurrus, that combines our sonification method with other accessibility features, such as keyboard interaction and text-to-speech feedback. Finally, we conducted a user study with 12 BLV participants and report the potential and application of natural sounds for sonification compared to existing sonification tools.",
        "keywords": [],
        "_type_counter": 63
    },
    {
        "type": "inproceedings",
        "id": "Shin2023",
        "title": "Perceptual Pat: A Virtual Human Visual System for Iterative Visualization Design",
        "author": [
            "Sungbok Shin",
            "Sanghyun Hong",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/perceptual-pat/perceptual-pat.pdf",
        "_url_type": "pdf",
        "year": 2023,
        "date": "2023-04-24T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Designing a visualization is often a process of iterative refinement where the designer improves a chart over time by adding features, improving encodings, and fixing mistakes. However, effective design requires external critique and evaluation. Unfortunately, such critique is not always available on short notice and evaluation can be costly. To address this need, we present Perceptual Pat, an extensible suite of AI and computer vision techniques that forms a virtual human visual system for supporting iterative visualization design. The system analyzes snapshots of a visualization using an extensible set of filters\u2014including gaze maps, text recognition, color analysis, etc\u2014and generates a report summarizing the findings. The web-based Pat Design Lab provides a version tracking system that enables the designer to track improvements over time. We validate Perceptual Pat using a longitudinal qualitative study involving 4 professional visualization designers that used the tool over a few days to design a new visualization",
        "keywords": [],
        "_type_counter": 62
    },
    {
        "type": "inproceedings",
        "id": "Saffo2023",
        "title": "Through Their Eyes and In Their Shoes: Providing Group Awareness During Collaboration Across Virtual Reality and Desktop Platforms",
        "author": [
            "David Saffo",
            "Andrea Batch",
            "Cody Dunne",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/vrxd/vrxd.pdf",
        "_url_type": "pdf",
        "year": 2023,
        "date": "2023-04-24T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Many collaborative data analysis situations benefit from collaborators utilizing different platforms. However, maintaining group awareness between team members using diverging devices is difficult, not least because common ground diminishes. A person using head-mounted VR cannot physically see a user on a desktop computer even while co-located, and the desktop user cannot easily relate to the VR user's 3D workspace. To address this, we propose the 'eyes-and-shoes' principles for group awareness and abstract them into four levels of techniques. Furthermore, we evaluate these principles with a qualitative user study of 6 participant pairs synchronously collaborating across distributed desktop and VR head-mounted devices. In this study, we vary the group awareness techniques between participants and explore two visualization contexts within participants. The results of this study indicate that the more visual metaphors and views of participants diverge, the greater the level of group awareness is needed. A \u271a Copy of this paper, the study preregistration, and all supplemental materials required to reproduce the study are available on https://osf.io/wgprb/. ",
        "keywords": [],
        "_type_counter": 61
    },
    {
        "type": "article",
        "id": "Batch2023",
        "title": "uxSense: Supporting User Experience Analysis with Visualization and Computer Vision",
        "author": [
            "Andrea Batch",
            "Yipeng Ji",
            "Mingming Fan",
            "Jian Zhao",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/uxsense/uxsense.pdf",
        "_url_type": "pdf",
        "year": 2023,
        "date": "2023-02-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Analyzing user behavior from usability evaluation can be a challenging and time-consuming task, especially as the number of participants and the scale and complexity of the evaluation grows. We propose UXSENSE, a visual analytics system using machine learning methods to extract user behavior from audio and video recordings as parallel time-stamped data streams. Our implementation draws on pattern recognition, computer vision, natural language processing, and machine learning to extract user sentiment, actions, posture, spoken words, and other features from such recordings. These streams are visualized as parallel timelines in a web-based front-end, enabling the researcher to search, filter, and annotate data across time and space. We present the results of a user study involving professional UX researchers evaluating user data using uxSense. In fact, we used uxSense itself to evaluate their sessions.",
        "keywords": [],
        "_type_counter": 85
    },
    {
        "type": "article",
        "id": "Datta2023",
        "title": "TimberSleuth: Visual Anomaly Detection with Human Feedback for Mitigating the Illegal Timber Trade",
        "author": [
            "Debanjan Datta",
            "Nathan Self",
            "John Simeone",
            "Amelia Meadows",
            "Willow Outhwaite",
            "Linda Walker",
            "Niklas Elmqvist",
            "Naren Ramkrishnan"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/timbersleuth/timbersleuth.pdf",
        "_url_type": "pdf",
        "year": 2023,
        "date": "2023-02-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "abstract": "Detecting illegal shipments in the global timber trade poses a massive challenge to enforcement agencies. The massive volume and complexity of timber shipments and obfuscations within international trade data, intentional or not, necessitates an automated system to aid in detecting specific shipments that potentially contain illegally harvested wood. To address these requirements we build a novel human-in-the-loop visual analytics system called TIMBERSLEUTH. TimberSleuth uses a novel scoring model reinforced through human feedback to improve upon the relevance of the results of the system while using an off-the-shelf anomaly detection model. Detailed evaluation is performed using real data with synthetic anomalies to test the machine intelligence that drives the system. We design interactive visualizations to enable analysis of pertinent details of anomalous trade records so that analysts can determine if a record is relevant and provide iterative feedback. This feedback is utilized by the machine learning model to improve the precision of the output.",
        "keywords": [],
        "_type_counter": 84
    },
    {
        "type": "article",
        "id": "Clegg2022",
        "title": "Data Everyday as Community Driven Science: Athletes\u2019 Critical Data Literacy Practices in Collegiate Sports Contexts",
        "author": [
            "Tamara L. Clegg",
            "Keaunna Cleveland",
            "Erianne Weight",
            "Daniel Greene",
            "Niklas Elmqvist"
        ],
        "url": "https://onlinelibrary.wiley.com/doi/epdf/10.1002/tea.21842",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-12-01T00:00:00.000Z",
        "journal": "Journal of Research in Science Teaching",
        "abstract": "In this article, we investigate the community-driven science happening organically in elite athletics as a means of engaging a community of learners\u2014collegiate athletes, many of whom come from underrepresented groups\u2014in STEM. We aim to recognize the data literacy practices inherent in sports play and to explore the potential of critical data literacy practices for enabling athletes to leverage data science as a means of addressing systemic racial, equity, and justice issues inherent in sports institutions. We leverage research on critical data literacies as a lens to present case studies of three athletes at an NCAA Division 1 university spanning three different sports. We focus on athletes' experiences as they engage in critical data literacy practices and the ways they welcome, adapt, resist, and critique such engagements. Our findings indicate ways in which athletes (1) readily accept data practices espoused by their coaches and sport, (2) critique and intentionally disengage from such practices, and (3) develop their own new data productions. In order to support community-driven science, our findings point to the critical role of athletics' organizations in promoting athletes' access to, as well as engagement and agency with data practices on their teams.",
        "keywords": [],
        "_type_counter": 83
    },
    {
        "type": "article",
        "id": "Shin2022b",
        "title": "A Scanner Deeply: Predicting Gaze Heatmaps on Visualizations Using Crowdsourced Eye Movement Data",
        "author": [
            "Sungbok Shin",
            "Sunghyo Chung",
            "Sanghyun Hong",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/scanner-deeply/scanner-deeply.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-10-20T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Visual perception is a key component of data visualization. Much prior empirical work uses eye movement as a proxy to understand human visual perception. Diverse apparatus and techniques have been proposed to collect eye movements, but there is still no optimal approach. In this paper, we review 30 prior works for collecting eye movements based on three axes: (1) the tracker technology used to measure eye movements; (2) the image stimulus shown to participants; and (3) the collection methodology used to gather the data. Based on this taxonomy, we employ a webcam-based eyetracking approach using task-specific visualizations as the stimulus. The low technology requirement means that virtually anyone can participate, thus enabling us to collect data at large scale using crowdsourcing: approximately 12,000 samples in total. Choosing visualization images as stimulus means that the eye movements will be specific to perceptual tasks associated with visualization. We use these data to propose a Scanner Deeply, a virtual eyetracker model that, given an image of a visualization, generates a gaze heatmap for that image. We employ a computationally efficient, yet powerful convolutional neural network for our model. We compare the results of our work with results from the DVS model and a neural network trained on the Salicon dataset. The analysis of our gaze patterns enables us to understand how users grasp the structure of visualized data. We also make our stimulus dataset of visualization images available as part of this paper\u2019s contribution.",
        "keywords": [],
        "_type_counter": 82
    },
    {
        "type": "article",
        "id": "Newburger2022",
        "title": "Fitting Bell Curves to Data Distributions using Visualization",
        "author": [
            "Eric Newburger",
            "Michael Correll",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/fitting-bells/fitting-bells.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-10-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "idealized probability distributions, such as normal or other curves, lie at the root of confirmatory statistical tests. But how well do people understand these idealized curves? In practical terms, does the human visual system allow us to match sample data distributions with hypothesized population distributions from which those samples might have been drawn? And how do different visualization techniques impact this capability? This paper shares the results of a crowdsourced experiment that tested the ability of respondents to fit normal curves to four different data distribution visualizations: bar histograms, dotplot histograms, strip plots, and boxplots. We find that the crowd can estimate the center (mean) of a distribution with some success and little bias. We also find that people generally overestimate the standard deviation\u2014which we dub the \u201cumbrella effect\u201d because people tend to want to cover the whole distribution using the curve, as if sheltering it from the heavens above\u2014and that strip plots yield the best accuracy.",
        "keywords": [],
        "_type_counter": 81
    },
    {
        "type": "article",
        "id": "Chundury2022",
        "title": "Contextual In-Situ Help for Visual Data Interfaces",
        "author": [
            "Pramod Chundury",
            "Mehmet Adil Yalcin",
            "Jonathan Crabtree",
            "Anup Mahurkar",
            "Lisa M. Shulman",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/contextual-help/contextual-help.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-09-09T00:00:00.000Z",
        "journal": "Information Visualization",
        "abstract": "As the complexity of data analysis increases, even well-designed data interfaces must guide experts in transforming their theoretical knowledge into actual features supported by the tool. This challenge is even greater for casual users who are increasingly turning to data analysis to solve everyday problems. To address this challenge, we propose data-driven, contextual, in-situ help features that can be implemented in visual data interfaces. We introduce five modes of help-seeking: (1) contextual help on selected interface elements, (2) topic listing, (3) overview, (4) guided tour, and (5) notifications. The difference between our work and general user interface help systems is that data visualization provide a unique environment for embedding context-dependent data inside on-screen messaging. We demonstrate the usefulness of such contextual help through two case studies of two visual data interfaces: Keshif and POD-Vis. We implemented and evaluated the help modes with two sets of participants, and found that directly selecting user interface elements was the most useful.",
        "keywords": [],
        "_type_counter": 80
    },
    {
        "type": "article",
        "id": "Patnaik2022",
        "title": "Sensemaking Sans Power: Interactive Data Visualization Using Color-Changing Ink",
        "author": [
            "Biswaksen Patnaik",
            "Huaishu Peng",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/sense-sans-power/sense-sans-power.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-09-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "We present an approach for interactively visualizing data using color-changing inks without the need for electronic displays or computers. Color-changing inks are a family of physical inks that change their color characteristics in response to an external stimulus such as heat, UV light, water, and pressure. Visualizations created using color-changing inks can embed interactivity in printed material without external computational media. In this paper, we survey current color-changing ink technology and then use these findings to derive a framework for how it can be used to construct interactive data representations. We also enumerate the interaction techniques possible using this technology. We then show some examples of how to use color-changing ink to create interactive visualizations on paper. While obviously limited in scope to situations where no power or computing is present, or as a complement to digital displays, our findings can be employed for paper, data physicalization, and embedded visualizations.",
        "keywords": [],
        "_type_counter": 79
    },
    {
        "type": "article",
        "id": "Badam2022",
        "title": "Integrating Annotations into Multidimensional Visual Dashboards",
        "author": [
            "Sriram Karthik Badam",
            "Senthil Chandrasegaran",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/facetnotes/facetnotes.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-05-10T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 21,
        "number": 3,
        "pages": "270--284",
        "abstract": "Multidimensional data is often visualized using coordinated multiple views in an interactive dashboard. However, unlike in infographics where text is often a central part of the presentation, there is currently little knowledge of how to best integrate text and annotations in a visualization dashboard. In this paper, we explore a technique called FacetNotes for presenting these textual annotations on top of any visualization within a dashboard irrespective of the scale of data shown or the design of visual representation itself. FacetNotes does so by grouping and ordering the textual annotations based on properties of (1) the individual data points associated with the annotations, and (2) the target visual representation on which they should be shown. We present this technique along with a set of user interface features and guidelines to apply it to visualization interfaces. We also demonstrate FacetNotes in a custom visual dashboard interface. Finally, results from a user study of FacetNotes show that the technique improves the scope and complexity of insights developed during visual exploration.",
        "keywords": [],
        "_type_counter": 78
    },
    {
        "type": "article",
        "id": "Shin2022",
        "title": "Roslingifier: Semi-Automated Storytelling for Animated Scatterplots",
        "author": [
            "Minjeong Shin",
            "Joohee Kim",
            "Yunha Han",
            "Lexing Xie",
            "Mitchell Whitelaw",
            "Bum Chul Kwon",
            "Sungahn Ko",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/roslingifier/roslingifier.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-05-10T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "abstract": "We present Roslingifier, a data-driven storytelling method for animated scatterplots. Like its namesake, Hans Rosling (1948--2017), a professor of public health and a spellbinding public speaker, Roslingifier turns a sequence of entities changing over time---such as countries and continents with their demographic data---into an engaging narrative telling the story of the data. This data-driven storytelling method with an in-person presenter is a new genre of storytelling technique and has never been studied before. In this paper, we aim to define a design space for this new genre---data presentation---and provide a semi-automated authoring tool for helping presenters create quality presentations. From an in-depth analysis of video clips of presentations using interactive visualizations, we derive three specific techniques to achieve this: natural language narratives, visual effects that highlight events, and temporal branching that changes playback time of the animation. Our implementation of the Roslingifier method is capable of identifying and clustering significant movements, automatically generating visual highlighting and a narrative for playback, and enabling the user to customize. From two user studies, we show that Roslingifier allows users to effectively create engaging data stories and the system features help both presenters and viewers find diverse insights.",
        "keywords": [],
        "_type_counter": 77
    },
    {
        "type": "inproceedings",
        "id": "Hubenschmid2022",
        "title": "ReLive: Bridging In-Situ and Ex-Situ Visual Analytics for Analyzing Mixed Reality User Studies",
        "author": [
            "Sebastian Hubenschmid",
            "Jonathan Wieland",
            "Daniel Immanuel Fink",
            "Andrea Batch",
            "Johannes Zagermann",
            "Niklas Elmqvist",
            "Harald Reiterer"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/relive/relive.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-05-10T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "24:1--24:20",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "The nascent field of mixed reality is seeing an ever-increasing need for user studies and field evaluation, which are particularly challenging given device heterogeneity, diversity of use, and mobile deployment. Immersive analytics tools have recently emerged to support such analysis in situ, yet the complexity of the data also warrants an ex-situ analysis using more traditional non-immersive visual analytics setups. To bridge the gap between both approaches, we introduce ReLive: a mixed-immersion visual analytics framework for exploring and analyzing mixed reality user studies. ReLive combines an in-situ virtual reality view with a complementary ex-situ desktop view. While the virtual reality view allows users to relive interactive spatial recordings replicating the original study, the synchronized desktop view provides a familiar interface for analyzing aggregated data. We validated our concepts in a two-step evaluation consisting of a design walkthrough and an empirical expert user study.",
        "keywords": [],
        "_type_counter": 60
    },
    {
        "type": "inproceedings",
        "id": "Hoque2022",
        "title": "DramatVis Personae: Visual Text Analytics for identifying Social Biases in Creative Writing",
        "author": [
            "Md Naimul Hoque",
            "Bhavya Ghai",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/dvp/dvp.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-05-10T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Designing Interactive Systems",
        "booktitle": "Proceedings of the ACM Conference on Designing Interactive Systems",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Implicit biases and stereotypes are often pervasive in different forms of creative writing such as novels, screenplays, and children's books. To understand the kind of biases writers are concerned about and how they mitigate those in their writing, we conducted formative interviews with nine writers. The interviews suggested that despite a writer's best interest, tracking and managing implicit biases such as a lack of agency, supporting or submissive roles, or harmful language for characters representing marginalized groups is challenging as the story becomes longer and complicated. Based on the interviews, we developed DramatVis Personae (DVP), a visual analytics tool that allows writers to assign social identities to characters, and evaluate how characters and different intersectional social identities are represented in the story. To evaluate DVP, we first conducted think-aloud sessions with three writers and found that DVP is easy-to-use, naturally integrates into the writing process, and could potentially help writers in several critical bias identification tasks. We then conducted a follow-up user study with 11 writers and found that participants could answer questions related to bias detection more efficiently using DVP in comparison to a simple text editor. ",
        "keywords": [],
        "_type_counter": 59
    },
    {
        "type": "article",
        "id": "Chundury2021",
        "title": "Towards Understanding Sensory Substitution for Accessible Visualization: An Interview Study",
        "author": [
            "Pramod Chundury",
            "Biswaksen Patnaik",
            "Yasmin Reyazuddin",
            "Christine W. Tang",
            "Jonathan Lazar",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/access-vis/access-vis.pdf",
        "_url_type": "pdf",
        "year": 2022,
        "date": "2022-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 28,
        "number": 1,
        "pages": "1084--1094",
        "abstract": "For all its potential in supporting data analysis, particularly in exploratory situations, visualization also creates barriers: accessibility for blind and visually impaired individuals. Regardless of how effective a visualization is, providing equal access for blind users requires a paradigm shift for the visualization research community. To enact such a shift, it is not sufficient to treat visualization accessibility as merely another technical problem to overcome. Instead, supporting the millions of blind and visually impaired users around the world who have equally valid needs for data analysis as sighted individuals requires a respectful, equitable, and holistic approach that includes all users from the onset. In this paper, we draw on accessibility research methodologies to make inroads towards such an approach. We first identify the people who have specific insight into how blind people perceive the world: orientation and mobility (O&M) experts, who are instructors that teach blind individuals how to navigate the physical world using non-visual senses. We interview 10 O&M experts---all of them blind---to understand how best to use sensory substitution other than the visual sense for conveying spatial layouts. Finally, we investigate our qualitative findings using thematic analysis. While blind people in general tend to use both sound and touch to understand their surroundings, we focused on auditory affordances and how they can be used to make data visualizations accessible---using sonification and auralization. However, our experts recommended supporting a combination of senses---sound and touch---to make charts accessible as blind individuals may be more familiar with exploring tactile charts. We report results on both sound and touch affordances, and conclude by discussing implications for accessible visualization for blind individuals.",
        "keywords": [],
        "_type_counter": 76
    },
    {
        "type": "inproceedings",
        "id": "Raghunandan2021",
        "title": "Lodestar: Supporting Independent Learning and Rapid Experimentation Through Data-Driven Analysis Recommendations",
        "author": [
            "Deepthi Raghunandan",
            "Zhe Cui",
            "Kartik Krishnan",
            "Segen Tirfe",
            "Shenzhi Shi",
            "Tejaswi Darshan Shrestha",
            "Leilani Battle",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/lodestar/lodestar.pdf",
        "_url_type": "pdf",
        "year": 2021,
        "date": "2021-10-01T00:00:00.000Z",
        "journal": "Proceedings of the Symposium on Visualization in Data Science",
        "booktitle": "Proceedings of the Symposium on Visualization in Data Science",
        "abstract": "Keeping abreast of current trends, technologies, and best practices in visualization and data analysis is becoming increasingly difficult, especially for fledgling data scientists. In this paper, we propose Lodestar, an interactive computational notebook that allows users to quickly explore and construct new data science workflows by selecting from a list of automated analysis recommendations. We derive our recommendations from directed graphs of known analysis states, with two input sources: one manually curated from online data science tutorials, and another extracted through semi-automatic analysis of a corpus of over 6,000 Jupyter notebooks. We evaluate Lodestar in a formative study guiding our next set of improvements to the tool. Our results suggest that users find Lodestar useful for rapidly creating data science workflows.",
        "keywords": [],
        "_type_counter": 58
    },
    {
        "type": "article",
        "id": "Wang2021",
        "title": "Topology-Aware Space Distortion for Structured Visualization Spaces",
        "author": [
            "Weihang Wang",
            "Sriram Karthik Badam",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/zoomhalo/zoomhalo.pdf",
        "_url_type": "pdf",
        "year": 2021,
        "date": "2021-09-29T00:00:00.000Z",
        "journal": "Information Visualization",
        "abstract": "We propose topology-aware space distortion (TASD), a family of interactive layout techniques for non-linearly distorting geometric space based on user attention and on the structure of the visual representation. TASD seamlessly adapts the visual substrate of any visualization to give more screen real estate to important regions of the representation at the expense of less important regions. In this paper, we present a concrete TASD technique that we call ZoomHalo for interactively distorting a two-dimensional space based on a degree-of-interest (DOI) function defined for the space. Using this DOI function, ZoomHalo derives several areas of interest, computes the available space around each area in relation to other areas and the current viewport extents, and then dynamically expands (or shrinks) each area given user input. We use our prototype to evaluate the technique in two user studies, as well as showcase examples of TASD for node-link diagrams, word clouds, and geographical maps.",
        "keywords": [],
        "_type_counter": 75
    },
    {
        "type": "article",
        "id": "Badam2021",
        "title": "Effects of Screen-Responsive Visualization on Data Comprehension",
        "author": [
            "Sriram Karthik Badam",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/touchinsight/touchinsight.pdf",
        "_url_type": "pdf",
        "year": 2021,
        "date": "2021-09-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 20,
        "number": 4,
        "pages": "229--244",
        "abstract": "Visualization interfaces designed for heterogeneous devices such as wall displays and mobile screens must be responsive to varying display dimensions, resolution, and interaction capabilities. In this paper, we report on two user studies of visual representations for large versus small displays. The goal of our experiments was to investigate differences between a large vertical display and a mobile hand-held display in terms of the data comprehension and the quality of resulting insights. To this end, we developed a visual interface with a coordinated multiple view layout for the large display and two alternative designs of the same interface---a space-saving boundary visualization layout and an overview layout---for the mobile condition. The first experiment was a controlled laboratory study designed to evaluate the effect of display size on the perception of changes in a visual representation, and yielded significant correctness differences even while completion time remained similar. The second evaluation was a qualitative study in a practical setting and showed that participants were able to easily associate and work with the responsive visualizations. Based on the results, we conclude the paper by providing new guidelines for screen-responsive visualization interfaces.",
        "keywords": [],
        "_type_counter": 74
    },
    {
        "type": "article",
        "id": "Park2021",
        "title": "StoryFacets: A Design Study on Storytelling with Visualizations for Collaborative Data Analysis",
        "author": [
            "Deokgun Park",
            "Mohamed Suhail",
            "Minsheng Zheng",
            "Cody Dunn",
            "Eric Ragan",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/storyfacets/storyfacets.pdf",
        "_url_type": "pdf",
        "year": 2021,
        "date": "2021-08-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "abstract": "Tracking the sensemaking process is a well-established practice in many data analysis tools, and many visualization tools facilitate overview and recall during and after exploration. However, the resulting communication materials such as presentations or infographics often omit provenance information for the sake of simplicity. This unfortunately limits later viewers from engaging in further collaborative sensemaking or discussion about the analysis. We present a design study where we introduced visual provenance and analytics to urban transportation planning. Maintaining the provenance of all analyses was critical to support collaborative sensemaking among the many and diverse stakeholders. Our system, StoryFacets, exposes several different views of the same analysis session, each view designed for a specific audience: (1) the trail view provides a data flow canvas that supports in-depth exploration+provenance (expert analysts); (2) the dashboard view organizes visualizations and other content into a space-filling layout to support high-level analysis (managers); and (3) the slideshow view supports linear storytelling via interactive step-by-step presentations (laypersons). Views are linked so that when one is changed, provenance is maintained. Visual provenance is available on demand to support iterative sensemaking for any team member.",
        "keywords": [],
        "_type_counter": 73
    },
    {
        "type": "article",
        "id": "Choudhry2021",
        "title": "Once Upon A Time In Visualization: Understanding the Use of Textual Narratives for Causality",
        "author": [
            "Arjun Choudhry",
            "Mandar Sharma",
            "Pramod Chundury",
            "Thomas Kapler",
            "Derek Gray",
            "Naren Ramakrishnan",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/causality/onceuponatime.pdf",
        "_url_type": "pdf",
        "year": 2021,
        "date": "2021-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 28,
        "number": 1,
        "abstract": "Causality visualization can help people understand temporal chains of events, such as messages sent in a distributed system, cause and effect in a historical conflict, or the interplay between political actors over time. However, as the scale and complexity of these event sequences grows, even these visualizations can become overwhelming to use. In this paper, we propose the use of textual narratives as a data-driven storytelling method to augment causality visualization. We first propose a design space for how textual narratives can be used to describe causal data. We then present results from a crowdsourced user study where participants were asked to recover causality information from two causality visualizations--causal graphs and Hasse diagrams--with and without an associated textual narrative. Finally, we describe CAUSEWORKS, a causality visualization system for understanding how specific interventions influence a causal model. The system incorporates an automatic textual narrative mechanism based on our design space. We validate CAUSEWORKS through interviews with experts who used the system for understanding complex events.",
        "keywords": [],
        "_type_counter": 72
    },
    {
        "type": "article",
        "id": "Ondov2021",
        "title": "Revealing Perceptual Proxies with Adversarial Examples",
        "author": [
            "Brian Ondov",
            "Fumeng Yang",
            "Matthew Kay",
            "Niklas Elmqvist",
            "Steven Franconeri"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/perceptual-proxies/revealing-proxies.pdf",
        "_url_type": "pdf",
        "year": 2021,
        "date": "2021-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 28,
        "number": 1,
        "abstract": "Data visualizations convert numbers into visual marks so that our visual system can extract data from an image instead of raw numbers. Clearly, the visual system does not compute these values as a computer would, as an arithmetic mean or a correlation.Instead, it extracts these patterns using perceptual proxies; heuristic shortcuts of the visual marks, such as a center of mass or a shape envelope. Understanding which proxies people use would lead to more effective visualizations. We present the results of a series of crowdsourced experiments that measure how powerfully a set of candidate proxies can explain human performance when comparing the mean and range of pairs of data series presented as bar charts. We generated datasets where the correct answer---the series with the larger arithmetic mean or range---was pitted against an \"adversarial\" series that should be seen as larger if the viewer uses a particular candidate proxy. We used both Bayesian logistic regression models and a robust Bayesian mixed-effects linear model to measure how strongly each adversarial proxy could drive viewers to answer incorrectly and whether different individuals may use different proxies. Finally, we attempt to construct adversarial datasets from scratch, using an iterative crowdsourcing procedure to perform black-box optimization.",
        "keywords": [],
        "_type_counter": 71
    },
    {
        "type": "inproceedings",
        "id": "Batch2019",
        "title": "Scents and Sensibility: Evaluating Information Olfactation",
        "author": [
            "Andrea Batch",
            "Biswaksen Patnaik",
            "Moses Akazue",
            "Niklas Elmqvist"
        ],
        "url": "https://users.umiacs.umd.edu/~elm/projects/info-olfac/scents-sense.pdf",
        "_url_type": "pdf",
        "year": 2020,
        "date": "2020-10-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Olfaction---the sense of smell---is one of the least explored of the human senses for conveying abstract information. In this paper, we conduct a comprehensive perceptual experiment on information olfactation: the use of olfactory and crossmodal sensory marks and channels to convey data. More specifically, following the example from graphical perception studies, we design an experiment that studies the perceptual accuracy of four cross-modal sensory channels---scent type, scent intensity, airflow, and temperature---for conveying three different types of data---nominal, ordinal, and quantitative. We also present details of a 24-scent multi-sensory display",
        "keywords": [],
        "_type_counter": 57
    },
    {
        "type": "article",
        "id": "Zhou2020",
        "title": "Using Social Interaction Trace Data and Context to Predict Collaboration Quality and Creative Fluency in Collaborative Design Learning Environments",
        "author": [
            "Ninger Zhou",
            "Lorraine Kisselburgh",
            "Senthil Chandrasegaran",
            "Karthik Badam",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "https://doi.org/10.1016/j.ijhcs.2019.102378",
        "_url_type": "doi",
        "year": 2020,
        "date": "2020-04-01T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Studies",
        "volume": 136,
        "number": 102378,
        "abstract": "Engineering design typically occurs as a collaborative process situated in specific context such as computer-supported environments, however there is limited research examining the dynamics of design collaboration in specific contexts. In this study, drawing from situative learning theory, we developed two analytic lenses to broaden theoretical insights into collaborative design practices in computer-supported environments: (a) the role of spatial and material context, and (b) the role of social interactions. We randomly assigned participants to four conditions varying the material context (paper vs. tablet sketching tools) and spatial environment (private room vs commons area) as they worked collaboratively to generate ideas for a toy design task. We used wearable sociometric badges to automatically and unobtrusively collect social interaction data. Using partial least squares regression, we generated two predictive models for collaboration quality and creative fluency. We found that context matters materially to perceptions of collaboration, where those using collaboration-support tools perceived higher quality collaboration. But context matters spatially to creativity, and those situated in private spaces are more fluent in generating ideas than those in commons areas. We also found that interaction dynamics differ: synchronous interaction is important to quality collaboration, but reciprocal interaction is important to creative fluency. These findings provide important insights into the processual factors in collaborative design in computer-supported environments, and the predictive role of context and conversation dynamics. We discuss the theoretical contributions to computer-supported collaborative design, the methodological contributions of wearable sensor tools, and the practical contributions to structuring computer-supported environments for engineering design practice.",
        "keywords": [],
        "_type_counter": 70
    },
    {
        "type": "article",
        "id": "Chalbi2020",
        "title": "Common Fate for Animated Transitions in Visualization",
        "author": [
            "Amira Chalbi",
            "Jacob Ritchie",
            "Deok Gun Park",
            "Jungu Choi",
            "Nicolas Roussel",
            "Niklas Elmqvist",
            "Fanny Chevalier"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/common-fate/common-fate.pdf",
        "_url_type": "pdf",
        "year": 2020,
        "date": "2020-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 26,
        "number": 1,
        "abstract": "The Law of Common Fate from Gestalt psychology states that visual objects moving with the same velocity along parallel trajectories will be perceived by a human observer as grouped. However, the concept of common fate is much broader than mere velocity; in this paper we explore how common fate results from coordinated changes in luminance and size. We present results from a crowdsourced graphical perception study where we asked workers to make perceptual judgments on a series of trials involving four graphical objects under the influence of conflicting static and dynamic visual factors (position, size and luminance) used in conjunction. Our results yield the following rankings for visual grouping: motion > (dynamic luminance, size, luminance); dynamic size > (dynamic luminance, position); and dynamic luminance > size. We also conducted a follow-up experiment to evaluate the three dynamic visual factors in a more ecologically valid setting, using both a Gapminder-like animated scatterplot and a thematic map of election data. The results indicate that in practice the relative grouping strengths of these factors may depend on various parameters including the visualization characteristics and the underlying data. We discuss design implications for animated transitions in data visualization.",
        "keywords": [],
        "_type_counter": 69
    },
    {
        "type": "article",
        "id": "Batch2020",
        "title": "There Is No Spoon: Evaluating Performance, Space Use, and Presence with Expert Domain Users in Immersive Analytics",
        "author": [
            "Andrea Batch",
            "Andrew Cunningham",
            "Maxime Cordeil",
            "Niklas Elmqvist",
            "Tim Dwyer",
            "Bruce H. Thomas",
            "Kim Marriott"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/nospoon/nospoon.pdf",
        "_url_type": "pdf",
        "year": 2020,
        "date": "2020-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 26,
        "number": 1,
        "abstract": "Immersive analytics turns the very space surrounding the user into a canvas for data analysis, supporting human cognitive abilities in myriad ways. We present the results of a design study, contextual inquiry, and longitudinal evaluation involving professional economists using a Virtual Reality (VR) system for multidimensional visualization to explore actual economic data. Results from our preregistered evaluation highlight the varied use of space depending on context (exploration vs. presentation), the organization of space to support work, and the impact of immersion on navigation and orientation in the 3D analysis space.",
        "keywords": [],
        "_type_counter": 68
    },
    {
        "type": "article",
        "id": "Jardine2020",
        "title": "The Perceptual Proxies of Visual Comparison",
        "author": [
            "Nicole Jardine",
            "Brian Ondov",
            "Niklas Elmqvist",
            "Steven Franconeri"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/perceptual-proxies/perceptual-proxies.pdf",
        "_url_type": "pdf",
        "year": 2020,
        "date": "2020-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 26,
        "number": 1,
        "abstract": "Perceptual tasks in visualizations often involve comparisons. Of two sets of values depicted in two charts, which set had values that were the highest overall? Which had the widest range? Prior empirical work found that the performance on different visual comparison tasks (e.g., \"biggest delta\", \"biggest correlation\") varied widely across different combinations of marks and spatial arrangements. In this paper, we expand upon these combinations in an empirical evaluation of two new comparison tasks: the \"biggest mean\" and \"biggest range\" between two sets of values. We used a staircase procedure to titrate the difficulty of the data comparison to assess which arrangements produced the most precise comparisons for each task. We find visual comparisons of biggest mean and biggest range are supported by some chart arrangements more than others, and that this pattern is substantially different from the pattern for other tasks. To synthesize these dissonant findings, we argue that we must understand which features of a visualization are actually used by the human visual system to solve a given task. We call these perceptual proxies. For example, when comparing the means of two bar charts, the visual system might use a \"Mean length\" proxy that isolates the actual lengths of the bars and then constructs a true average across these lengths. Alternatively, it might use a \"Hull Area\" proxy that perceives an implied hull bounded by the bars of each chart and then compares the areas of these hulls. We propose a series of potential proxies across different tasks, marks, and spatial arrangements. Simple models of these proxies can be empirically evaluated for their explanatory power by matching their performance to human performance across these marks, arrangements, and tasks. We use this process to highlight candidates for perceptual proxies that might scale more broadly to explain performance in visual comparison.",
        "keywords": [],
        "_type_counter": 67
    },
    {
        "type": "article",
        "id": "Cui2020",
        "title": "Proactive Visual and Statistical Analysis of Genomic Data in Epiviz",
        "author": [
            "Zhe Cui",
            "Jayaram Kancherla",
            "Kyle W. Chang",
            "Niklas Elmqvist",
            "Hector Corrada Bravo"
        ],
        "url": "https://academic.oup.com/bioinformatics/article-pdf/36/7/2195/50670470/btz883.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-11-29T00:00:00.000Z",
        "journal": "Bioinformatics",
        "volume": 36,
        "number": 7,
        "pages": "2195--2201",
        "abstract": "In this article, we present Epiviz Feed, a proactive and automatic visual analytics system integrated with Epiviz that alleviates the burden of manually executing data analysis required to test biologically meaningful hypotheses. Results of interest that are proactively identified by server-side computations are listed as notifications in a feed. The feed turns genomic data analysis into a collaborative work between the analyst and the computational environment, which shortens the analysis time and allows the analyst to explore results efficiently.",
        "keywords": [],
        "_type_counter": 66
    },
    {
        "type": "inproceedings",
        "id": "Cui2019",
        "title": "Sherpa: Leveraging User Attention for Computational Steering in Visual Analytics",
        "author": [
            "Zhe Cui",
            "Jayaram Kancherla",
            "Hector Corrada Bravo",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/sherpa/sherpa.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-10-20T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Symposium on Visualization in Data Science",
        "booktitle": "Proceedings of the IEEE Symposium on Visualization in Data Science",
        "publisher": "IEEE",
        "abstract": "We present Sherpa, a computational steering mechanism for progressive visual analytics that automatically prioritizes computations based on the analyst\u2019s navigational behavior in the data. The intuition is that navigation in data space is an indication of the analyst's interest in the data. Sherpa implementation provides computational modules, such as statistics of biological inferences about gene regulation. The position of the navigation window on the genomic sequence over time is used to prioritize computations. In a study with genomic and visualization analysts, we found that Sherpa provided comparable accuracy to the offline condition, where computations were completed prior to analysis, with shorter completion times. We also provide a second example on stock market analysis.",
        "keywords": [],
        "_type_counter": 56
    },
    {
        "type": "article",
        "id": "Mathisen2019",
        "title": "InsideInsights: Integrating Data\u2010Driven Reporting in Collaborative Visual Analytics",
        "author": [
            "Andreas Mathisen",
            "Tom Horak",
            "Clemens Nylandsted Klokmose",
            "Kaj Gr\u00f8nb\u00e6k",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/insideinsights/insideinsights.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-06-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 38,
        "number": 3,
        "pages": "649--661",
        "abstract": "Analyzing complex data is a non\u2010linear process that alternates between identifying discrete facts and developing overall assessments and conclusions. In addition, data analysis rarely occurs in solitude; multiple collaborators can be engaged in the same analysis, or intermediate results can be reported to stakeholders. However, current data\u2010driven communication tools are detached from the analysis process and promote linear stories that forego the hierarchical and branching nature of data analysis, which leads to either too much or too little detail in the final report. We propose a conceptual design for integrated data\u2010driven reporting that allows for iterative structuring of insights into hierarchies linked to analytic provenance and chosen analysis views. The hierarchies become dynamic and interactive reports where collaborators can review and modify the analysis at a desired level of detail. Our web\u2010based InsideInsights system provides interaction techniques to annotate states of analytic components, structure annotations, and link them to appropriate presentation views. We demonstrate the generality and usefulness of our system with two use cases and a qualitative expert review.",
        "keywords": [],
        "_type_counter": 65
    },
    {
        "type": "article",
        "id": "Choi2019",
        "title": "Visualizing for the Non\u2010Visual: Enabling the Visually Impaired to Use Visualization",
        "author": [
            "Jinho Choi",
            "Sanghun Jung",
            "Deok Gun Park",
            "Jaegul Choo",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/vis4nonvisual/vis4nonvisual.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-06-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 38,
        "number": 3,
        "pages": "249--260",
        "abstract": "The majority of visualizations on the web are still stored as raster images, making them inaccessible to visually impaired users. We propose a deep\u2010neural\u2010network\u2010based approach that automatically recognizes key elements in a visualization, including a visualization type, graphical elements, labels, legends, and most importantly, the original data conveyed in the visualization. We leverage such extracted information to provide visually impaired people with the reading of the extracted information. Based on interviews with visually impaired users, we built a Google Chrome extension designed to work with screen reader software to automatically decode charts on a webpage using our pipeline. We compared the performance of the back\u2010end algorithm with existing methods and evaluated the utility using qualitative feedback from visually impaired users.",
        "keywords": [],
        "_type_counter": 64
    },
    {
        "type": "article",
        "id": "Yau2019",
        "title": "Bridging the Data Analysis Communication Gap Utilizing a Three-Component Summarized Line Graph",
        "author": [
            "Calvin Yau",
            "Morteza Karimzadeh",
            "Chittayong Surakitbanharn",
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/sumlinegraph/sumlinegraph.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-06-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 38,
        "number": 3,
        "pages": "375--386",
        "abstract": "Communication\u2010minded visualizations are designed to provide their audience\u2014managers, decision\u2010makers, and the public\u2014with new knowledge. Authoring such visualizations effectively is challenging because the audience often lacks the expertise, context, and time that professional analysts have at their disposal to explore and understand datasets. We present a novel summarized line graph visualization technique designed specifically for data analysts to communicate data to decision\u2010makers more effectively and efficiently. Our summarized line graph reduces a large and detailed dataset of multiple quantitative time\u2010series into (1) representative data that provides a quick takeaway of the full dataset; (2) analytical highlights that distinguish specific insights of interest; and (3) a data envelope that summarizes the remaining aggregated data. Our summarized line graph achieved the best overall results when evaluated against line graphs, band graphs, stream graphs, and horizon graphs on four representative tasks.",
        "keywords": [],
        "_type_counter": 63
    },
    {
        "type": "inproceedings",
        "id": "Chidambaram2019",
        "title": "Shape Structuralizer: Design, Fabrication and Exploring Structually-Sound Scaffolded Constructions using 3D Mesh Models",
        "author": [
            "Subramanian Chidambaram",
            "Yunbo Zhang",
            "Venkatraghavan Sundararajan",
            "Ana M. Villanueva",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "https://engineering.purdue.edu/cdesign/wp/wp-content/uploads/2019/02/Shape-Structuralizer-Design-Fabrication-and-User-driven-Iterative-Refinement-of-3D-Mesh-Models.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-05-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "663:1--663:12",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Current Computer-Aided Design (CAD) tools lack proper support for guiding novice users towards designs ready for fabrication. We propose Shape Structuralizer (SS), an interactive design support system that repurposes surface models into structural constructions using rods and custom 3D-printed joints. Shape Structuralizer embeds a recommendation system that computationally supports the user during design ideation by providing design suggestions on local refinements of the design. This strategy enables novice users to choose designs that both satisfy stress constraints as well as their personal design intent. The interactive guidance enables users to repurpose existing surface mesh models, analyze them in-situ for stress and displacement constraints, add movable joints to increase functionality, and attach a customized appearance. This also empowers novices to fabricate even complex constructs while ensuring structural soundness. We validate the Shape Structuralizer tool with a qualitative user study where we observed that even novice users were able to generate a large number of structurally safe designs for fabrication.",
        "keywords": [],
        "_type_counter": 55
    },
    {
        "type": "inproceedings",
        "id": "Mylvarapu2019",
        "title": "Ranked-List Visualization: A Graphical Perception Study",
        "author": [
            "Pranathi Mylavarapu",
            "Mehmet Adil Yalcin",
            "Xan Gregg",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/ranked-list/ranked-list.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-05-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "192:1--192:12",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "Visualization of ranked lists is a common occurrence, but many in-the-wild solutions fly in the face of vision science and visualization wisdom. For example, treemaps and bubble charts are commonly used for this purpose, despite the fact that the data is not hierarchical and that length is easier to perceive than area. Furthermore, several new visual representations have recently been suggested in this area, including wrapped bars, packed bars, piled bars, and Zvinca plots. To quantify the differences and trade-offs for these ranked-list visualizations, we here report on a crowdsourced graphical perception study involving six such visual representations, including the ubiquitous scrolled barchart, in three tasks: ranking (assessing a single item), comparison (two items), and average (assessing global distribution). Results show that wrapped bars may be the best choice for visualizing ranked lists, and that treemaps are surprisingly accurate despite the use of area rather than length to represent value.",
        "keywords": [],
        "_type_counter": 54
    },
    {
        "type": "inproceedings",
        "id": "Horak2019",
        "title": "Vistribute: Distributing Interactive Visualizations in Dynamic Multi-Device Setups",
        "author": [
            "Tom Horak",
            "Andreas Mathisen",
            "Clemens Nylandsted Klokmose",
            "Raimund Dachselt",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/vistribute/vistribute.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-05-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "616:1--616:13",
        "publisher": "ACM",
        "address": "New York, NY, USA",
        "abstract": "We present Vistribute, a framework for the automatic distribution of visualizations and UI components across multiple heterogeneous devices. Our framework consists of three parts: (i) a design space considering properties and relationships of interactive visualizations, devices, and user preferences in multi-display environments; (ii) specific heuristics incorporating these dimensions for guiding the distribution for a given interface and device ensemble; and (iii) a web-based implementation instantiating these heuristics to automatically generate a distribution as well as providing interaction mechanisms for user-defined adaptations. In contrast to existing UI distribution systems, we are able to infer all required information by analyzing the visualizations and devices without relying on additional input provided by users or programmers. In a qualitative study, we let experts create their own distributions and rate both other manual distributions and our automatic ones. We found that all distributions provided comparable quality, hence validating our framework.",
        "keywords": [],
        "_type_counter": 53
    },
    {
        "type": "inproceedings",
        "id": "Zhao2019",
        "title": "Understanding Partitioning and Sequence in Data-Driven Storytelling: The Case for Comic Strip Narration.",
        "author": [
            "Zhenpeng Zhao",
            "Rachael Marr",
            "Jason Shaffer",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/datacomics/datacomics.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-04-01T00:00:00.000Z",
        "journal": "Proceedings of the iConference",
        "booktitle": "Proceedings of the iConference",
        "volume": 11420,
        "pages": "327--338",
        "publisher": "Springer",
        "abstract": "The comic strip narrative style is an effective method for data-driven storytelling. However, surely it is not enough to just add some speech bubbles and clipart to your PowerPoint slideshow to turn it into a data comic? In this paper, we investigate aspects of partitioning and sequence as fundamental mechanisms for comic strip narration: chunking complex visuals into manageable pieces, and organizing them into a meaningful order, respectively. We do this by presenting results from a qualitative study designed to elicit differences in participant behavior when solving questions using a complex infographic compared to when the same visuals are organized into a data comic.",
        "keywords": [],
        "_type_counter": 52
    },
    {
        "type": "article",
        "id": "Patnaik2019",
        "title": "Information Olfactation: Harnessing Scent to Convey Data",
        "author": [
            "Biswaksen Patnaik",
            "Andrea Batch",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/info-olfac/info-olfac.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present VISCENT: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general",
        "keywords": [],
        "_type_counter": 62
    },
    {
        "type": "article",
        "id": "Ondov2019",
        "title": "Face to Face: Evaluating Visual Comparison",
        "author": [
            "Brian Ondov",
            "Nicole Jardine",
            "Niklas Elmqvist",
            "Steven Franconeri"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/face2face/face2face.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.",
        "keywords": [],
        "_type_counter": 61
    },
    {
        "type": "article",
        "id": "Badam2019b",
        "title": "Elastic Documents: Coupling Text and Tables through Contextual Visualizations for Enhanced Document Reading",
        "author": [
            "Sriram Karthik Badam",
            "Zhicheng Liu",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/elastic-documents/elastic-documents.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Today's data-rich documents are often complex datasets in themselves, consisting of information in different formats such as text, gures, and data tables. These additional media augment the textual narrative in the document. However, the static layout of a traditional for-print document often impedes deep understanding of its content because of the need to navigate to access content scattered throughout the text. In this paper, we seek to facilitate enhanced comprehension of such documents through a contextual visualization technique that couples text content with data tables contained in the document. We parse the text content and data tables, cross-link the components using a keyword-based matching algorithm, and generate on-demand visualizations based on the reader's current focus within a document. We evaluate this technique in a user study comparing our approach to a traditional reading experience. Results from our study show that (1) participants comprehend the content better with tighter coupling of text and data, (2) the contextual visualizations enable participants to develop better summaries that capture the main data-rich insights within the document, and (3) overall, our method enables participants to develop a more detailed understanding of the document content.",
        "keywords": [],
        "_type_counter": 60
    },
    {
        "type": "article",
        "id": "Badam2019a",
        "title": "Vistrates: A Component Model for Ubiquitous Analytics",
        "author": [
            "Sriram Karthik Badam",
            "Andreas Mathisen",
            "Roman R\u00e4dle",
            "Clemens Nylandsted Klokmose",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/vistrates/vistrates.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Visualization tools are often specialized for specic tasks, which turns the user's analytical workow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components\u2014the building blocks of this model\u2014can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic \"anytime\" and \"anywhere\" motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices.",
        "keywords": [],
        "_type_counter": 59
    },
    {
        "type": "article",
        "id": "zcui2018",
        "title": "DataSite: Proactive Visual Data Exploration with Computation of Insight-based Recommendations",
        "author": [
            "Zhe Cui",
            "Sriram Karthik Badam",
            "Mehmet Adil Yalcin",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/datasite/datasite.pdf",
        "_url_type": "pdf",
        "year": 2019,
        "date": "2019-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 18,
        "number": 2,
        "pages": "251--267",
        "abstract": "Effective data analysis ideally requires the analyst to have high expertise as well as high knowledge of the data. Even with such familiarity, manually pursuing all potential hypotheses and exploring all possible views is impractical. We present DataSite, a proactive visual analytics system where the burden of selecting and executing appropriate computations is shared by an automatic server-side computation engine. Salient features identified by these automatic background processes are surfaced as notifications in a feed timeline. DataSite effectively turns data analysis into a conversation between analyst and computer, thereby reducing the cognitive load and domain knowledge requirements. We validate the system with a user study comparing it to a recent visualization recommendation system, yielding significant improvement, particularly for complex analyses that existing analytics systems do not support well.",
        "keywords": [],
        "_type_counter": 58
    },
    {
        "type": "inproceedings",
        "id": "Batch2018",
        "title": "Gesture and Action Discovery for Evaluating Virtual Environments with Semi-Supervised Segmentation of Telemetry Records",
        "author": [
            "Andrea Batch",
            "Hanuma Teja Maddali",
            "Kyungjun Lee",
            "Niklas Elmqvist"
        ],
        "url": "http://users.umiacs.umd.edu/~elm/projects/hceye/vr-telemetry.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-12-10T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Conference on Artificial Intelligence & Virtual Reality",
        "booktitle": "Proceedings of the IEEE Conference on Artificial Intelligence & Virtual Reality",
        "pages": "1--10",
        "publisher": "IEEE",
        "abstract": "In this paper, we propose a novel pipeline for semi-supervised behavioral coding of videos of users testing a device or interface, with an eye toward human-computer interaction evaluation for virtual reality. Our system applies existing statistical techniques for time-series classification, including e-divisive change point detection and \"Symbolic Aggregate approXimation\" (SAX) with agglomerative hierarchical clustering, to 3D pose telemetry data. These techniques create classes of short segments of single-person video data\u2013short actions of potential interest called \"micro-gestures.\" A long short-term memory (LSTM) layer then learns these micro-gestures from pose features generated purely from video via a pretrained OpenPose convolutional neural network (CNN) to predict their occurrence in unlabeled test videos. We present and discuss the results from testing our system on the single user pose videos of the CMU Panoptic Dataset. ",
        "keywords": [],
        "_type_counter": 51
    },
    {
        "type": "inproceedings",
        "id": "Gold2018",
        "title": "Clinical Concept Value Sets and Interoperability in Health Data Analytics",
        "author": [
            "Sigfried Gold",
            "Andrea Batch",
            "Robert McClure",
            "Guoqian Jiang",
            "Hadi Kharrazi",
            "Rishi Saripalle",
            "Vojtech Huser",
            "Chunhua Weng",
            "Nancy Roderer",
            "Ana Szarfman",
            "Niklas Elmqvist",
            "David Gotz"
        ],
        "url": "https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6371254/pdf/2973630.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-11-03T00:00:00.000Z",
        "journal": "Proceedings of the Annual AMIA Symposium",
        "booktitle": "Proceedings of the Annual AMIA Symposium",
        "abstract": "This paper focuses on value sets as an essential component in the health analytics ecosystem. We discuss shared repositories of reusable value sets and offer recommendations for their further development and adoption. In order to motivate these contributions, we explain how value sets fit into specific analytic tasks and the health analytics landscape more broadly; their growing importance and ubiquity with the advent of Common Data Models, Distributed Research Networks, and the availability of higher order, reusable analytic resources like electronic phenotypes and electronic clinical quality measures; the formidable barriers to value set reuse; and our introduction of a concept-agnostic orientation to vocabulary collections. The costs of ad hoc value set management and the benefits of value set reuse are described or implied throughout. Our standards, infrastructure, and design recommendations are not systematic or comprehensive but invite further work to support value set reuse for health analytics",
        "keywords": [],
        "_type_counter": 50
    },
    {
        "type": "inproceedings",
        "id": "Chandrasegaran2018",
        "title": "How Do Sketching and Non-Sketching Actions Convey Design Intent?",
        "author": [
            "Senthil Chandrasegaran",
            "Devarajan Ramanujan",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/design-intent/design-intent.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-06-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Designing Interactive Systems",
        "booktitle": "Proceedings of the ACM Conference on Designing Interactive Systems",
        "pages": "373--385",
        "abstract": "Sketches are much more than marks on paper; they play a key role for designers both in ideation and problem-solving as well as in communication with other designers. Thus, the act of sketching is often enriched with annotations, references, and physical actions, such as gestures or speech\u2014all of which constitute meta-data about the designer\u2019s reasoning. Conventional paper-based design notebooks cannot capture this rich metadata, but digital design notebooks can. To understand how and what data to capture, we conducted an observational study of design practitioners where they explore design solutions for a set of problems. We recorded and coded their sketching and non-sketching actions that reflect their exploration of the design space. We then categorized the captured meta-data and mapped observed physical actions to design intent. These findings inform the creation of future digital design notebooks that can better capture designers\u2019 reasoning during sketching.",
        "keywords": [],
        "_type_counter": 49
    },
    {
        "type": "article",
        "id": "Chevalier2018",
        "title": "Observations and Reflections on Visualization Literacy at the Elementary School Level",
        "author": [
            "Fanny Chevalier",
            "Nathalie Henry Riche",
            "Basak Alper",
            "Catherine Plaisant",
            "Jeremy Boy",
            "Niklas Elmqvist"
        ],
        "url": "http://www.cs.umd.edu/hcil/trs/2018-06/2018-06.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-05-01T00:00:00.000Z",
        "journal": "IEEE Computer Graphics & Applications",
        "volume": 38,
        "number": 3,
        "pages": "21--29",
        "abstract": "In this article, we share our reflections on visualization literacy and how it might be better developed in early education. We base this on lessons we learned while studying how teachers instruct, and how members acquire basic visualization principles and skills in elementary school. We use these findings to propose directions for future research on visualization literacy. ",
        "keywords": [],
        "_type_counter": 57
    },
    {
        "type": "article",
        "id": "Wagner2018",
        "title": "Metaviz: interactive statistical and visual analysis of metagenomic data",
        "author": [
            "Justin Wagner",
            "Florin Chelaru",
            "Jayaram Kancherla",
            "Joseph N. Paulson",
            "Alexander Zhang",
            "Victor Felix",
            "Anup Mahurkar",
            "Niklas Elmqvist",
            "Hector Corrada Bravo"
        ],
        "url": "https://academic.oup.com/nar/article-pdf/46/6/2777/24526301/gky136.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-03-06T00:00:00.000Z",
        "journal": "Nucleic Acids Research",
        "volume": 46,
        "number": 6,
        "pages": "2777--2787",
        "abstract": "Large studies profiling microbial communities and their association with healthy or disease phenotypes are now commonplace. Processed data from many of these studies are publicly available but significant effort is required for users to effectively organize, explore and integrate it, limiting the utility of these rich data resources. Effective integrative and interactive visual and statistical tools to analyze many metagenomic samples can greatly increase the value of these data for researchers. We present Metaviz, a tool for interactive exploratory data analysis of annotated microbiome taxonomic community profiles derived from marker gene or whole metagenome shotgun sequencing. Metaviz is uniquely designed to address the challenge of browsing the hierarchical structure of metagenomic data features while rendering visualizations of data values that are dynamically updated in response to user navigation. We use Metaviz to provide the UMD Metagenome Browser web service, allowing users to browse and explore data for more than 7000 microbiomes from published studies. Users can also deploy Metaviz as a web service, or use it to analyze data through the metavizr package to interoperate with state-of-the-art analysis tools available through Bioconductor. Metaviz is free and open source with the code, documentation and tutorials publicly accessible.",
        "keywords": [],
        "_type_counter": 56
    },
    {
        "type": "article",
        "id": "Cui2018",
        "title": "VisHive: Supporting Web-based Visualization through Ad-hoc Computational Clusters of Mobile Devices",
        "author": [
            "Zhe Cui",
            "Shivalik Sen",
            "Sriram Karthik Badam",
            "Niklas Elmqvist "
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/vishive/vishive.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "abstract": "Current web-based visualizations are designed for single computers and cannot make use of additional devices on the client side, even if today\u2019s users often have access to several, such as a tablet, a smartphone, and a smartwatch. We present a framework for ad-hoc computational clusters that leverage these local devices for visualization computations. Furthermore, we present an instantiating JavaScript toolkit called VisHive for constructing web-based visualization applications that can transparently connect multiple devices---called cells---into such ad-hoc clusters---called a hive---for local computation. Hives are formed either using a matchmaking service or through manual configuration. Cells are organized into a master-slave architecture, where the master provides the visual interface to the user and controls the slaves, and the slaves perform computation. VisHive is built entirely using current web technologies, runs in the native browser of each cell, and requires no specific software to be downloaded on the involved devices. We demonstrate VisHive using four distributed examples: a text analytics visualization, a database query for exploratory visualization, a",
        "keywords": [],
        "_type_counter": 55
    },
    {
        "type": "article",
        "id": "Park2018",
        "title": "ATOM: A Grammar for Unit Visualization",
        "author": [
            "Deok Gun Park",
            "Steven M. Drucker",
            "Roland Fernandez",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/atom/atom.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Unit visualizations are a family of visualizations where every data item is represented by a unique visual mark---a visual unit---during visual encoding. For certain datasets and tasks, unit visualizations can provide more information, better match the user's mental model, and enable novel interactions compared to traditional aggregated visualizations. Current visualization grammars cannot fully describe the unit visualization family. In this paper, we characterize the design space of unit visualizations to derive a grammar that can express them. The resulting grammar is called ATOM, and is based on passing data through a series of layout operations that divide the output of previous operations recursively until the size and position of every data point can be determined. We evaluate the expressive power of the grammar by both using it to describe existing unit visualizations, as well as to suggest new unit visualizations.",
        "keywords": [],
        "_type_counter": 54
    },
    {
        "type": "inproceedings",
        "id": "Zhang2018",
        "title": "TopoText: Context-Preserving Semantic Exploration Across Multiple Spatial Scales",
        "author": [
            "Jiawei Zhang",
            "Chittayong Surakitbanharn",
            "Niklas Elmqvist",
            "Ross Maciejewski",
            "Zhenyu Quan",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/topotext/topotext.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "abstract": "TopoText is a context-preserving technique for visualizing semantic data for multi-scale spatial aggregates to gain insight into spatial phenomena. Conventional exploration requires users to navigate across multiple scales but only presents the information related to the current scale. This limitation potentially adds more steps of interaction and cognitive overload to the users. TopoText renders multi-scale aggregates into a single visual display combining novel text-based encoding and layout methods that draw labels along the boundary or filled within the aggregates. The text itself not only summarizes the semantics at each individual scale, but also indicates the spatial coverage of the aggregates and their underlying hierarchical",
        "keywords": [],
        "_type_counter": 48
    },
    {
        "type": "inproceedings",
        "id": "Horak2018",
        "title": "When David Meets Goliath: Combining Smartwatches with a Large Vertical Display for Visual Data Exploration",
        "author": [
            "Tom Horak",
            "Sriram Karthik Badam",
            "Niklas Elmqvist",
            "Raimund Dachselt"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/david-goliath/david-goliath.pdf",
        "_url_type": "pdf",
        "year": 2018,
        "date": "2018-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "abstract": "We explore the combination of smartwatches and a large interactive display to support visual data analysis. These two extremes of interactive surfaces are increasingly popular, but feature different characteristics\u2014display and input modalities, personal/public use, performance, and portability. In this paper, we first identify possible roles for both devices and the interplay between them through an example scenario. We then propose a conceptual framework to enable analysts to explore data items, track interaction histories, and alter visualization configurations through mechanisms using both devices in combination. We validate an implementation of our framework through a formative evaluation and a user study. The results show that this device combination, compared to just a large display, allows users to develop complex insights more fluidly by leveraging the roles of the two devices. Finally, we report on the interaction patterns and interplay between the devices for visual exploration as observed during our study.",
        "keywords": [],
        "_type_counter": 47
    },
    {
        "type": "article",
        "id": "Batch2017",
        "title": "The Interactive Visualization Gap in Initial Exploratory Data Analysis",
        "author": [
            "Andrea Batch",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/visgap/visgap.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-10-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Data scientists and other analytic professionals often use interactive visualization in the dissemination phase at the end of a workflow during which findings are communicated to a wider audience. Visualization scientists, however, hold that interactive representation of data can also be used during exploratory analysis itself. Since the use of interactive visualization is optional rather than mandatory, this leaves a \u201cvisualization gap\u201d during initial exploratory analysis that is the onus of visualization researchers to fill. In this paper, we explore areas where visualization would be beneficial in applied research by conducting a design study using a novel variation on contextual inquiry conducted with professional data analysts. Based on these interviews and experiments, we propose a set of interactive initial exploratory visualization guidelines which we believe will promote adoption by this type of user.",
        "keywords": [],
        "_type_counter": 53
    },
    {
        "type": "article",
        "id": "Park2017",
        "title": "ConceptVector: Text Visual Analytics via Interactive Lexicon Building using Word Embedding",
        "author": [
            "Deok Gun Park",
            "Seungyeon Kim",
            "Jurim Lee",
            "Jaegul Choo",
            "Nicholas Diakopoulos",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/conceptvector/conceptvector.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-10-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building such concepts from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of human language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides the user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts using user seed terms, we introduce a bipolar concept model and support for irrelevant words. We validate the interactive lexicon building interface via a user study and expert reviews. The quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.",
        "keywords": [],
        "_type_counter": 52
    },
    {
        "type": "article",
        "id": "Badam2017bb",
        "title": "Visfer: Camera-based Visual Data Transfer for Cross-Device Visualization",
        "author": [
            "Sriram Karthik Badam",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/qrvis/visfer.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-09-08T00:00:00.000Z",
        "journal": "Information Visualization",
        "abstract": "Going beyond the desktop to leverage novel devices\u2014such as smartphones, tablets, or large displays\u2014for visual sensemaking typically requires supporting extraneous operations for device discovery, interaction sharing, and view management. Such operations can be time-consuming and tedious, and distract the user from the actual analysis. Embodied interaction models in these multi-device environments can take advantage of the natural interaction and physicality afforded by multimodal devices and help effectively carry out these operations in visual sensemaking. In this paper, we present cross-device interaction models for visualization spaces, that are embodied in nature, by conducting a user study to elicit actions from participants that could trigger a portrayed effect of sharing visualizations (and therefore information) across devices. We then explore one common interaction style from this design elicitation called Visfer, a technique for effortlessly sharing visualizations across devices using the visual medium. More specifically, this technique involves taking pictures of visualizations, or rather the QR codes augmenting them, on a display using the built-in camera on a handheld device. Our contributions include a conceptual framework for cross-device interaction and the Visfer technique itself, as well as transformation guidelines to exploit the capabilities of each specific device and a web framework for encoding visualization components into animated QR codes, which capture multiple frames of QR codes to embed more information. Beyond this, we also present the results from a performance evaluation for the visual data transfer enabled by Visfer. We end the paper by presenting the application examples of our Visfer framework. ",
        "keywords": [],
        "_type_counter": 51
    },
    {
        "type": "article",
        "id": "Yalcin2017b",
        "title": "Keshif: Rapid and Expressive Tabular Data Exploration for Novices",
        "author": [
            "Mehmet Adil Yalcin",
            "Niklas Elmqvist",
            "Benjamin B. Bederson"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/keshif/keshif.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-19T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "abstract": "General purpose graphical interfaces for data exploration are typically based on manual visualization and interaction specifications. While designing manual specification can be very expressive, it demands high efforts to make effective decisions, therefore reducing exploratory speed. Instead, principled automated designs can increase exploratory speed, decrease learning efforts, help avoid ineffective decisions, and therefore better support data analytics novices. Towards these goals, we present Keshif, a new systematic design for tabular data exploration. To summarize a given dataset, Keshif aggregates records by value within attribute summaries, and visualizes aggregate characteristics using a consistent design based on data types. To reveal data distribution details, Keshif features three complementary linked selections: highlighting, filtering, and comparison. Keshif further increases expressiveness through aggregate metrics, absolute/part-of scale modes, calculated attributes, and saved selections, all working in synchrony. Its automated design approach also simplifies authoring of dashboards composed of summaries and individual records from raw data using fluid interaction. We show examples selected from 160+ datasets from diverse domains. Our study with novices shows that after exploring raw data for 15 minutes, our participants reached close to 30 data insights on average, comparable to other studies with skilled users using more complex tools.",
        "keywords": [],
        "_type_counter": 50
    },
    {
        "type": "inproceedings",
        "id": "Yalcin2017",
        "title": "Raising the Bars: Evaluating Treemaps vs. Wrapped Bars for Dense Visualization of Sorted Numeric Data",
        "author": [
            "Mehmet Adil Yalcin",
            "Niklas Elmqvist",
            "Benjamin B. Bederson"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/raising-bars/RaisingTheBars-GI2017.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-15T00:00:00.000Z",
        "journal": "Proceedings of Graphics Interface",
        "booktitle": "Proceedings of Graphics Interface",
        "abstract": "A standard (single-column) bar chart can effectively visualize a sorted list of numeric records. However, the chart height limits the number of visible records. To show more records, the bars could be made thinner (which could hinder identifying records individually), and scrolling requires interaction to see the overview. Treemaps have been used in practice in non-hierarchical settings for dense visualization of numeric data. Alternatively, we consider wrapped bars, a multi-column bar chart that uses length instead of area to encode numeric values. We compare treemaps and wrapped bars based on their design characteristics, and graphical perception performance for comparison, ranking, and overview tasks using crowdsourced experiments. Our analysis found that wrapped bars perceptually outperform treemaps in all three tasks for dense visualization of non-hierarchical, sorted numeric data.",
        "keywords": [],
        "_type_counter": 46
    },
    {
        "type": "inproceedings",
        "id": "Chandrasegaran2017",
        "title": "Merging Sketches for Creative Design Exploration: An Evaluation of Physical and Cognitive Operations",
        "author": [
            "Senthil Chandrasegaran",
            "Sriram Karthik Badam",
            "Ninger Zhou",
            "Zhenpeng Zhao",
            "Lorraine Kisselburgh",
            "Kylie Peppler",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/merge-study/merge-study.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-15T00:00:00.000Z",
        "journal": "Proceedings of Graphics Interface",
        "booktitle": "Proceedings of Graphics Interface",
        "abstract": "Despite its grounding in creativity techniques, merging multiple source sketches to create new ideas has received scant attention in design literature. In this paper, we identify the physical operations that in merging sketch components. We also introduce cognitive operations of reuse, repurpose, refactor, and reinterpret, and explore their relevance to creative design. To examine the relationship of cognitive operations, physical techniques, and creative sketch outcomes, we conducted a qualitative user study where student designers merged existing sketches to generate either an alternative design, or an unrelated new design. We compared two digital selection techniques: freeform selection, and a stroke-cluster-based \"object select\" technique. The resulting merge sketches were subjected to crowdsourced evaluation of these sketches, and manual coding for the use of cognitive operations. Our findings establish a firm connection between the proposed cognitive operations and the context and outcome of creative tasks. Key findings indicate that reinterpret cognitive operations correlate strongly with creativity in merged sketches, while reuse operations correlate negatively with creativity. Furthermore, freeform selection techniques are preferred significantly by designers. We discuss the empirical contributions of understanding the use of cognitive operations during design exploration, and the practical implications for designing interfaces in digital tools that facilitate creativity in merging sketches. ",
        "keywords": [],
        "_type_counter": 45
    },
    {
        "type": "inproceedings",
        "id": "Badam2017",
        "title": "Supporting Team-First Visual Analytics through Group Activity Representations",
        "author": [
            "Sriram Karthik Badam",
            "Zehua Zheng",
            "Emily Wall",
            "Alex Endert",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/group-awareness/group-awareness.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-15T00:00:00.000Z",
        "journal": "Proceedings of Graphics Interface",
        "booktitle": "Proceedings of Graphics Interface",
        "abstract": "Collaborative visual analytics (CVA) involves sensemaking activities within teams of analysts based on coordination of work across team members, awareness of team activity, and communication of hypotheses, observations, and insights. We introduce a new type of CVA tools based on the notion of \"team-first\" visual analytics, where supporting the analytical process and needs of the entire team is the primary focus of the graphical user interface before that of the individual analysts. To this end, we present the design space and guidelines for team-first tools in terms of conveying analyst presence, focus, and activity within the interface. We then introduce InsightsDrive, a CVA tool for multidimensional data, that contains team-first features into the interface through group activity visualizations. This includes (1) in-situ representations that show the focus regions of all users integrated in the data visualizations themselves using color-coded selection shadows, as well as (2) ex-situ representations showing the data coverage of each analyst using multidimensional visual representations. We conducted two user studies, one with individual analysts to identify the affordances of different visual representations to inform data coverage, and the other to evaluate the performance of our team-first design with exsitu and in-situ awareness for visual analytic tasks. Our results give an understanding of the performance of our team-first features and unravel their advantages for team coordination.",
        "keywords": [],
        "_type_counter": 44
    },
    {
        "type": "article",
        "id": "Badam2017b",
        "title": "Steering the Craft: UI Elements and Visualizations for Supporting Progressive Visual Analytics",
        "author": [
            "Sriram Karthik Badam",
            "Niklas Elmqvist",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/insightsfeed/insightsfeed.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-15T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 36,
        "abstract": "Progressive visual analytics (PVA) has emerged in recent years to manage the latency of data analysis systems. When analysis is performed progressively, rough estimates of the results are generated quickly and are then improved over time. Analysts can therefore monitor the progression of the results, steer the analysis algorithms, and make early decisions if the estimates provide a convincing picture. In this article, we describe interface design guidelines for helping users understand progressively updating results and make early decisions based on progressive estimates. To illustrate our ideas, we present a prototype PVA tool called InsightsFeed for exploring Twitter data at scale. As validation, we investigate the tradeoffs of our tool when exploring a Twitter dataset in a user study. We report the usage patterns in making early decisions using the user interface, guiding computational methods, and exploring different subsets of the dataset, compared to sequential analysis without progression.",
        "keywords": [],
        "_type_counter": 49
    },
    {
        "type": "article",
        "id": "Chandrasegaran2017c",
        "title": "Integrating Visual Analytics Support for Grounded Theory Practice in Qualitative Text Analysis",
        "author": [
            "Senthil Chandrasegaran",
            "Sriram Karthik Badam",
            "Lorraine Kisselburgh",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/gthelper/gthelper.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-15T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 36,
        "abstract": "We present an argument for using visual analytics to aid Grounded Theory methodologies in qualitative data analysis. Grounded theory methods involve the inductive analysis of data to generate novel insights and theoretical constructs. Making sense of unstructured text data is uniquely suited for visual analytics. Using natural language processing techniques such as parts-of-speech tagging, retrieving information content, and topic modeling, different parts of the data can be structured and semantically associated, and interactively explored, thereby providing conceptual depth to the guided discovery process. We review grounded theory methods and identify processes that can be enhanced through visual analytic techniques. Next, we develop an interface for qualitative text analysis, and evaluate our design with qualitative research practitioners who analyze texts with and without visual analytics support. The results of our study suggest how visual analytics can be incorporated into qualitative data analysis tools, and the analytic and interpretive benefits that can result.",
        "keywords": [],
        "_type_counter": 48
    },
    {
        "type": "inproceedings",
        "id": "Zhang2017",
        "title": "TopoGroups: Context-Preserving Visual Illustration of Multi-Scale Spatial Aggregates",
        "author": [
            "Jiawei Zhang",
            "Abish Malik",
            "Benjamin Ahlbrand",
            "Niklas Elmqvist",
            "Ross Maciejewski",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/topogroups/topogroups.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-08T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "2940--2951",
        "publisher": "ACM",
        "abstract": "Spatial datasets, such as tweets in a geographic area, often exhibit different distribution patterns at multiple levels of scale, such as live updates about events occurring in very specific locations on the map. Navigating in such multi-scale data-rich spaces is often inefficient, requires users to choose between overview or detail information, and does not support identifying spatial patterns at varying scales. In this paper, we propose TopoGroups, a novel context-preserving technique that aggregates spatial data into hierarchical clusters to improve exploration and navigation at multiple spatial scales. The technique uses a boundary distortion algorithm to minimize the visual clutter caused by overlapping aggregates. Our user study explores multiple visual encoding strategies for TopoGroups including color, transparency, shading, and shapes in order to convey the hierarchical and statistical information of the geographical aggregates at different scales.",
        "keywords": [],
        "_type_counter": 43
    },
    {
        "type": "inproceedings",
        "id": "Piya2017",
        "title": "Co-3Deator: A Team-First Collaborative 3D Design ideation Tool",
        "author": [
            "Cecil Piya",
            "Vinayak",
            "Senthil Chandrasegaran",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/co3deator/co3deator.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-05-08T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "6581--6592",
        "abstract": "We present CO-3DEATOR, a sketch-based collaborative 3D modeling system based on the notion of \u201cteam-first\u201d ideation tools, where the needs and processes of the entire design team come before that of an individual designer. Co-3Deator includes two specific team-first features: a concept component hierarchy which provides a design representation suitable for multi-level sharing and reusing of design information, and a collaborative design explorer for storing, viewing, and accessing hierarchical design data during collaborative design activities. We conduct two controlled user studies, one with individual designers to elicit the form and functionality of the collaborative design explorer, and the other with design teams to evaluate the utility of the concept component hierarchy and design explorer towards collaborative design ideation. Our results support our rationale for both of the proposed team-first collaboration mechanisms and suggest further ways to streamline collaborative design.",
        "keywords": [],
        "_type_counter": 42
    },
    {
        "type": "article",
        "id": "Chandrasegaran2017b",
        "title": "VizScribe: A Visual Analytics Approach to Understand Designer Behavior",
        "author": [
            "Senthil Chandrasegaran",
            "Sriram Karthik Badam",
            "Lorraine Kisselburgh",
            "Kylie Peppler",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/vizscribe/vizscribe.pdf",
        "_url_type": "pdf",
        "year": 2017,
        "date": "2017-01-02T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Interaction",
        "volume": 100,
        "pages": "66--80",
        "abstract": "Design protocol analysis is a technique to understand designers\u2019 cognitive processes by analyzing sequences of observations on their behavior. These observations typically use audio, video, and transcript data in order to gain insights into the designer's behavior and the design process. The recent availability of sophisticated sensing technology has made such data highly multimodal, requiring more flexible protocol analysis tools. To address this need, we present VizScribe, a visual analytics framework that employs multiple coordinated multiple views that enable the viewing of such data from different perspectives. VizScribe allows designers to create, customize, and extend interactive visualizations for design protocol data such as video, transcripts, sketches, sensor data, and user logs. User studies where design researchers used VizScribe for protocol analysis indicated that the linked views and interactive navigation offered by VizScribe afforded the researchers multiple, useful ways to approach and interpret such multimodal data.",
        "keywords": [],
        "_type_counter": 47
    },
    {
        "type": "inproceedings",
        "id": "Nielsen2016",
        "title": "Scribble Query: Fluid Touch Brushing for Multivariate Data Visualization",
        "author": [
            "Matthias Nielsen",
            "Niklas Elmqvist",
            "Kaj Gr\u00f8nb\u00e6k"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/scribble-query/scribble-query.pdf",
        "_url_type": "pdf",
        "year": 2016,
        "date": "2016-12-01T00:00:00.000Z",
        "journal": "Proceedings of the Australian Conference on Human-Computer Interaction",
        "booktitle": "Proceedings of the Australian Conference on Human-Computer Interaction",
        "abstract": "The wide availability of touch-enabled devices is a unique opportunity for visualization research to invent novel techniques to fluently explore, analyse, and understand complex and large-scale data. In this paper, we introduce Scribble Query, a novel interaction technique for fluid freehand scribbling (casual drawing) on touch-enabled devices to support interactive querying in data visualizations. Inspired by the low-entry yet rich interaction of touch drawing applications, a Scribble Query can be created with a single touch stroke yet have the expressiveness of multiple brushes (a conventionally used interaction technique). We have applied the Scribble Query interaction technique in a multivariate visualization tool, deployed the tool with domain experts from five different domains, and conducted deployment studies with these domain experts on their utilization of multivariate visualization with Scribble Query. The studies suggest that Scribble Query has a low entry barrier facilitating easy adoption, casual and infrequent usage, and in one case, enabled live dissemination of findings by the domain expert to managers in the organization.",
        "keywords": [],
        "_type_counter": 41
    },
    {
        "type": "inproceedings",
        "id": "Badam2016b",
        "title": "Supporting Visual Exploration for Multiple Users in Large Display Environments",
        "author": [
            "Sriram Karthik Badam",
            "Feresteh Amini",
            "Niklas Elmqvist",
            "Pourang Irani"
        ],
        "url": "http://umiacs.umd.edu/~elm/projects/multiuser-vis/multiuser-vis.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=xd7G_q8nocc",
        "year": 2016,
        "date": "2016-10-21T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Conference on Visual Analytics Science & Technology",
        "booktitle": "Proceedings of the IEEE Conference on Visual Analytics Science & Technology",
        "abstract": "We present a design space exploration of interaction techniques for supporting multiple collaborators exploring data on a shared large display. Our proposed solution is based on users controlling individual lenses using both explicit gestures as well as proxemics: the spatial relations between people and physical artifacts such as their distance, orientation, and movement. We discuss different design considerations for implicit and explicit interactions through the lens, and evaluate the user experience to find a balance between the implicit and explicit interaction styles. Our findings indicate that users favor implicit interaction through proxemics for navigation and collaboration, but prefer using explicit mid-air gestures to perform actions that are perceived to be direct, such as terminating a lens composition. Based on these results, we propose a hybrid technique utilizing both proxemics and mid-air gestures, along with examples applying this technique to other datasets. Finally, we performed a usability evaluation of the hybrid technique and observed user performance improvements in the presence of both implicit and explicit interaction styles. ",
        "keywords": [],
        "_type_counter": 40
    },
    {
        "type": "article",
        "id": "Kim2017",
        "title": "TopicLens: Efficient Multi-Level Visual Topic Exploration of Large-Scale Document Collections",
        "author": [
            "Minjeong Kim",
            "Kyeongpil Kang",
            "Deokgun Park",
            "Jaegul Choo",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/topiclens/topiclens.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=RKC5w9dZmXQ",
        "year": 2016,
        "date": "2016-08-10T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 23,
        "number": 1,
        "pages": "151--160",
        "abstract": "Topic modeling, which reveals underlying topics of a document corpus, has been actively adopted in visual analytics for large-scale document collections. However, due to its significant processing time and non-interactive nature, topic modeling has so far not been tightly integrated into a visual analytics workflow. Instead, most such systems are limited to utilizing a fixed, initial set of topics. Motivated by this gap in the literature, we propose a novel interaction technique called TopicLens that allows a user to dynamically explore data through a lens interface where topic modeling and the corresponding 2D embedding are efficiently computed on the fly. To support this interaction in real time while maintaining view consistency, we propose a novel efficient topic modeling method and a semi-supervised 2D embedding algorithm. Our work is based on improving state-of-the-art methods such as nonnegative matrix factorization and t-distributed stochastic neighbor embedding. Furthermore, we have built a web-based visual analytics system integrated with TopicLens. We use this system to measure the performance and the visualization quality of our proposed methods. We provide several scenarios showcasing the capability of TopicLens using real-world datasets.",
        "keywords": [],
        "_type_counter": 46
    },
    {
        "type": "inproceedings",
        "id": "Park2016",
        "title": "Supporting Comment Moderators in identifying High Quality Online News Comments",
        "author": [
            "Deok Gun Park",
            "Simranjit Singh",
            "Nicholas Diakopoulos",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/commentiq/commentiq.pdf",
        "_url_type": "pdf",
        "year": 2016,
        "date": "2016-05-05T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "1114--1125",
        "abstract": "Online comments submitted by readers of news articles can provide valuable feedback and critique, personal views and perspectives, and opportunities for discussion. The varying quality of these comments necessitates that publishers remove the low quality ones, but there is also a growing awareness that by identifying and highlighting high quality contributions this can promote the general quality of the community. In this paper we take a user-centered design approach towards developing a system, CommentIQ, which supports comment moderators in interactively identifying high quality comments using a combination of comment analytic scores as well as visualizations and flexible UI components. We evaluated this system with professional comment moderators working at local and national news outlets and provide insights into the utility and appropriateness of features for journalistic tasks, as well as how the system may enable or transform journalistic practices around online comments.",
        "keywords": [],
        "_type_counter": 39
    },
    {
        "type": "inproceedings",
        "id": "Badam2016",
        "title": "TimeFork: Interactive Prediction of Time Series",
        "author": [
            "Sriram Karthik Badam",
            "Jieqiong Zhao",
            "Shivalik Sen",
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/timefork/timefork.pdf",
        "_url_type": "pdf",
        "year": 2016,
        "date": "2016-05-05T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "5409--5420",
        "abstract": "We present TimeFork, an interactive prediction technique to support users predicting the future of time-series data, such as in financial, scientific, or medical domains. TimeFork combines visual representations of multiple time series with prediction information generated by computational models. Using this method, analysts engage in a back-and-forth dialogue with the computational model by alternating between manually predicting future changes through interaction and letting the model automatically determine the most likely outcomes, to eventually come to a common prediction using the model. This computer-supported prediction approach allows for harnessing the user\u2019s knowledge of factors influencing future behavior, as well as sophisticated computational models drawing on past performance. To validate the TimeFork technique, we conducted a user study in a stock market prediction game. We present evidence of improved performance for participants using TimeFork compared to fully manual or fully automatic predictions, and characterize qualitative usage patterns observed during the user study.",
        "keywords": [],
        "_type_counter": 38
    },
    {
        "type": "article",
        "id": "Umapathi2016",
        "title": "Mushaca: A 3-Degrees-of-Freedom Mouse Supporting Rotation",
        "author": [
            "Udayan Umapathi",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/mushaca/mushaca.pdf",
        "_url_type": "pdf",
        "year": 2016,
        "date": "2016-03-09T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Interaction",
        "volume": 32,
        "number": 6,
        "pages": "481--492",
        "abstract": "Based on kinesiology research demonstrating that translation and rotation are inseparable actions in the physical world, we present Mushaca, a 3-degrees-of-freedom mouse that senses rotation in addition to traditional planar position. We present an optical realization of the Mushaca device based on two optical sensors and then evaluate the device through a series of controlled experiments. Our results show that rotation is indeed a useful input modality for a pointing device, and also give some insight into how users perceive the changing coordinate system of the rotating mouse and adapt to this change through kinesthetic learning.",
        "keywords": [],
        "_type_counter": 45
    },
    {
        "type": "article",
        "id": "Bernstein2015",
        "title": "Mutually Coordinated Visualization of Product and Supply Chain Metadata for Sustainable Design",
        "author": [
            "William Z. Bernstein",
            "Devarajan Ramanujan",
            "Devadatta M. Kulkarni",
            "Jeffrey Tew",
            "Niklas Elmqvist",
            "Fu Zhao",
            "Karthik Ramani"
        ],
        "url": "http://doi.org/10.1115/1.4031293",
        "_url_type": "doi",
        "year": 2015,
        "date": "2015-10-01T00:00:00.000Z",
        "journal": "Journal of Mechanical Design",
        "volume": 137,
        "number": 12,
        "pages": 121101,
        "abstract": "In this paper, we present a novel visualization framework for product and supply chain metadata in the context of redesign-related decision scenarios. Our framework is based on the idea of overlaying product-related metadata onto the interactive graph representations of a supply chain and its associated product architecture. By coupling environmental data with graph-based visualizations of product architecture, our framework provides a novel decision platform for expert designers. Here, the user can balance the advantages of a redesign opportunity and manage the associated risk on the product and supply chain. For demonstration, we present ViSER, an interactive visualization tool that provides an interface consisting of different mutually coordinated views providing multiple perspectives on a particular supply chain presentation. To explore the utility of ViSER, we conduct a domain expert exploration using a case study of peripheral computer equipment. Results indicate that ViSER enables new affordances within the decision making process for supply chain redesign.",
        "keywords": [],
        "_type_counter": 44
    },
    {
        "type": "article",
        "id": "Jang2015",
        "title": "MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data",
        "author": [
            "Sujin Jang",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/motionflow/motionflow.pdf",
        "_url_type": "pdf",
        "year": 2015,
        "date": "2015-08-14T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 21,
        "number": 1,
        "pages": "21--30",
        "abstract": "Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose MotionFlow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of MotionFlow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used MotionFlow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use MotionFlow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.",
        "keywords": [],
        "_type_counter": 43
    },
    {
        "type": "article",
        "id": "Yalcin2015",
        "title": "AggreSet: Rich and Scalable Set Exploration using Visualizations of Element Aggregations",
        "author": [
            "Mehmet Adil Yalcin",
            "Niklas Elmqvist",
            "Benjamin B. Bederson"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/aggreset/aggreset.pdf",
        "_url_type": "pdf",
        "year": 2015,
        "date": "2015-08-14T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 21,
        "number": 1,
        "pages": "688--697",
        "abstract": "Datasets commonly include multi-value (set-typed) attributes that describe set memberships over elements, such as genres per movie or courses taken per student. Set-typed attributes describe rich relations across elements, sets, and the set intersections. Increasing the number of sets results in a combinatorial growth of relations and creates scalability challenges. Exploratory tasks (e.g. selection, comparison) have commonly been designed in separation for set-typed attributes, which reduces interface consistency. To improve on scalability and to support rich, contextual exploration of set-typed data, we present AggreSet. AggreSet creates aggregations for each data dimension: sets, set-degrees, set-pair intersections, and other attributes. It visualizes the element count per aggregate using a matrix plot for set-pair intersections, and histograms for set lists, set-degrees and other attributes. Its non-overlapping visual design is scalable to numerous and large sets. AggreSet supports selection, filtering, and comparison as core exploratory tasks. It allows analysis of set relations inluding subsets, disjoint sets and set intersection strength, and also features perceptual set ordering for detecting patterns in set matrices. Its interaction is designed for rich and rapid data exploration. We demonstrate results on a wide range of datasets from different domains with varying characteristics, and report on expert reviews and a case study using student enrollment and degree data with assistant deans at a major public university.",
        "keywords": [],
        "_type_counter": 42
    },
    {
        "type": "article",
        "id": "Elmqvist2015",
        "title": "Patterns for Visualization Evaluation",
        "author": [
            "Niklas Elmqvist",
            "Ji Soo Yi"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/eval-patterns/eval-patterns.pdf",
        "_url_type": "pdf",
        "year": 2015,
        "date": "2015-07-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 14,
        "number": 3,
        "pages": "250--269",
        "abstract": "We propose a pattern-based approach to evaluating data visualization: a set of general and reusable solutions to commonly occurring problems in evaluating visualization tools, techniques, and systems. Patterns have had significant impact in a wide array of disciplines, particularly software engineering, and we believe that they provide a powerful lens for characterizing visualization evaluation practices by offering practical, tried-and-tested tips, and tricks that can be adopted immediately. The 20 patterns presented here have also been added to a freely editable Wiki repository. The motivation for creating this evaluation pattern language is to (a) capture and formalize \"dark\" practices for visualization evaluation not currently recorded in the literature, (b) disseminate these hard-won experiences to researchers and practitioners alike, (c) provide a standardized vocabulary for designing visualization evaluation, and (d) invite the community to add new evaluation patterns to a growing repository of patterns.",
        "keywords": [],
        "_type_counter": 41
    },
    {
        "type": "inproceedings",
        "id": "Dancu2015",
        "title": "Map Navigation Using a Wearable Mid-air Display",
        "author": [
            "Alexandru Dancu",
            "Mickael Fourgeaud",
            "Mohammad Obaid",
            "Morten Fjeld",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/midairmap/midairmap.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=yswf1bJafp8",
        "year": 2015,
        "date": "2015-07-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human-Computer Interaction with Mobile Devices and Services",
        "journal": "Proceedings of the ACM Conference on Human-Computer Interaction with Mobile Devices and Services",
        "pages": "71--76",
        "abstract": "Advances in display technologies will soon make wearable mid-air displays---devices that project dynamic images floating in mid-air relative to a mobile user---widely available. This kind of device will offer new input and output modalities compared to current mobile devices, and display information on the go. In this paper, we present a functional prototype for the purpose of understanding these modalities in more detail, including suitable applications and device placement. We first collected results from an online survey that identified map navigation as one of the most desirable applications and suggested placement preferences. Based on these rankings, we built a physical mid-air display prototype consisting of mobile phone, pico projector, and a holder frame, mountable in two different configurations: wrist and chest. We then designed a user study, asking participants to navigate different physical routes using map navigation displayed in midair. Participants considered the wrist mount to be three times safer in map navigation than the chest mount. The study results validate the use of a mid-air display for map navigation. Based on both our online survey and user study, we derive implications for the design of wearable mid-air displays.",
        "keywords": [],
        "_type_counter": 37
    },
    {
        "type": "article",
        "id": "Zhao2015",
        "title": "Sketcholution: Interaction Histories for Sketching",
        "author": [
            "Zhenpeng Zhao",
            "William Benjamin",
            "Niklas Elmqvist",
            "K. Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/sketcholution/sketcholution.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=SYvkidJQtEk",
        "year": 2015,
        "date": "2015-05-16T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Studies",
        "volume": 82,
        "pages": "11--20",
        "abstract": "We present Sketcholution, a method for automatically creating visual histories of hand-drawn sketches. Such visual histories are useful for a designer to reflect on a sketch, communicate ideas to others, and fork from or revert to an earlier point in the creative process. Our approach uses a bottom-up agglomerative clustering mechanism that groups adjacent frames based on their perceptual similarity while maintaining the causality of how a sketch was constructed. The resulting aggregation dendrogram can be cut at any level depending on available display space, and can be used to create a visual history consisting of either a comic strip of highlights, or a single annotated summary frame. We conducted a user study comparing the speed and accuracy of participants recovering causality in a sketch history using comic strips, summary frames, and simple animations. Although animations with interaction may seem better than static graphics, our results show that both comic strip and summary frame significantly outperform animation.",
        "keywords": [],
        "_type_counter": 40
    },
    {
        "type": "article",
        "id": "Choi2015",
        "title": "VisDock: A Toolkit for Cross-Cutting Interactions in Visualization",
        "author": [
            "Jungu Choi",
            "Deok Gun Park",
            "Yuetling Wong",
            "Eli Raymond Fisher",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/visdock/visdock.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=LUC-nGR-fOk",
        "year": 2015,
        "date": "2015-03-21T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 21,
        "number": 9,
        "pages": "1087--1100",
        "abstract": "Standard user applications provide a range of cross-cutting interaction techniques that are common to virtually all such tools: selection, filtering, navigation, layer management, and cut-and-paste.We present VisDock, a JavaScript mixin library that provides a core set of these cross-cutting interaction techniques for visualization, including selection (lasso, paths, shape selection, etc), layer management (visibility, transparency, set operations, etc), navigation (pan, zoom, overview, magnifying lenses, etc), and annotation (point-based, region-based, data-space based, etc). To showcase the utility of the library, we have released it as Open Source and integrated it with a large number of existing web-based visualizations. Furthermore, we have evaluated VisDock using qualitative studies with both developers utilizing the toolkit to build new web-based visualizations, as well as with end-users utilizing it to explore movie ratings data. Results from these studies highlight the usability and effectiveness of the toolkit from both developer and end-user perspectives.",
        "keywords": [],
        "_type_counter": 39
    },
    {
        "type": "article",
        "id": "Wong2015",
        "title": "Evaluating Social Navigation Visualization in Online Geographic Maps",
        "author": [
            "Yuetling Wong",
            "Jieqiong Zhao",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/socnav-eval/socnav-eval.pdf",
        "_url_type": "pdf",
        "year": 2015,
        "date": "2015-02-22T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Interaction",
        "volume": 31,
        "number": 2,
        "pages": "118--127",
        "abstract": "Social navigation enables emergent collaboration between independent collaborators by exposing the behavior of each individual. This is a powerful idea for web-based visualization, where the work of one user can inform other users interacting with the same visualization. We present results from a crowdsourced user study evaluating the value of such social navigation cues for a geographic map service. Our results show significantly improved performance for participants who interacted with the map when the visual footprints of previous users were visible.",
        "keywords": [],
        "_type_counter": 38
    },
    {
        "type": "article",
        "id": "Gad2015",
        "title": "ThemeDelta: Dynamic Segmentations over Temporal Topic Models",
        "author": [
            "Samah Gad",
            "Waqas Javed",
            "Sohaib Ghani",
            "Niklas Elmqvist",
            "Tom Ewing",
            "Keith N. Hampton",
            "Naren Ramakrishnan"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/theme-delta/theme-delta.pdf",
        "_url_type": "pdf",
        "year": 2015,
        "date": "2015-02-17T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 21,
        "number": 5,
        "pages": "672--685",
        "abstract": "We present ThemeDelta, a visual analytics system for extracting and visualizing temporal trends, clustering, and reorganization in time-indexed textual datasets. ThemeDelta is supported by a dynamic temporal segmentation algorithm that integrates with topic modeling algorithms to identify change points where significant shifts in topics occur. This algorithm detects not only the clustering and associations of keywords in a time period, but also their convergence into topics (groups of keywords) that may later diverge into new groups. The visual representation of ThemeDelta uses sinuous, variable-width lines to show this evolution on a timeline, utilizing color for categories, and line width for keyword strength. We demonstrate how interaction with ThemeDelta helps capture the rise and fall of topics by analyzing archives of historical newspapers, of U.S. presidential campaign speeches, and of social messages collected through iNeighbors, a web-based social website. ThemeDelta was evaluated using a qualitative expert user study involving three researchers from rhetoric and history using the historical newspapers corpus.",
        "keywords": [],
        "_type_counter": 37
    },
    {
        "type": "article",
        "id": "Badam2015",
        "title": "Munin: A Peer-to-Peer Middleware for Ubiquitous Analytics and Visualization Spaces",
        "author": [
            "Sriram Karthik Badam",
            "Eli Raymond Fisher",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/munin/munin.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=ZKIXSdUm6-s",
        "year": 2015,
        "date": "2015-02-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 21,
        "number": 2,
        "pages": "215--228",
        "abstract": "We present Munin, a software framework for building ubiquitous analytics environments consisting of multiple input and output surfaces, such as tabletop displays, wall-mounted displays, and mobile devices. Munin utilizes a service-based model where each device provides one or more dynamically loaded services for input, display, or computation. Using a peer-to-peer model for communication, it leverages IP multicast to replicate the shared state among the peers. Input is handled through a shared event channel that lets input and output devices be fully decoupled. It also provides a data-driven scene graph to delegate rendering to peers, thus creating a robust, fault-tolerant, decentralized system. In this paper, we describe Munin's general design and architecture, provide several examples of how we are using the framework for ubiquitous analytics and visualization, and present a case study on building a Munin assembly for multidimensional visualization. We also present performance results and anecdotal user feedback for the framework that suggests that combining a service-oriented, data-driven model with middleware support for data sharing and event handling eases the design and execution of high performance distributed visualizations.",
        "keywords": [],
        "_type_counter": 36
    },
    {
        "type": "article",
        "id": "Roberts2014",
        "title": "Visualization Beyond the Desktop --- The Next Big Thing",
        "author": [
            "Jonathan C. Roberts",
            "Panagiotis D. Ritsos",
            "Sriram Karthik Badam",
            "Dominique Brodbeck",
            "Jessie Kennedy",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/beyond-desktop/beyond-desktop.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-12-02T00:00:00.000Z",
        "journal": "IEEE Computer Graphics & Applications",
        "volume": 34,
        "number": 6,
        "pages": "26--34",
        "abstract": "Visualization is coming of age: with visual depictions being seamlessly integrated into documents and data visualization techniques being used to understand datasets that are ever-growing in size and complexity, the term visualization is becoming used in everyday conversations. But we are on a cusp; visualization researchers need to develop and adapt to today's new devices and tomorrows technology. Today, we are interacting with visual depictions through a mouse. Tomorrow, we will be touching, swiping, grasping, feeling, hearing, smelling and even tasting our data. The next big thing is multi-sensory visualization that goes beyond the desktop.",
        "keywords": [],
        "_type_counter": 35
    },
    {
        "type": "article",
        "id": "Ko2014",
        "title": "VASA: Interactive Computational Steering of Large Asynchronous Simulation Pipelines for Societal Infrastructure",
        "author": [
            "Sungahn Ko",
            "Jieqiong Zhao",
            "Jing Xia",
            "Shehzad Afzal",
            "Xiaoyu Wang",
            "Greg Abram",
            "Niklas Elmqvist",
            "Len Kne",
            "David Van Riper",
            "Kelly Gaither",
            "Shaun Kennedy",
            "William Tolone",
            "William Ribarsky",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/vasa/vasa.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-11-13T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 20,
        "number": 12,
        "pages": "1853--1862",
        "abstract": "We present VASA, a visual analytics platform consisting of a desktop application, a component model, and a suite of distributed simulation components for modeling the impact of societal threats such as weather, food contamination, and traffic on critical infrastructure such as supply chains, road networks, and power grids. Each component encapsulates a high-fidelity simulation model that together form an asynchronous simulation pipeline: a system of systems of individual simulations with a common data and parameter exchange format. At the heart of VASA is theWorkbench, a visual analytics application providing three distinct features: (1) low-fidelity approximations of the distributed simulation components using local simulation proxies to enable analysts to interactively configure a simulation run; (2) computational steering mechanisms to manage the execution of individual simulation components; and (3) spatiotemporal and interactive methods to explore the combined results of a simulation run. We showcase the utility of the platform using examples involving supply chains during a hurricane as well as food contamination in a fast food restaurant chain.",
        "keywords": [],
        "_type_counter": 34
    },
    {
        "type": "article",
        "id": "Madhavan2014",
        "title": "DIA2: Web-based Cyberinfrastructure for Visual Analytics of Funding Portfolios",
        "author": [
            "Krishna Madhavan",
            "Niklas Elmqvist",
            "Mihaela Vorvoreanu",
            "Xin Chen",
            "Yuetling Wong",
            "Hanjun Xian",
            "Zhihua Dong",
            "Aditya Johri"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/dia2/dia2-vast2014.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-11-13T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization & Computer Graphics",
        "volume": 20,
        "number": 12,
        "pages": "1823--1832",
        "abstract": "We present a design study of the Deep Insights Anywhere, Anytime (DIA2) platform, a web-based visual analytics system that allows program managers and academic staff at the U.S. National Science Foundation to search, view, and analyze their research funding portfolio. The goal of this system is to facilitate users\u02bc understanding of both past and currently active research awards in order to make more informed decisions of their future funding. This user group is characterized by high expertise yet not necessarily high literacy in visualization and visual analytics--they are essentially \"casual experts\"--and thus require careful visual and information design, including adhering to user experience standards, providing a self-instructive interface, and progressively refining visualizations to minimize complexity. We discuss the challenges of designing a system for \"casual experts\" and highlight how we addressed this issue by modeling the organizational structure and workflows of the NSF within our system. We discuss each stage of the design process, starting with formative interviews, participatory design, prototypes, and",
        "keywords": [],
        "_type_counter": 33
    },
    {
        "type": "inproceedings",
        "id": "Badam2014a",
        "title": "Tracing and Sketching Performance using Blunt-Tipped Styli on Direct-Touch Tablets",
        "author": [
            "Sriram Karthik Badam",
            "Senthil Chandrasegaran",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/sketch-media/sketch-media.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-07-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Advanced Visual Interfaces",
        "booktitle": "Proceedings of the ACM Conference on Advanced Visual Interfaces",
        "pages": "193--200",
        "abstract": "Direct-touch tablets are quickly replacing traditional pen-and-paper tools in many applications, but not in case of the designer\u2019s sketchbook. In this paper, we explore the tradeoffs inherent in replacing such paper sketchbooks with digital tablets in terms of two major tasks: tracing and free-hand sketching. Given the importance of the pen for sketching, we also study the impact of using a blunt-and-soft-tipped capacitive stylus in tablet settings. We thus conducted experiments to evaluate three sketch media: pen-paper, finger-tablet, and stylus-tablet based on the above tasks. We analyzed the tracing data with respect to speed and accuracy, and the quality of the free-hand sketches through a crowdsourced survey. The pen-paper and stylus-tablet media both performed significantly better than the finger-tablet medium in accuracy, while the pen-paper sketches were significantly rated higher quality compared to both tablet interfaces. A follow-up study comparing the performance of this stylus with a sharp, hard-tip version showed no significant difference in tracing performance, though participants preferred the sharp tip for sketching.",
        "keywords": [],
        "_type_counter": 36
    },
    {
        "type": "inproceedings",
        "id": "Sujin2014",
        "title": "GestureAnalyzer: Visual Analytics for Exploratory Analysis of Gesture Patterns",
        "author": [
            "Sujin Jang",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/gesture-analyzer/gesture-analyzer.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-07-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Symposium on Spatial User Interfaces",
        "booktitle": "Proceedings of the ACM Symposium on Spatial User Interfaces",
        "pages": "30--39",
        "abstract": "Understanding the intent behind human gestures is a critical problem in the design of gestural interactions. A common method to observe and understand how users express gestures is to use elicitation studies. However, these studies require time-consuming analysis of user data to identify gesture patterns. Also, the analysis by humans cannot describe gestures in as detail as in data-based representations of motion features. In this paper, we present GestureAnalyzer, a system that supports exploratory analysis of gesture patterns by applying interactive clustering and visualization techniques to motion tracking data. GestureAnalyzer enables rapid categorization of similar gestures, and visual investigation of various geometric and kinematic properties of user gestures. We describe the system components, and then demonstrate its utility through a case study on mid-air hand gestures obtained from elicitation studies.",
        "keywords": [],
        "_type_counter": 35
    },
    {
        "type": "inproceedings",
        "id": "Badam2014b",
        "title": "PolyChrome: A Cross-Device Framework for Collaborative Web Visualization",
        "author": [
            "Sriram Karthik Badam",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/polychrome/polychrome.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-07-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "journal": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "pages": "109--118",
        "abstract": "We present PolyChrome, an application framework for creating web-based collaborative visualizations that can span multiple devices. The framework supports (1) co-browsing new web applications as well as legacy websites with no migration costs (i.e., a distributed web browser); (2) an API to develop new web applications that can synchronize the UI state on multiple devices to support synchronous and asynchronous collaboration; and (3) maintenance of state and input events on a server to handle common issues with distributed applications such as consistency management, conflict resolution, and undo operations. We describe PolyChrome's general design, architecture, and implementation followed by application examples showcasing collaborative web visualizations created using the framework. Finally, we present performance results that suggest that PolyChrome adds minimal overhead compared to single-device applications.",
        "keywords": [],
        "_type_counter": 34
    },
    {
        "type": "inproceedings",
        "id": "Benjamin2014",
        "title": "Juxtapoze: supporting serendipity and creative expression in clipart compositions",
        "author": [
            "William Benjamin",
            "Senthil Chandrasegaran",
            "Devarajan Ramanujan",
            "Niklas Elmqvist",
            "SVN Vishwanathan",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/juxtapoze/juxtapoze.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "341--350",
        "abstract": "Juxtapoze is a clipart composition workflow that supports creative expression and serendipitous discoveries in the shape domain. We achieve creative expression by supporting a workflow of searching, editing, and composing: the user queries the shape database using strokes, selects the desired search result, and finally modifies the selected image before composing it into the overall drawing. Serendipitous discovery of shapes is facilitated by allowing multiple exploration channels, such as doodles, shape filtering, and relaxed search. Results from a qualitative evaluation show that Juxtapoze makes the process of creating image compositions enjoyable and supports creative expression and serendipity.",
        "keywords": [],
        "_type_counter": 33
    },
    {
        "type": "inproceedings",
        "id": "Razip2014",
        "title": "A Mobile Visual Analytics Approach for Law Enforcement Situation Awareness",
        "author": [
            "Ahmad M. M. Razip",
            "Shehzad Afzak",
            "Matthew Potrawski",
            "Ross Maciejewski",
            "Yun Jang",
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/iVALET/iVALET.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "booktitle": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "pages": "1235--1244",
        "abstract": "The advent of modern smartphones and handheld devices has given analysts, decision-makers, and even the general public the ability to rapidly ingest data and translate it into actionable information on-the-go. In this paper, we explore the design and use of a mobile visual analytics toolkit for public safety data that equips law enforcement agencies with effective situation awareness and risk assessment tools. Our system provides users with a suite of interactive tools that allow them to perform analysis and detect trends, patterns and anomalies among criminal, traffic and civil (CTC) incidents. The system also provides interactive risk assessment tools that allow users to identify regions of potential high risk and determine the risk at any user-specified location and time. Our system has been designed for the iPhone/iPad environment and is currently being used and evaluated by a consortium of law enforcement agencies. We report their use of the system and some initial feedback.",
        "keywords": [],
        "_type_counter": 32
    },
    {
        "type": "inproceedings",
        "id": "Zhao2014",
        "title": "skWiki: A Multimedia Sketching System for Collaborative Creativity",
        "author": [
            "Zhenpeng Zhao",
            "Sriram Karthik Badam",
            "Senthil Chandrasegaran",
            "Deo Gun Park",
            "Niklas Elmqvist",
            "Lorraine Kisselburgh",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/skwiki/skwiki.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=QxtTR14EXFQ",
        "year": 2014,
        "date": "2014-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "1235--1244",
        "abstract": "We present skWiki, a web application framework for collaborative creativity in digital multimedia projects, including text, hand-drawn sketches, and photographs. skWiki overcomes common drawbacks of existing wiki software by providing a rich viewer/editor architecture for all media types that is integrated into the web browser itself, thus avoiding dependence on client-side editors. Instead of files, skWiki uses the concept of paths as trajectories of persistent state over time. This model has intrinsic support for collaborative editing, including cloning, branching, and merging paths edited by multiple contributors. We demonstrate skWiki's utility using a qualitative, sketching-based user study.",
        "keywords": [],
        "_type_counter": 31
    },
    {
        "type": "article",
        "id": "Fisher2014",
        "title": "Designing Peer-to-Peer Distributed User Interfaces: Case Studies on Building Distributed Applications",
        "author": [
            "Eli Raymond Fisher",
            "Sriram Karthik Badam",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/dui-design/dui-design.pdf",
        "_url_type": "pdf",
        "year": 2014,
        "date": "2014-01-01T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Studies",
        "volume": 72,
        "number": 1,
        "pages": "100--110",
        "abstract": "Building a distributed user interface (DUI) application should ideally not require any additional effort beyond that necessary to build a non-distributed interface. In practice, however, DUI development is fraught with several technical challenges such as synchronization, resource management, and data transfer. In this paper, we present three case studies on building distributed user interface applications: a distributed media player for multiple displays and controls, a collaborative search system integrating a tabletop and mobile devices, and a multiplayer Tetris game for multi-surface use. While there exist several possible network architectures for such applications, our particular approach focuses on peer-to-peer (P2P) architectures. This focus leads to a number of challenges and opportunities. Drawing from these studies, we derive general challenges for P2P DUI development in terms of design, architecture, and implementation. We conclude with some general guidelines for practical DUI application development using peer-to-peer architectures.",
        "keywords": [],
        "_type_counter": 32
    },
    {
        "type": "article",
        "id": "MacNeil2013",
        "title": "Visualization Mosaics for Multivariate Visual Exploration",
        "author": [
            "Stephen MacNeil",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/mosaics/mosaics.pdf",
        "_url_type": "pdf",
        "year": 2013,
        "date": "2013-06-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 32,
        "number": 6,
        "pages": "38--50",
        "abstract": "We present a new model for creating composite visualizations of multidimensional datasets using simple visual representations such as point charts, scatterplots, and parallel coordinates as components. Each visual representation is contained in a tile, and the tiles are arranged in a mosaic of views using a space-filling slice-and-dice layout. Tiles can be created, resized, split, or merged using a versatile set of interaction techniques, and the visual representation of individual tiles can also be dynamically changed to another representation. Because each tile is self-contained and independent, it can be implemented in any programming language, on any platform, and using any visual representation. We also propose a formalism for expressing visualization mosaics. A web-based implementation called MosaicJS supporting multidimensional visual exploration showcases the versatility of the concept and illustrates how it can be used to integrate visualization components provided by different toolkits.",
        "keywords": [],
        "_type_counter": 31
    },
    {
        "type": "article",
        "id": "Elmqvist2013",
        "title": "Ubiquitous Analytics: Interacting with Big Data Anywhere, Anytime",
        "author": [
            "Niklas Elmqvist",
            "Pourang Irani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/ubilytics/ubilytics.pdf",
        "_url_type": "pdf",
        "year": 2013,
        "date": "2013-01-01T00:00:00.000Z",
        "journal": "IEEE Computer",
        "volume": 46,
        "number": 4,
        "pages": "86--89",
        "abstract": "With more than 4 billion mobile devices in the world today, mobile computing is quickly becoming the universal computational platform of the world. Building on this new wave of mobile devices are personal computing activities such as microblogging, social networking, and photo sharing, which are intrinsically mobile phenomena that occur while on-the-go. Mobility is now propagating to more professional activities such as data analytics, which need no longer be restricted to the workplace. In fact, the rise of big data increasingly demands that we be able to access data resources anytime and anywhere, whether to support decisions and activities for travel, telecommuting, or distributed teamwork. In other words, it is high time to fully realize Mark Weiser\u2019s vision of ubiquitous computing in the realm of data analytics.",
        "keywords": [],
        "_type_counter": 30
    },
    {
        "type": "article",
        "id": "Ghani2013",
        "title": "Visual Analytics for Multimodal Social Network Analysis: A Design Study with Social Scientists",
        "author": [
            "Sohaib Ghani",
            "Bumchul Kwon",
            "Seungyoon Lee",
            "Ji Soo Yi",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/mmgraph/mmgraph.pdf",
        "_url_type": "pdf",
        "year": 2013,
        "date": "2013-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 19,
        "number": 12,
        "pages": "2032--2041",
        "abstract": "Social network analysis (SNA) is becoming increasingly concerned not only with actors and their relations, but also with distinguishing between different types of such entities. For example, social scientists may want to investigate asymmetric relations in organizations with strict chains of command, or incorporate non-actors such as conferences and projects when analyzing co-authorship patterns. Multimodal social networks are those where actors and relations belong to different types, or modes, and multimodal social network analysis (mSNA) is accordingly SNA for such networks. In this paper, we present a design study that we conducted with several social scientist collaborators on how to support mSNA using visual analytics tools. Based on an open-ended, formative design process, we devised a visual representation called parallel node-link bands (PNLBs) that splits modes into separate bands and renders connections between adjacent ones, similar to the list view in Jigsaw. We then used the tool in a qualitative evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics. Finally, we conducted a second qualitative evaluation with our social scientist collaborators that provided further insights on the utility of the PNLBs representation and the potential of visual analytics for mSNA.",
        "keywords": [],
        "_type_counter": 29
    },
    {
        "type": "article",
        "id": "Javed2013b",
        "title": "Stack Zooming for Multi-Focus Interaction in Skewed-Aspect Visual Spaces",
        "author": [
            "Waqas Javed",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/stackzoom/stackzoom-journal.pdf",
        "_url_type": "pdf",
        "year": 2013,
        "date": "2013-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 19,
        "number": 8,
        "pages": "1362--1374",
        "abstract": "Many 2D visual spaces have a virtually one-dimensional nature with very high aspect ratio between the dimensions: examples include time-series data, multimedia data such as sound or video, text documents, and bipartite graphs. Common among these is that the space can become very large, e.g., temperature measurements could span a long time period, surveillance video could cover entire days or weeks, and documents can have thousands of pages. Many analysis tasks for such spaces require several foci while retaining context and distance awareness. In this extended version of our IEEE PacificVis 2010 paper, we introduce a method for supporting this kind of multi-focus interaction that we call stack zooming. The approach is based on building hierarchies of 1D strips stacked on top of each other, where each subsequent stack represents a higher zoom level, and sibling strips represent branches in the exploration. Correlation graphics show the relation between stacks and strips of different levels, providing context and distance awareness for the foci. The zoom hierarchies can also be used as graphical histories and for communicating insights to stakeholders, and can be further extended with annotation and integrated statistics.",
        "keywords": [],
        "_type_counter": 28
    },
    {
        "type": "article",
        "id": "Javed2013",
        "title": "ExPlates: Spatializing Interactive Analysis to Scaffold Visual Exploration",
        "author": [
            "Waqas Javed",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/explates/explates.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=UNhlhFUcDDo",
        "year": 2013,
        "date": "2013-01-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 32,
        "number": 2,
        "pages": "441--450",
        "abstract": "Visual exploration involves using visual representations to investigate data where the goals of the process are unclear and poorly defined. However, this often places unduly high cognitive load on the user, particularly in terms of keeping track of multiple investigative branches, remembering earlier results, and correlating between different views. We propose a new methodology for automatically spatializing the individual steps in visual exploration onto a large visual canvas, allowing users to easily recall, reflect, and assess their progress. We also present a web-based implementation of our methodology called ExPlatesJS where users can manipulate multidimensional data in their browsers, automatically building visual queries as they explore the data.",
        "keywords": [],
        "_type_counter": 27
    },
    {
        "type": "inproceedings",
        "id": "Javed2012c",
        "title": "GravNav: Using a Gravity Model for Multi-Scale Navigation",
        "author": [
            "Waqas Javed",
            "Sohaib Ghani",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/gravnav/gravnav.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Advanced Visual Interfaces",
        "booktitle": "Proceedings of the ACM Conference on Advanced Visual Interfaces",
        "pages": "217--224",
        "abstract": "We present gravity navigation (GravNav), a family of multi-scale navigation techniques that use a gravity-inspired model for assisting navigation in large visual 2D spaces based on the interest and salience of visual objects in the space. GravNav is an instance of topology-aware navigation, which makes use of the structure of the visual space to aid navigation. We have performed a controlled study comparing GravNav to standard zoom and pan navigation, with and without variable-rate zoom control. Our results show a significant improvement for GravNav over standard navigation, particularly when coupled with variable-rate zoom. We also report findings on user behavior in multi-scale navigation.",
        "keywords": [],
        "_type_counter": 30
    },
    {
        "type": "inproceedings",
        "id": "Javed2012b",
        "title": "PolyZoom: Multiscale and Multifocus Exploration in 2D Visual Spaces",
        "author": [
            "Waqas Javed",
            "Sohaib Ghani",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/polyzoom/polyzoom.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "287--296",
        "abstract": "The most common techniques for navigating in multiscale visual spaces are pan, zoom, and bird\u2019s eye views. However, these techniques are often tedious and cumbersome to use, especially when objects of interest are located far apart. We present the PolyZoom technique where users progressively build hierarchies of focus regions, stacked on each other such that each subsequent level shows a higher magnification. Correlation graphics show the relation between parent and child viewports in the hierarchy. To validate the new technique, we compare it to standard navigation techniques in two user studies, one on multiscale visual search and the other on multifocus interaction. Results show that PolyZoom performs better than current standard techniques. ",
        "keywords": [],
        "_type_counter": 29
    },
    {
        "type": "inproceedings",
        "id": "Javed2012a",
        "title": "Exploring the Design Space of Composite Visualization",
        "author": [
            "Waqas Javed",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/compvis/compvis.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "booktitle": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "pages": "1--8",
        "abstract": "We propose the notion of composite visualization views (CVVs) as a theoretical model that unifies the existing coordinated multiple views (CMV) paradigm with other strategies for combining visual representations in the same geometrical space. We identify five such strategies--called CVV design patterns--based on an extensive review of the literature in composite visualization. We go on to show how these design patterns can all be expressed in terms of a design space describing the correlation between two visualizations in terms of spatial mapping as well as the data relationships between items in the visualizations. We also discuss how to use this design space to suggest potential directions for future research.",
        "keywords": [],
        "_type_counter": 28
    },
    {
        "type": "inproceedings",
        "id": "Malik2012",
        "title": "A Correlative Analysis Process in a Visual Analytics Environment",
        "author": [
            "Abish Malik",
            "Ross Maciejewski",
            "Yun Jang",
            "Whitney Huang",
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "https://doi.org/10.1109/VAST.2012.6400491",
        "_url_type": "doi",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Conference on Visual Analytics Science and Technology",
        "booktitle": "Proceedings of the IEEE Conference on Visual Analytics Science and Technology",
        "pages": "33--42",
        "abstract": "Finding patterns and trends in spatial and temporal datasets has been a long studied problem in statistics and different domains of science. This paper presents a visual analytics approach for the interactive exploration and analysis of spatiotemporal correlations among multivariate datasets. Our approach enables users to discover correlations and explore potentially causal or predictive links at different spatiotemporal aggregation levels among the datasets, and allows them to understand the underlying statistical foundations that precede the analysis. Our technique utilizes the Pearson's product-moment correlation coefficient and factors in the lead or lag between different datasets to detect trends and periodic patterns amongst them.",
        "keywords": [],
        "_type_counter": 27
    },
    {
        "type": "inproceedings",
        "id": "McGrath2012",
        "title": "Branch-Explore-Merge: Facilitating Real-Time Revision Control in Collaborative Visual Exploration",
        "author": [
            "Will McGrath",
            "Brian Bowman",
            "David McCallum",
            "Juan-David Hincapie-Ramos",
            "Niklas Elmqvist",
            "Pourang Irani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/bem/bem.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "booktitle": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "pages": "235--244",
        "abstract": "Collaborative work is characterized by participants seamlessly transitioning from working together (coupled) to working alone (decoupled). Groupware should therefore facilitate smoothly varying coupling throughout the entire collaborative session. Towards achieving such transitions for collaborative exploration and search, we propose a protocol based on managing revisions for each collaborator exploring a dataset. The protocol allows participants to diverge from the shared analysis path (branch), study the data independently (explore), and then contribute back their findings onto the shared display (merge). We apply this concept to collaborative search in multidimensional data, and propose an implementation where the public view is a tabletop display and the private views are embedded in handheld tablets. We then use this implementation to perform a qualitative user study involving a real estate dataset. Results show that participants leverage the BEM protocol, spend significant time using their private views (40% to 80% of total task time), and apply public view changes for consultation with collaborators.",
        "keywords": [],
        "_type_counter": 26
    },
    {
        "type": "inproceedings",
        "id": "Murugappan2012",
        "title": "Extended Multitouch: Recovering Touch Posture and Differentiating Users using a Depth Camera",
        "author": [
            "Sundar Murugappan",
            "Vinayak",
            "Niklas Elmqvist",
            "Karthik Ramani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/emtouch/emtouch.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Symposium on User Interface Software and Technology",
        "booktitle": "Proceedings of the ACM Symposium on User Interface Software and Technology",
        "pages": "487--496",
        "abstract": "Multitouch surfaces are becoming prevalent, but most existing technologies are only capable of detecting the user\u2019s actual points of contact on the surface and not the identity, posture, and handedness of the user. In this paper, we define the concept of extended multitouch interaction as a richer input modality that includes all of this information. We further present a practical solution to achieve this on tabletop displays based on mounting a single commodity depth camera above a horizontal surface. This will enable us to not only detect when the surface is being touched, but also recover the user\u2019s exact finger and hand posture, as well as distinguish between different users and their handedness. We validate our approach using two user studies, and deploy the technique in a scratchpad tool and in a pen + touch sketch tool.",
        "keywords": [],
        "_type_counter": 25
    },
    {
        "type": "article",
        "id": "Afzal2012",
        "title": "Spatial Text Visualization Using Automatic Typographic Maps",
        "author": [
            "Shehzad Afzal",
            "Ross Maciejewski",
            "Yun Jang",
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/typomapvis/typomapvis.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "IEEE Computer Graphics and Applications (Proc. Vis/InfoVis 2012)",
        "volume": 18,
        "number": 12,
        "pages": "2556-2564",
        "abstract": "We present a method for automatically building typographic maps that merge text and spatial data into a visual representation where text alone forms the graphical features. We further show how to use this approach to visualize spatial data such as traffic density, crime rate, or demographic data. The technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints. Our sample implementation runs as a Web service, spatializing shape files from the OpenStreetMap project into typographic maps for any region.",
        "keywords": [],
        "_type_counter": 26
    },
    {
        "type": "article",
        "id": "Bowman2012",
        "title": "Toward Visualization for Games: Theory, Design Space, and Patterns",
        "author": [
            "Brian Bowman",
            "Niklas Elmqvist",
            "T.J. Jankun-Kelly"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/visgames/visgames.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 18,
        "number": 11,
        "pages": "1956-1968",
        "abstract": "Electronic games are starting to incorporate in-game telemetry that collects data about player, team, and community performance on a massive scale, and as data begins to accumulate, so does the demand for effectively analyzing this data. In this paper, we use examples from both old and new games of different genres to explore the theory and design space of visualization for games. Drawing on these examples, we define a design space for this novel research topic and use it to formulate design patterns for how to best apply visualization technology to games. We then discuss the implications that this new framework will potentially have on the design and development of game and visualization technology in the future.",
        "keywords": [],
        "_type_counter": 25
    },
    {
        "type": "article",
        "id": "Elmqvist2012",
        "title": "Leveraging Multidisciplinarity in a Visual Analytics Graduate Course",
        "author": [
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/va-education/va-education.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "IEEE Computer Graphics and Applications",
        "volume": 32,
        "number": 3,
        "pages": "84--87",
        "abstract": "There is a growing demand in engineering, business, science, research, and industry for members with visual analytics expertise, but teaching visual analytics is challenging due to the multidisciplinary nature of the topic matter, the diverse backgrounds of the members, and the corresponding requirements on the instructor. We report some best practices from our experience teaching several offerings of a visual analytics graduate course at Purdue University where we leveraged these multidisciplinary challenges to our advantage instead of attempting to mitigate them.",
        "keywords": [],
        "_type_counter": 24
    },
    {
        "type": "article",
        "id": "3Ghani2012",
        "title": "Perception of Animated Node-Link Diagrams for Dynamic Graphs",
        "author": [
            "Sohaib Ghani",
            "Niklas Elmqvist",
            "Ji Soo Yi"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/dyngraph/dyngraph.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 31,
        "number": 3,
        "pages": "1205--1214",
        "abstract": "Effective visualization of dynamic graphs remains an open research topic, and many state-of-the-art tools use animated node-link diagrams for this purpose. Despite its intuitiveness, the effectiveness of animation in node-link diagrams has been questioned, and several empirical studies have shown that animation is not necessarily superior to static visualizations. However, the exact mechanics of perceiving animated node-link diagrams are still unclear. In this paper, we study the impact of different dynamic graph metrics on user perception of the animation. After deriving candidate visual graph metrics, we perform an exploratory user study where participants are asked to reconstruct the event sequence in animated node-link diagrams. Based on these findings, we conduct a second user study where we investigate the most important visual metrics in depth. Our findings show that node speed and target separation are prominent visual metrics to predict the performance of event sequencing tasks.",
        "keywords": [],
        "_type_counter": 23
    },
    {
        "type": "article",
        "id": "Kim2012",
        "title": "Embodied Lenses for Collaborative Visual Queries on Tabletop Displays",
        "author": [
            "KyungTae Kim",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/emblens/emblens.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 11,
        "number": 4,
        "pages": "336--355",
        "abstract": "We introduce embodied lenses for visual queries on tabletop surfaces using physical interaction. The lenses are simply thin sheets of paper or transparent foil decorated with fiducial markers, allowing them to be tracked by a diffuse illumination tabletop display. The physical affordance of these embodied lenses allow them to be overlapped, causing composition in the underlying virtual space. We perform a formative evaluation to study users\u2019 conceptual models for overlapping physical lenses. This is followed by a quantitative user study comparing performance for embodied versus purely virtual lenses. Results show that embodied lenses are equally efficient compared to purely virtual lenses, and also support tactile and eyes-free interaction. We then present several examples of the technique, including image layers, map layers, image manipulation, and multidimensional data visualization. The technique is simple, cheap, and can be integrated into many existing tabletop displays.",
        "keywords": [],
        "_type_counter": 22
    },
    {
        "type": "article",
        "id": "Kwon2012",
        "title": "Evaluating the Role of Time in Investigative Analysis of Document Collections",
        "author": [
            "Bumchul Kwon",
            "Waqas Javed",
            "Sohaib Ghani",
            "Niklas Elmqvist",
            "Ji Soo Yi",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/time-analysis/time-analysis.pdf",
        "_url_type": "pdf",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 18,
        "number": 11,
        "pages": "199--2004",
        "abstract": "Time is a universal and essential aspect of data in any investigative analysis. It helps analysts establish causality, build storylines from evidence, and reject infeasible hypotheses. For this reason, many investigative analysis tools provide visual representations designed for making sense of temporal data. However, the field of visual analytics still needs more evidence explaining how temporal visualization actually aids the analysis process, as well as design recommendations for how to build these visualizations. To fill this gap, we conducted an insight-based qualitative study to investigate the influence of temporal visualization on investigative analysis. We found that visualizing temporal information helped participants externalize chains of events. Another contribution of our work is the lightweight evaluation approach used to collect, visualize, and analyze insight.",
        "keywords": [],
        "_type_counter": 21
    },
    {
        "type": "article",
        "id": "Madhavan2012",
        "title": "Portfolio Mining",
        "author": [
            "Krishna Madhavan",
            "Mihaela Vorvoreanu",
            "Niklas Elmqvist",
            "Aditya Johri",
            "Naren Ramakrishnan",
            "G. Alan Wang",
            "Ann McKenna"
        ],
        "url": "https://doi.org/10.1109/MC.2012.351",
        "_url_type": "doi",
        "year": 2012,
        "date": "2012-01-01T00:00:00.000Z",
        "journal": "IEEE Computer",
        "volume": 45,
        "number": 10,
        "pages": "95--99",
        "abstract": "Portfolio mining facilitates the creation of actionable knowledge, catalyzes innovations, and sustains research communities.",
        "keywords": [],
        "_type_counter": 20
    },
    {
        "type": "article",
        "id": "Elmqvist2011b",
        "title": "Color Lens: Adaptive Color Scale Optimization for Visual Exploration",
        "author": [
            "Niklas Elmqvist",
            "Pierre Dragicevic",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/colorlens/colorlens.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-06-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 17,
        "number": 6,
        "pages": "795-807",
        "abstract": "Visualization applications routinely map quantitative attributes to color using color scales. Although color is an effective visualization channel, it is limited by both display hardware and the human visual system. We propose a new interaction technique that overcomes these limitations by dynamically optimizing color scales based on a set of sampling lenses. The technique inspects the lens contents in data space, optimizes the initial color scale, and then renders the contents of the lens to the screen using the modified color scale. We present two prototype implementations of this pipeline and describe several case studies involving both information visualization and image inspection applications. We validate our approach with two mutually linked and complementary user studies comparing the Color Lens with explicit contrast control for visual search.",
        "keywords": [],
        "_type_counter": 19
    },
    {
        "type": "article",
        "id": "Ghani2011",
        "title": "Dynamic Insets for Context-Aware Graph Navigation",
        "author": [
            "Sohaib Ghani",
            "Nathalie Henry Riche",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/dyninsets/dyninsets.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-06-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 30,
        "number": 3,
        "pages": "861-870",
        "abstract": "Maintaining both overview and detail while navigating in graphs, such as road networks, airline route maps, or social networks, is difficult, especially when targets of interest are located far apart. We present a navigation technique called Dynamic Insets that provides context awareness for graph navigation. Dynamic insets utilize the topological structure of the network to draw a visual inset for off-screen nodes that shows a portion of the surrounding area for links leaving the edge of the screen. We implement dynamic insets for general graph navigation as well as geographical maps. We also present results from a set of user studies that show that our technique is more efficient than most of the existing techniques for graph navigation in different networks.",
        "keywords": [],
        "_type_counter": 18
    },
    {
        "type": "inproceedings",
        "id": "Dragicevic2011",
        "title": "Temporal Distortion for Animated Transitions",
        "author": [
            "Pierre Dragicevic",
            "Anastasia Bezerianos",
            "Waqas Javed",
            "Niklas Elmqvist",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/timedistort/timedistort.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "2009-2018",
        "abstract": "Animated transitions are popular in many visual applications but they can be difficult to follow, especially when many objects move at the same time. One informal design guideline for creating effective animated transitions has long been the use of slow-in/slow-out pacing, but no empirical data exist to support this practice. We remedy this by studying object tracking performance under different conditions of temporal distortion, i.e., constant speed transitions, slow-in/slow-out, fast-in/fast-out, and an adaptive technique that slows down the visually complex parts of the animation. Slow-in/slow-out outperformed other techniques, but we saw technique differences depending on the type of visual transition.",
        "keywords": [],
        "_type_counter": 24
    },
    {
        "type": "inproceedings",
        "id": "Ghani2011b",
        "title": "Improving Revisitation in Graphs through Static Spatial Features",
        "author": [
            "Sohaib Ghani",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/ssgf/ssgf.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "journal": "Proceedings of Graphics Interface",
        "booktitle": "Proceedings of Graphics Interface",
        "pages": "175-182",
        "abstract": "People generally remember locations in visual spaces with respect to spatial features and landmarks. Geographical maps provide many spatial features and hence are easy to remember. However, graphs are often visualized as node-link diagrams with few spatial features. We evaluate whether adding static spatial features to node-link diagrams will help in graph revisitation. We discuss three strategies for embellishing a graph and evaluate each in a user study. In our first study, we evaluate how to best add background features to a graph. In the second, we encode position using node size and color. In the third and final study, we take the best techniques from the first and second study, as well as shapes added to the graph as virtual landmarks, to find the best combination of spatial features for graph revisitation. We discuss the user study results and give our recommendations for design of graph visualization software.",
        "keywords": [],
        "_type_counter": 23
    },
    {
        "type": "inproceedings",
        "id": "Javed2011",
        "title": "Evaluating Physical/Virtual Occlusion Management Techniques for Horizontal Displays",
        "author": [
            "Waqas Javed",
            "KyungTae Kim",
            "Sohaib Ghani",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/occtable/occtable.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "journal": "Proceedings of INTERACT",
        "booktitle": "Proceedings of INTERACT",
        "pages": "391-408",
        "abstract": "We evaluate unguided and guided visual search performance for a set of techniques that mitigate occlusion between physical and virtual objects on a tabletop display. The techniques are derived from a general model of hybrid physical/virtual occlusion, and take increasingly drastic measures to make the user aware of, identify, and access hidden objects---but with increasingly space-consuming and disruptive impact on the display. Performance is different depending on the visual display, suggesting a tradeoff between management strength and visual space deformation.",
        "keywords": [],
        "_type_counter": 22
    },
    {
        "type": "inproceedings",
        "id": "Kim2011",
        "title": "WordBridge: Using Composite Tag Clouds in Node-Link Diagrams for Visualizing Content and Relations in Text Corpora",
        "author": [
            "KyungTae Kim",
            "Sungahn Ko",
            "Niklas Elmqvist",
            "David S. Ebert"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/wordbridge/wordbridge.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "journal": "Proceedings of the Hawaii International Conference on System Sciences",
        "booktitle": "Proceedings of the Hawaii International Conference on System Sciences",
        "pages": ".",
        "abstract": "We introduce WordBridge, a novel graph-based visualization technique for showing relationships between entities in text corpora. The technique is a node-link visualization where both nodes and links are tag clouds. Using these tag clouds, WordBridge can reveal relationships by representing not only entities and their connections, but also the nature of their relationship using representative keywords for nodes and edges. In this paper, we apply the technique to an interactive web-based visual analytics environment---Apropos---where a user can explore a text corpus using WordBridge. We validate the technique using several case studies based on document collections such as intelligence reports, co-authorship networks, and works of fiction.",
        "keywords": [],
        "_type_counter": 21
    },
    {
        "type": "inproceedings",
        "id": "Ko2011",
        "title": "Applying Mobile Device Soft Keyboards to Collaborative Multitouch Tabletop Displays: Design and Evaluation",
        "author": [
            "Sungahn Ko",
            "KyungTae Kim",
            "Tejas Kulkarni",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/table-text/table-text.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "booktitle": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "pages": "130-139",
        "abstract": "We present an evaluation of text entry methods for tabletop displays given small display space allocations, an increasingly important design constraint as tabletops become collaborative platforms. Small space is already a requirement of mobile text entry methods, and these can often be easily ported to tabletop settings. The purpose of this work is to determine whether these mobile text entry methods are equally useful for tabletop displays, or whether there are unique aspects of text entry on large, horizontal surfaces that influence design. Our evaluation consists of two studies designed to elicit differences between the mobile and tabletop domains. Results show that standard soft keyboards perform best, even at small space allocations. Furthermore, occlusion-reduction methods like Shift do not yield significant improvements to text entry; we speculate that this is due to the low ratio of resolution per surface units (i.e., DPI) for current tabletops.",
        "keywords": [],
        "_type_counter": 20
    },
    {
        "type": "inproceedings",
        "id": "Kwon2011",
        "title": "Direct Manipulation Through Surrogate Objects",
        "author": [
            "Bumchul Kwon",
            "Waqas Javed",
            "Niklas Elmqvist",
            "Ji Soo Yi"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/surrogate/surrogate.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "627-636",
        "abstract": "Direct manipulation has had major influence on interface design since it was proposed by Shneiderman in 1982. Although directness generally benefits users, direct manipulation also has weaknesses. In some cases, such as when a user needs to manipulate small, attribute-rich objects or multiple objects simultaneously, indirect manipulation may be more efficient at the cost of directness or intuitiveness of the interaction. Several techniques have been developed over the years to address these issues, but these are all isolated and limited efforts with no coherent underlying principle. We propose the notion of Surrogate Interaction that ties together a large subset of these techniques through the use of a surrogate object that allow users to interact with the surrogate instead of the domain object. We believe that formalizing this family of interaction techniques will provide an additional and powerful interface design alternative for interaction designers, as well as uncover opportunities for future research. ",
        "keywords": [],
        "_type_counter": 19
    },
    {
        "type": "article",
        "id": "Elmqvist2011",
        "title": "Fluid Interaction for Information Visualization",
        "author": [
            "Niklas Elmqvist",
            "Andrew Vande Moere",
            "Hans-Christian Jetter",
            "Daniel Cernea",
            "Harald Reiterer",
            "T.J. Jankun-Kelly"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/fluidity/fluidity.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 10,
        "number": 4,
        "pages": "327-340",
        "abstract": "Despite typically receiving little emphasis in visualization research, interaction in visualization is the catalyst for the user's dialogue with the data, and, ultimately, the user\u2019s actual understanding and insight into this data. There are many possible reasons for this skewed balance between the visual and interactive aspects of a visualization. One reason is that interaction is an intangible concept that is difficult to design, quantify, and evaluate. Unlike for visual design, there are few examples that show visualization practitioners and researchers how to best design the interaction for a new visualization. In this paper, we attempt to address this issue by collecting examples of visualizations with \"best-in-class\" interaction and using them to extract practical design guidelines for future designers and researchers. We call this concept fluid interaction, and we propose an operational definition in terms of the direct manipulation and embodied interaction paradigms, the psychological concept of \"flow\", and Norman\u2019s gulfs of execution and evaluation.",
        "keywords": [],
        "_type_counter": 17
    },
    {
        "type": "article",
        "id": "Isenberg2011",
        "title": "Collaborative Visualization: Definition, Challenges, and Research Agenda",
        "author": [
            "Petra Isenberg",
            "Niklas Elmqvist",
            "Daniel Cernea",
            "Jean Scholtz",
            "Kwan-Liu Ma",
            "Hans Hagen"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/collabvis/collabvis.pdf",
        "_url_type": "pdf",
        "year": 2011,
        "date": "2011-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 10,
        "number": 4,
        "pages": "310-326",
        "abstract": "The conflux of two growing areas of technology---collaboration and visualization---into a new research direction, collaborative visualization, provides new research challenges. Technology now allows us to easily connect and collaborate with one another---in settings as diverse as over networked computers, across mobile devices, or using shared displays such as interactive walls and tabletop surfaces. Digital information is now regularly accessed by multiple people in order to share information, to view it together, to analyze it, or to form decisions. Visualizations are used to deal more effectively with large amounts of information while interactive visualizations allow users to explore the underlying data. While researchers face many challenges in collaboration and in visualization, the emergence of collaborative visualization poses additional challenges but is also an exciting opportunity to reach new audiences and applications for visualization tools and techniques.",
        "keywords": [],
        "_type_counter": 16
    },
    {
        "type": "inproceedings",
        "id": "Javed2010",
        "title": "Stack Zooming for Multi-Focus Interaction in Time-Series Data Visualization",
        "author": [
            "Waqas Javed",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/stackzoom/stackzoom.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=dK0De4XPm5Y",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "booktitle": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "pages": "33--40",
        "abstract": "Information visualization shows tremendous potential for helping both expert and casual users alike make sense of temporal data, but current time series visualization tools provide poor support for comparing several foci in a temporal dataset while retaining context and distance awareness. We introduce a method for supporting this kind of multi-focus interaction that we call stack zooming. The approach is based on the user interactively building hierarchies of 1D strips stacked on top of each other, where each subsequent stack represents a higher zoom level, and sibling strips represent branches in the visual exploration. Correlation graphics show the relation between stacks and strips of different levels, providing context and distance awareness among the focus points. The zoom hierarchies can also be used as graphical histories and for communicating insights to stakeholders. We also discuss how visual spaces that support stack zooming can be extended with annotation and local statistics computations that fit the hierarchical stacking metaphor.",
        "keywords": [],
        "_type_counter": 18
    },
    {
        "type": "inproceedings",
        "id": "Kim2010",
        "title": "Hugin: A Framework Awareness and Coordination in Mixed-Presence Collaborative Information Visualization",
        "author": [
            "KyungTae Kim",
            "Waqas Javed",
            "Cary Williams",
            "Niklas Elmqvist",
            "Pourang Irani"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/hugin/hugin.pdf",
        "_url_type": "pdf",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "booktitle": "Proceedings of the ACM Conference on Interactive Tabletops and Surfaces",
        "pages": "231--240",
        "abstract": "Analysts are increasingly encountering datasets that are larger and more complex than ever before. Effectively exploring such datasets requires collaboration between multiple analysts, who more often than not are distributed in time or in space. Mixed-presence groupware provide a shared workspace medium that supports this combination of co-located and distributed collaboration. However, collaborative visualization systems for such distributed settings have their own cost and are still uncommon in the visualization community. We present Hugin, a novel layer-based graphical framework for this kind of mixed-presence synchronous collaborative visualization over digital tabletop displays. The design of the framework focuses on issues like awareness and access control, while using information visualization for the collaborative data exploration on network-connected tabletops. To validate the usefulness of the framework, we also present examples of how Hugin can be used to implement new visualizations supporting these collaborative mechanisms.",
        "keywords": [],
        "_type_counter": 17
    },
    {
        "type": "article",
        "id": "Bezerianos2010",
        "title": "GraphDice: A System for Exploring Multivariate Social Networks",
        "author": [
            "Anastasia Bezerianos",
            "Fanny Chevalier",
            "Pierre Dragicevic",
            "Niklas Elmqvist",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/graphdice/graphdice.pdf",
        "_url_type": "pdf",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "Computer Graphics Forum",
        "volume": 29,
        "number": 3,
        "pages": "863--872",
        "abstract": "Social networks collected by historians or sociologists typically have a large number of actors and edge attributes. Applying social network analysis (SNA) algorithms to these networks produces additional attributes such as degree, centrality, and clustering coefficients. Understanding the effects of this plethora of attributes is one of the main challenges of multivariate SNA. We present the design of GraphDice, a multivariate network visualization system for exploring the attribute space of edges and actors. GraphDice builds upon the ScatterDice system for its main multidimensional navigation paradigm, and extends it with novel mechanisms to support network exploration in general and SNA tasks in particular. Novel mechanisms include visualization of attributes of interval type and projection of numerical edge attributes to node attributes. We show how these extensions to the original ScatterDice system allow to support complex visual analysis tasks on networks with hundreds of actors and up to 30 attributes, while providing a simple and consistent interface for interacting with network data.",
        "keywords": [],
        "_type_counter": 15
    },
    {
        "type": "article",
        "id": "Elmqvist2010a",
        "title": "M\u00e9lange: Space Folding for Visual Exploration",
        "author": [
            "Niklas Elmqvist",
            "Nathalie Henry Riche",
            "Yann Riche",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/melange/melange-journal.pdf",
        "_url_type": "pdf",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 16,
        "number": 3,
        "pages": "468--483",
        "abstract": "Navigating in large geometric spaces---such as maps, social networks, or long documents---typically require a sequence of pan and zoom actions. However, this strategy is often ineffective and cumbersome, especially when trying to study and compare several distant objects. We propose a new distortion technique that folds the intervening space to guarantee visibility of multiple focus regions. The folds themselves show contextual information and support unfolding and paging interactions. We conducted a study comparing the space-folding technique to existing approaches, and found that participants performed significantly better with the new technique. We also describe how to implement this distortion technique, and give an in-depth case study on how to apply it to the visualization of large-scale 1D time-series data.",
        "keywords": [],
        "_type_counter": 14
    },
    {
        "type": "article",
        "id": "Elmqvist2010b",
        "title": "Hierarchical Aggregation for Information Visualization: Overview, Techniques and Design Guidelines",
        "author": [
            "Niklas Elmqvist",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/hieragg/hieragg.pdf",
        "_url_type": "pdf",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 16,
        "number": 3,
        "pages": "439--454",
        "abstract": "We present a model for building, visualizing, and interacting with multiscale representations of information visualization techniques using hierarchical aggregation. The motivation for this work is to make visual representations more visually scalable and less cluttered. The model allows for augmenting existing techniques with multiscale functionality, as well as for designing new visualization and interaction techniques that conform to this new class of visual representations. We give some examples of how to use the model for standard information visualization techniques such as scatterplots, parallel coordinates, and node-link diagrams, and discuss existing techniques that are based on hierarchical aggregation. This yields a set of design guidelines for aggregated visualizations. We also present a basic vocabulary of interaction techniques suitable for navigating these multiscale visualizations.",
        "keywords": [],
        "_type_counter": 13
    },
    {
        "type": "article",
        "id": "Javed2010b",
        "title": "Graphical Perception of Multiple Time Series",
        "author": [
            "Waqas Javed",
            "Bryan McDonnel",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/multilinevis/multilinevis.pdf",
        "_url_type": "pdf",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics (Proc. IEEE InfoVis 2010)",
        "volume": 16,
        "number": 6,
        "pages": "927--934",
        "abstract": "Line graphs have been the visualization of choice for temporal data ever since the days of William Playfair (1759\u20131823), but realistic temporal analysis tasks often include multiple simultaneous time series. In this work, we explore user performance for comparison, slope, and discrimination tasks for different line graph techniques involving multiple time series. Our results show that techniques that create separate charts for each time series\u2014such as small multiples and horizon graphs---are generally more efficient for comparisons across time series with a large visual span. On the other hand, shared-space techniques---like standard line graphs---are typically more efficient for comparisons over smaller visual spans where the impact of overlap and clutter is reduced.",
        "keywords": [],
        "_type_counter": 12
    },
    {
        "type": "article",
        "id": "Yi2010",
        "title": "TimeMatrix: Visualizing Temporal Social Networks Using Interactive Matrix-Based Visualizations",
        "author": [
            "Ji Soo Yi",
            "Niklas Elmqvist",
            "Seungyoon Lee"
        ],
        "url": "https://doi.org/10.1080/10447318.2010.516722",
        "_url_type": "doi",
        "video": "https://www.youtube.com/watch?v=PjJOPX_ezzc",
        "year": 2010,
        "date": "2010-01-01T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Interaction",
        "volume": 26,
        "number": "11-12",
        "pages": "1031--1051",
        "abstract": "Visualization plays a crucial role in understanding dynamic social networks at many different",
        "keywords": [],
        "_type_counter": 11
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2009a",
        "title": "Motion-Pointing: Target Selection using Elliptical Motions",
        "author": [
            "Jean-Daniel Fekete",
            "Niklas Elmqvist",
            "Yves Guiard"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/motionpointing/motionpointing.pdf",
        "_url_type": "pdf",
        "year": 2009,
        "date": "2009-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "289--298",
        "abstract": "We present a novel method called motion-pointing for selecting a set of visual items, such as push-buttons or radio-buttons, without actually pointing to them. Instead, each potential target displays an animated point we call the driver. To select a specific item, the user only has to imitate the motion of its driver using the input device. Once the motion has been recognized by the system, the user can confirm the selection to trigger the action. We consider cyclic motions on an elliptic trajectory with a specific period, and study the most effective methods for real-time matching such a trajectory, as well as the range of parameters a human can reliably reproduce. We then show how to implement motion-pointing in real applications using an interaction technique we call move-and-stroke. Finally, we measure the input throughput and error rate of move-and-stroke in a controlled experiment. We show that the selection time is linearly proportional to the number of input bits conveyed up to 6 bits, confirming that motion-pointing is a practical input method.",
        "keywords": [],
        "_type_counter": 16
    },
    {
        "type": "article",
        "id": "Elmqvist2009b",
        "title": "Dynamic Transparency for 3D Visualization: Design and Evaluation",
        "author": [
            "Niklas Elmqvist",
            "Ulf Assarsson",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/dyntrans/dyntrans-journal.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=77N5KVbbEmQ",
        "year": 2009,
        "date": "2009-01-01T00:00:00.000Z",
        "journal": "International Journal of Virtual Reality",
        "volume": 8,
        "number": 1,
        "pages": "65--78",
        "abstract": "Recent developments in occlusion management for 3D environments often involve the use of dynamic transparency, or \"virtual X-ray vision\", to promote target discovery and access in complex 3D worlds. However, there are many different approaches to achieving this effect and their actual utility for the user has yet to be evaluated. Furthermore, the introduction of semitransparent surfaces adds additional visual complexity that may actually have a negative impact on task performance. In this paper, we report on an empirical user study investigating these human aspects of dynamic transparency. Our implementation of the technique is an image-space algorithm built using modern programmable shaders to achieve real-time performance and visually pleasing results. Results from the user study indicate that dynamic transparency provides superior performance for perceptual tasks in terms of both efficiency and correctness. Subjective ratings are also firmly in favor of the method.",
        "keywords": [],
        "_type_counter": 10
    },
    {
        "type": "article",
        "id": "McDonnel2009",
        "title": "Towards Utilizing GPUs in Information Visualization: A Model and Implementation of Image-Space Operations",
        "author": [
            "Bryan McDonnel",
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/gpuvis/gpuvis.pdf",
        "_url_type": "pdf",
        "year": 2009,
        "date": "2009-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 15,
        "number": 6,
        "pages": "1105--1112",
        "abstract": "Modern programmable GPUs represent a vast potential in terms of performance and visual flexibility for information visualization research, but surprisingly few applications even begin to utilize this potential. In this paper, we conjecture that this may be due to the mismatch between the high-level abstract data types commonly visualized in our field, and the low-level floating-point model supported by current GPU shader languages. To help remedy this situation, we present a refinement of the traditional information visualization pipeline that is amenable to implementation using GPU shaders. The refinement consists of a final image-space step in the pipeline where the multivariate data of the visualization is sampled in the resolution of the current view. To concretize the theoretical aspects of this work, we also present a visual programming environment for constructing visualization shaders using a simple drag-and-drop interface. Finally, we give some examples of the use of shaders for well-known visualization techniques.",
        "keywords": [],
        "_type_counter": 9
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2008f",
        "title": "Semantic Pointing for Object Picking in Complex 3D Environments",
        "author": [
            "Niklas Elmqvist",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/sempoint3d/sempoint3d.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=Ebv7QG0Z6lM",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "journal": "Proceedings of Graphics Interface",
        "booktitle": "Proceedings of Graphics Interface",
        "pages": "243--250",
        "abstract": "Today's large and high-resolution displays coupled with powerful graphics hardware offer the potential for highly realistic 3D virtual environments, but also cause increased target acquisition difficulty for users interacting with these environments. We present an adaptation of semantic pointing to object picking in 3D environments. Essentially, semantic picking shrinks empty space and expands potential targets on the screen by dynamically adjusting the ratio between movement in visual space and motor space for relative input devices such as the mouse. Our implementation operates in the image-space using a hierarchical representation of the standard stencil buffer to allow for real-time calculation of the closest targets for all positions on the screen. An informal user study indicates that subjects perform more accurate pointing with semantic 3D pointing than without.",
        "keywords": [],
        "_type_counter": 15
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2008d",
        "title": "M\u00e9lange: Space Folding for Multi-Focus Interaction",
        "author": [
            "Niklas Elmqvist",
            "Nathalie Henry Riche",
            "Yann Riche",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/melange/melange.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=I1KiO1iZ1DI",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "1333--1342",
        "abstract": "Interaction and navigation in large geometric spaces typically require a sequence of pan and zoom actions. This strategy is often ineffective and cumbersome, especially when trying to study several distant objects. We propose a new distortion technique that folds the intervening space to guarantee visibility of multiple focus regions. The folds themselves show contextual information and support unfolding and paging interactions. Compared to previous work, our method provides more context and distance awareness. We conducted a study comparing the space-folding technique to existing approaches, and found that participants performed significantly better with the new technique.",
        "keywords": [],
        "_type_counter": 14
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2008c",
        "title": "Evaluating Motion Constraints for 3D Wayfinding in Immersive and Desktop Virtual Environments",
        "author": [
            "Niklas Elmqvist",
            "Mihail Eduard Tudoreanu",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/motcon/motcon.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=LRVTyoeuhpo",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "booktitle": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "journal": "Proceedings of the ACM Conference on Human Factors in Computing Systems",
        "pages": "1769--1778",
        "abstract": "Motion constraints providing guidance for 3D navigation have recently been suggested as a way of offloading some of the cognitive effort of traversing complex 3D environments on a computer. We present findings from an evaluation of the benefits of this practice where users achieved significantly better results in memory recall and performance when given access to such a guidance method. The study was conducted on both standard desktop computers with mouse and keyboard, as well as on an immersive CAVE system. Interestingly, our results also show that the improvements were more dramatic for desktop users than for CAVE users, even outperforming the latter. Furthermore, the study indicates that allowing the users to retain local control over the navigation on the desktop platform helps them in familiarizing themselves with the 3D world.",
        "keywords": [],
        "_type_counter": 13
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2008b",
        "title": "ZAME: Interactive Large-Scale Graph Visualization",
        "author": [
            "Niklas Elmqvist",
            "Thanh-Nghi Do",
            "Howard Goodell",
            "Nathalie Henry Riche",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/zame/zame.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=Zr25Lt_pmfw",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "booktitle": "Proceedings of the IEEE Pacific Symposium on Visualization",
        "pages": "215--222",
        "abstract": "We present the Zoomable Adjacency Matrix Explorer (ZAME), a visualization tool for exploring graphs at a scale of millions of nodes and edges. ZAME is based on an adjacency matrix graph representation aggregated at multiple scales. It allows analysts to explore a graph at many levels, zooming and panning with interactive performance from an overview to the most detailed views. Several components work together in the ZAME tool to make this possible. Efficient matrix ordering algorithms group related elements. Individual data cases are aggregated into higher-order meta representations. Aggregates are arranged into a pyramid hierarchy that allows for on-demand paging to GPU shader programs to support smooth multiscale browsing. Using ZAME, we are able to explore the entire French Wikipedia---over 500,000 articles and 6,000,000 links---with interactive performance on standard consumer-level computer hardware.",
        "keywords": [],
        "_type_counter": 12
    },
    {
        "type": "article",
        "id": "Elmqvist2008g",
        "title": "Rolling the Dice: Multidimensional Visual Exploration using Scatterplot Matrix Navigation",
        "author": [
            "Niklas Elmqvist",
            "Pierre Dragicevic",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/scatterdice/scatterdice.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=E1birsp9iYk",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 14,
        "number": 6,
        "pages": "1141--1148",
        "abstract": "Scatterplots remain one of the most popular and widely-used visual representations for multidimensional data due to their simplicity, familiarity and visual clarity, even if they lack some of the flexibility and visual expressiveness of newer multidimensional visualization techniques. This paper presents new interactive methods to explore multidimensional data using scatterplots. This exploration is performed using a matrix of scatterplots that gives an overview of the possible configurations, thumbnails of the scatterplots, and support for interactive navigation in the multidimensional space. Transitions between scatterplots are performed as animated rotations in 3D space, somewhat akin to rolling dice. Users can iteratively build queries using bounding volumes in the dataset, sculpting the query from different viewpoints to become more and more refined. Furthermore, the dimensions in the navigation space can be reordered, manually or automatically, to highlight salient correlations and differences among them. An example scenario presents the interaction techniques supporting smooth and effortless visual exploration of multidimensional datasets.",
        "keywords": [],
        "_type_counter": 8
    },
    {
        "type": "article",
        "id": "Elmqvist2008e",
        "title": "A Taxonomy of 3D Occlusion Management for Visualization",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/occmgt/occmgt-journal.pdf",
        "_url_type": "pdf",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "journal": "IEEE Transactions on Visualization and Computer Graphics",
        "volume": 14,
        "number": 5,
        "pages": "1095--1109",
        "abstract": "While an important factor in depth perception, the occlusion effect in 3D environments also has a detrimental impact on tasks involving discovery, access, and spatial relation of objects in a 3D visualization. A number of interactive techniques have been developed in recent years to directly or indirectly deal with this problem using a wide range of different approaches. In this paper, we build on previous work on mapping out the problem space of 3D occlusion by defining a taxonomy of the design space of occlusion management techniques in an effort to formalize a common terminology and theoretical framework for this class of interactions. We classify a total of 50 different techniques for occlusion management using our taxonomy and then go on to analyze the results, deriving a set of five orthogonal design patterns for effective reduction of 3D occlusion. We also discuss the \"gaps\" in the design space, areas of the taxonomy not yet populated with existing techniques, and use these to suggest future research directions into occlusion management.",
        "keywords": [],
        "_type_counter": 7
    },
    {
        "type": "article",
        "id": "Elmqvist2008a",
        "title": "DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data",
        "author": [
            "Niklas Elmqvist",
            "John Stasko",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/datameadow/datameadow-journal.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=FO2MsmtWX_4",
        "year": 2008,
        "date": "2008-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 7,
        "number": 1,
        "pages": "18--33",
        "abstract": "Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to stakeholders. A powerful direct manipulation interface allows for selection, filtering, and creation of sets, subsets, and data dependencies. We have evaluated our system using a qualitative expert review involving two visualization researchers. Results from this review are favorable for the new method.",
        "keywords": [],
        "_type_counter": 6
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2007i",
        "title": "Tour Generation for Exploration of 3D Virtual Environments",
        "author": [
            "Niklas Elmqvist",
            "Mihail Eduard Tudoreanu",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/tourgen/tourgen.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=LRVTyoeuhpo",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Symposium on Virtual Reality Software and Technology",
        "booktitle": "Proceedings of the ACM Symposium on Virtual Reality Software and Technology",
        "pages": "207--210",
        "abstract": "Navigation in complex and large-scale 3D virtual environments has been shown to be a difficult task, imposing a high cognitive load on the user. In this paper, we present a comprehensive method for assisting users in exploring and understanding such 3D worlds. The method consists of two distinct phases: an off-line computation step deriving a grand tour using the world geometry and any semantic target information as input, and an on-line interactive navigation step providing guided exploration and improved spatial perception",
        "keywords": [],
        "_type_counter": 11
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2007h",
        "title": "DataMeadow: A Visual Canvas for Analysis of Large-Scale Multivariate Data",
        "author": [
            "Niklas Elmqvist",
            "John Stasko",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/datameadow/datameadow.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=FO2MsmtWX_4",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Symposium on Visual Analytics Science and Technology",
        "booktitle": "Proceedings of the IEEE Symposium on Visual Analytics Science and Technology",
        "pages": "187--194",
        "abstract": "Supporting visual analytics of multiple large-scale multidimensional datasets requires a high degree of interactivity and user control beyond the conventional challenges of visualizing such datasets. We present the DataMeadow, a visual canvas providing rich interaction for constructing visual queries using graphical set representations called DataRoses. A DataRose is essentially a starplot of selected columns in a dataset displayed as multivariate visualizations with dynamic query sliders integrated into each axis. The purpose of the DataMeadow is to allow users to create advanced visual queries by iteratively selecting and filtering into the multidimensional data. Furthermore, the canvas provides a clear history of the analysis that can be annotated to facilitate dissemination of analytical results to outsiders. Towards this end, the DataMeadow has a direct manipulation interface for selection, filtering, and creation of sets, subsets, and data dependencies using both simple and complex mouse gestures. We have evaluated our system using a qualitative expert review involving two researchers working in the area. Results from this review are favorable for our new method.",
        "keywords": [],
        "_type_counter": 10
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2007b",
        "title": "Employing Dynamic Transparency for 3D Occlusion Management: Design Issues and Evaluation",
        "author": [
            "Niklas Elmqvist",
            "Ulf Assarsson",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/dyntrans/dyntrans.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=77N5KVbbEmQ",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Proceedings of INTERACT",
        "booktitle": "Proceedings of INTERACT",
        "pages": "532--545",
        "abstract": "Recent developments in occlusion management for 3D environments often involve the use of dynamic transparency, or virtual \"X-ray vision\", to promote target discovery and access in complex 3D worlds. However, there are many different approaches to achieving this effect and their actual utility for the user has yet to be evaluated. Furthermore, the introduction of semi-transparent surfaces adds additional visual complexity that may actually have a negative impact on task performance. In this paper, we report on an empirical user study comparing dynamic transparency to standard viewpoint controls. Our implementation of the technique is an image-space algorithm built using modern programmable shaders to achieve real-time performance and visually pleasing results. Results from the user study indicate that dynamic transparency is superior for perceptual tasks in terms of both efficiency and correctness.",
        "keywords": [],
        "_type_counter": 9
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2007e",
        "title": "TrustNeighborhoods: Visualizing Trust in Distributed File Systems",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/trustvis/trustvis.pdf",
        "_url_type": "pdf",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Proceedings of the Eurographics/IEEE VGTC Symposium on Visualization",
        "booktitle": "Proceedings of the Eurographics/IEEE VGTC Symposium on Visualization",
        "pages": "107--114",
        "abstract": "We present TrustNeighborhoods, a security trust visualization for situational awareness on the Internet aimed at novice and intermediate users of a distributed file sharing system. The TrustNeighborhoods technique uses the metaphor of a multi-layered city or fortress to intuitively represent trust as a simple geographic relation. The visualization uses a radial space-filling layout; there is a 2D mode for editing and configuration, as well as a 3D mode for exploration and overview. In addition, the 3D mode supports a simple animated \"fly-to\" command that is intended to show the user the context and trust of a particular document by zooming in on the document and its immediate neighborhood in the 3D city. The visualization is intended for integration into an existing desktop environment, connecting",
        "keywords": [],
        "_type_counter": 8
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2007f",
        "title": "A Taxonomy of 3D Occlusion Management Techniques",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/occmgt/occmgt.pdf",
        "_url_type": "pdf",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Conference on Virtual Reality",
        "booktitle": "Proceedings of the IEEE Conference on Virtual Reality",
        "pages": "51--58",
        "abstract": "While an important factor in depth perception, the occlusion effect in 3D environments also has a detrimental impact on tasks involving discovery, access, and spatial relation of objects in a 3D visualization. A number of interactive techniques have been developed in recent years to directly or indirectly deal with this problem using a wide range of different approaches. In this paper, we build on previous work on mapping out the problem space of 3D occlusion by defining a taxonomy of the design space of occlusion management techniques in an effort to formalize a common terminology and theoretical framework for this class of interactions. We classify a total of 25 different techniques for occlusion management using our taxonomy and then go on to analyze the results, deriving a set of five orthogonal design patterns for effective reduction of 3D occlusion. We also discuss the \"gaps\" in the design space, areas of the taxonomy not yet populated with existing techniques, and use these to suggest future research directions into occlusion management.",
        "keywords": [],
        "_type_counter": 7
    },
    {
        "type": "article",
        "id": "Elmqvist2007j",
        "title": "View-Projection Animation for 3D Occlusion Management",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/pmorph/pmorph-journal.pdf",
        "_url_type": "pdf",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Computers & Graphics",
        "volume": 31,
        "number": 6,
        "pages": "864--876",
        "abstract": "Inter-object occlusion is inherent to 3D environments and is one of the challenges of using 3D instead of 2D computer graphics for visualization. Based on an analysis of this effect, we present an interaction technique for view-projection animation that reduces inter-object occlusion in 3D environments without modifying the geometrical properties of the objects themselves. The technique allows for smooth on-demand animation between parallel and perspective projection modes as well as online manipulation of view parameters, enabling the user to quickly and easily adapt the view to reduce occlusion. A user study indicates that the technique provides many of the occlusion reduction benefits of traditional camera movement, but without the need to actually change the viewpoint. We have also implemented a prototype of the technique in the Blender 3D modeler.",
        "keywords": [],
        "_type_counter": 5
    },
    {
        "type": "article",
        "id": "Elmqvist2007c",
        "title": "CiteWiz: A Tool for the Visualization of Scientific Citation Networks",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/citewiz/citewiz.pdf",
        "_url_type": "pdf",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 6,
        "number": 3,
        "pages": "215--232",
        "abstract": "We present CiteWiz, an extensible framework for visualization of scientific citation networks. The system is based on a taxonomy of citation database usage for researchers, and provides a timeline visualization for overviews and an influence visualization for detailed views. The timeline displays the general chronology and importance of authors and articles in a citation database, whereas the influence visualization is implemented using the Growing Polygons technique, suitably modified to the context of browsing citation data. Using the latter technique, hierarchies of articles with potentially very long citation chains can be graphically represented. The visualization is augmented with mechanisms for parent-child visualization and suitable interaction techniques for interacting with the view hierarchy and the individual articles in the dataset. We also provide an interactive concept map for keywords and co-authorship using a basic force-directed graph layout scheme. A formal user study indicates that CiteWiz is significantly more efficient than traditional database interfaces for high-level analysis tasks relating to influence and overviews, and equally efficient for low-level tasks such as finding a paper and correlating bibliographical data.",
        "keywords": [],
        "_type_counter": 4
    },
    {
        "type": "article",
        "id": "Elmqvist2007d",
        "title": "Occlusion Management in Immersive and Desktop 3D Virtual Environments: Theory and Evaluation",
        "author": [
            "Niklas Elmqvist",
            "Mihail Eduard Tudoreanu"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/balloonprobe/balloonprobe-journal.pdf",
        "_url_type": "pdf",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "International Journal of Virtual Reality",
        "volume": 6,
        "number": 2,
        "pages": "21--32",
        "abstract": "We present an empirical usability experiment studying the relative strengths and weaknesses of three different occlusion management techniques for discovering and accessing objects in information-rich 3D virtual environments. More specifically, the study compares standard 3D navigation, generalized fisheye techniques using object scaling and transparency, and the BalloonProbe interactive 3D space distortion technique. Subjects are asked to complete a number of representative tasks, including counting, pattern recognition, and object relation, in different kinds of environments and on both immersive and desktop-based VR systems. The environments include a free-space abstract 3D environment and a virtual 3D walkthrough application for a simple building floor. Our results confirm the general guideline that each task calls for a specialized interaction---no single technique performed best across all tasks and worlds. The results also indicate a clear trade-off between speed and accuracy: simple navigation was the fastest but also most error-prone technique, whereas spherical BalloonProbe and transparency-based fisheye proved the most accurate but required longer completion time, making it suitable for applications where mistakes incur a high cost.",
        "keywords": [],
        "_type_counter": 3
    },
    {
        "type": "article",
        "id": "Henry2007",
        "title": "20 Years of Four HCI Conferences: A Visual Exploration",
        "author": [
            "Nathalie Henry Riche",
            "Howard Goodell",
            "Niklas Elmqvist",
            "Jean-Daniel Fekete"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/20yearshci/20yearshci.pdf",
        "_url_type": "pdf",
        "year": 2007,
        "date": "2007-01-01T00:00:00.000Z",
        "journal": "International Journal of Human-Computer Interaction",
        "volume": 23,
        "number": 3,
        "pages": "239--285",
        "abstract": "We present a visual exploration of the field of human\u2013computer interaction (HCI) through the author and article metadata of four of its major conferences: the ACM conferences on Computer-Human Interaction (CHI), User Interface Software and Technology, and Advanced Visual Interfaces and the IEEE Symposium on Information Visualization. This article describes many global and local patterns we discovered in this data set, together with the exploration process that produced them. Some expected patterns emerged, such as that---like most social networks---coauthorship and citation networks exhibit a power-law degree distribution, with a few widely collaborating authors and highly cited articles. Also, the prestigious and long-established CHI conference has the highest impact (citations by the others). Unexpected insights included that the years when a given conference was most selective are not correlated with those that produced its most highly referenced articles and that influential authors have distinct patterns of collaboration. An interesting sidelight is that methods from the HCI field---exploratory data analysis by information visualization and direct-manipulation interaction---proved useful for this analysis. They allowed us to take an open-ended, exploratory approach, guided by the data itself. As we answered our original questions, new ones arose; as we confirmed patterns we expected, we discovered refinements, exceptions, and fascinating new ones.",
        "keywords": [],
        "_type_counter": 2
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2006a",
        "title": "Evaluating the Effectiveness of Occlusion Reduction Techniques for 3D Virtual Environments",
        "author": [
            "Niklas Elmqvist",
            "Mihail Eduard Tudoreanu"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/balloonprobe/balloonprobe-full.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=ynqG3JE6744",
        "year": 2006,
        "date": "2006-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Symposium on Virtual Reality Software and Technology",
        "booktitle": "Proceedings of the ACM Symposium on Virtual Reality Software and Technology",
        "pages": "9-18",
        "abstract": "We present an empirical usability experiment studying the relative strengths and weaknesses of three different occlusion reduction techniques for discovering and accessing objects in information-rich 3D virtual environments. More specifically, the study compares standard 3D navigation, generalized fisheye techniques using object scaling and transparency, and the BalloonProbe interactive 3D space distortion technique. Subjects are asked to complete a number of different tasks, including counting, pattern recognition, and object relation, in different kinds of environments with various properties. The environments include a free-space abstract 3D environment and a virtual 3D walkthrough application for a simple building floor. The study involved 16 subjects and was conducted in a three-sided CAVE environment. Our results confirm the general guideline that each task calls for a specialized interaction---no single technique performed best across all tasks and worlds. The results also indicate a clear trade-off between speed and accuracy; simple navigation was the fastest but also most error-prone technique, whereas spherical BalloonProbe proved the most accurate",
        "keywords": [],
        "_type_counter": 6
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2006b",
        "title": "View Projection Animation for Occlusion Reduction",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/pmorph/pmorph.pdf",
        "_url_type": "pdf",
        "year": 2006,
        "date": "2006-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Conference on Advanced Visual Interfaces",
        "booktitle": "Proceedings of the ACM Conference on Advanced Visual Interfaces",
        "pages": "471--475",
        "abstract": "Inter-object occlusion is inherent to 3D environments and is one of the challenges of using 3D instead of 2D computer graphics for information visualization. In this paper, we examine this occlusion problem by building a theoretical framework of its causes and components. As a result of this analysis, we present an interaction technique for view projection animation that reduces inter-object occlusion in 3D environments without modifying the geometrical properties of the objects themselves. The technique provides smooth on-demand animation between parallel and perspective projection modes as well as online manipulation of view parameters, allowing the user to quickly and easily adapt the view to avoid occlusion. A user study indicates that the technique significantly improves object discovery over normal perspective views. We have also implemented a prototype of the technique in the Blender 3D modeller.",
        "keywords": [],
        "_type_counter": 5
    },
    {
        "type": "inproceedings",
        "id": "Sandberg2006a",
        "title": "Using 3D Audio Guidance to Locate Indoor Static Objects",
        "author": [
            "Samuel Sandberg",
            "Calle H\u00e5kansson",
            "Niklas Elmqvist",
            "Philippas Tsigas",
            "Fang Chen"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/3daudio/3daudio.pdf",
        "_url_type": "pdf",
        "year": 2006,
        "date": "2006-01-01T00:00:00.000Z",
        "journal": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting",
        "booktitle": "Proceedings of the Human Factors and Ergonomics Society Annual Meeting",
        "pages": "1581--1584",
        "abstract": "Is 3D audio an interesting technology for displaying navigational information in an indoor environment? This study found no significant differences between map- and 3D audio navigation. The user tasks tested involved finding objects in a real office environment. In order to conduct the study, a custom-made 3D audio system was built based on a public-domain HRTF-library to playback 3D sound beacons through a pair of earphones. Our results indicate that 3D audio is indeed a qualified candidate for navigation systems, and may be especially suitable for environments or individuals where vision is obstructed, insufficient, or unavailable. The study also suggests that special cues should be added to the pure spatial information to emphasize important information.",
        "keywords": [],
        "_type_counter": 4
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2005a",
        "title": "BalloonProbe: Reducing Occlusion in 3D using Interactive Space Distortion",
        "author": [
            "Niklas Elmqvist"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/balloonprobe/balloonprobe.pdf",
        "_url_type": "pdf",
        "video": "https://www.youtube.com/watch?v=ynqG3JE6744",
        "year": 2005,
        "date": "2005-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Symposium on Virtual Reality Software and Technology",
        "booktitle": "Proceedings of the ACM Symposium on Virtual Reality Software and Technology",
        "pages": "134--137",
        "abstract": "Using a 3D virtual environment for information visualization is a promising approach, but can in many cases be plagued by a phenomenon of literally not being able to see the forest for the trees. Some parts of the 3D visualization will inevitably occlude other parts, leading both to loss of efficiency and, more seriously, correctness; users may have to change their viewpoint in a non-trivial way to be able to access hidden objects, and, worse, they may not even discover some of the objects in the visualization due to this inter-object occlusion. In this paper, we present a space distortion interaction technique called the BalloonProbe which, on the user\u2019s command, inflates a spherical force field that repels objects around the 3D cursor to the surface of the sphere, separating occluding objects from each other. Inflating and deflating the sphere is performed through smooth animation, ghosted traces showing the displacement of each repelled object. Our prototype implementation uses a 3D cursor for positioning as well as for inflating and deflating the force field \"balloon\". Informal testing suggests that the BalloonProbe is a powerful way of giving users interactive control over occlusion in 3D visualizations.",
        "keywords": [],
        "_type_counter": 3
    },
    {
        "type": "article",
        "id": "Elmqvist2004a",
        "title": "Animated Visualization of Causal Relations Through Growing 2D Geometry",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/causality/causality.pdf",
        "_url_type": "pdf",
        "year": 2004,
        "date": "2004-01-01T00:00:00.000Z",
        "journal": "Information Visualization",
        "volume": 3,
        "number": 3,
        "pages": "154--172",
        "abstract": "Causality visualization is an important tool for many scientific domains that involve complex interactions between multiple entities (examples include parallel and distributed systems in computer science). However, traditional visualization techniques such as Hasse diagrams are not well-suited to large system executions, and users often have difficulties answering even basic questions using them, or have to spend inordinate amounts of time to do so. In this paper we present the Growing Squares and Growing Polygons methods, two sibling visualization techniques that were designed to solve this problem by providing efficient 2D causality visualization through the use of color, texture, and animation. Both techniques have abandoned the traditional linear timeline and instead map the time parameter to the size of geometrical primitives representing the processes; in the Growing Squares case, each process is a color-coded square that receives color influences from other process squares as messages reach it; in the Growing Polygons case, each process is instead an n-sided polygon consisting of triangular sectors showing color-coded influences from the other processes. We have performed user studies of both techniques, comparing them with Hasse diagrams, and they have been shown to be significantly more efficient than old techniques, both in terms of objective performance as well as the subjective opinion of the test subjects (the Growing Squares technique is, however, only significantly more efficient for small",
        "keywords": [],
        "_type_counter": 1
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2003b",
        "title": "Causality Visualization Using Animated Growing Polygons",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/causality/growing-polys.pdf",
        "_url_type": "pdf",
        "year": 2003,
        "date": "2003-01-01T00:00:00.000Z",
        "journal": "Proceedings of the IEEE Symposium on Information Visualization",
        "booktitle": "Proceedings of the IEEE Symposium on Information Visualization",
        "pages": "189--196",
        "abstract": "We present Growing Polygons, a novel visualization technique for the graphical representation of causal relations and information flow in a system of interacting processes. Using this method, individual processes are displayed as partitioned polygons with color-coded segments showing dependencies to other processes. The entire visualization is also animated to communicate the dynamic execution of the system to the user. The results from a comparative user study of the method show that the Growing Polygons technique is significantly more efficient than the traditional Hasse diagram visualization for analysis tasks related to deducing information flow in a system for both small and large executions. Furthermore, our findings indicate that the correctness when solving causality tasks is significantly improved using our method. In addition, the subjective ratings of the users rank the method as superior in all regards, including usability, efficiency, and enjoyability.",
        "keywords": [],
        "_type_counter": 2
    },
    {
        "type": "inproceedings",
        "id": "Elmqvist2003a",
        "title": "Growing Squares: Animated Visualization of Causal Relations",
        "author": [
            "Niklas Elmqvist",
            "Philippas Tsigas"
        ],
        "url": "http://www.umiacs.umd.edu/~elm/projects/causality/causalviz.pdf",
        "_url_type": "pdf",
        "year": 2003,
        "date": "2003-01-01T00:00:00.000Z",
        "journal": "Proceedings of the ACM Symposium on Software Visualization",
        "booktitle": "Proceedings of the ACM Symposium on Software Visualization",
        "pages": "17--26",
        "abstract": "We present a novel information visualization technique for the graphical representation of causal relations, that is based on the metaphor of color pools spreading over time on a piece of paper. Messages between processes in the system affect the colors of their respective pool, making it possible to quickly see the influences each process has received. This technique, called Growing Squares, has been evaluated in a comparative user study and shown to be significantly faster and more efficient for sparse data sets than the traditional Hasse diagram visualization. Growing Squares were also more efficient for large data sets, but not significantly so. Test subjects clearly favored Growing Squares over old methods, naming the new technique easier, more efficient, and much more enjoyable to use.",
        "keywords": [],
        "_type_counter": 1
    }
]